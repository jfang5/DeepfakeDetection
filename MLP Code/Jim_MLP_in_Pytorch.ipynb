{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jim MLP in Pytorch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R61kSqpDNC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Iris dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2RLRZ_UiF8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim,50)\n",
        "        self.layer2 = nn.Linear(50, 20)\n",
        "        self.layer3 = nn.Linear(20, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x)) # To check with the loss function\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVQ4gv0biKes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features, labels = load_iris(return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OInrnhiciPYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_train,features_test, labels_train, labels_test = train_test_split(features, labels, random_state=42, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqirJNXgiSsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "model = Model(features_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = 100\n",
        "\n",
        "def print_(loss):\n",
        "    print (\"The loss calculated: \", loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaL9QKsQiUpO",
        "colab_type": "code",
        "outputId": "8f64ccc1-868e-4ffc-a8a5-9755942afea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Not using dataloader\n",
        "x_train, y_train = Variable(torch.from_numpy(features_train)).float(), Variable(torch.from_numpy(labels_train)).long()\n",
        "for epoch in range(1, epochs+1):\n",
        "    print (\"Epoch #\",epoch)\n",
        "    y_pred = model(x_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    print_(loss.item())\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # Gradients\n",
        "    optimizer.step() # Update"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch # 1\n",
            "The loss calculated:  1.0930968523025513\n",
            "Epoch # 2\n",
            "The loss calculated:  1.074546456336975\n",
            "Epoch # 3\n",
            "The loss calculated:  1.0589396953582764\n",
            "Epoch # 4\n",
            "The loss calculated:  1.0448226928710938\n",
            "Epoch # 5\n",
            "The loss calculated:  1.0266164541244507\n",
            "Epoch # 6\n",
            "The loss calculated:  1.0069530010223389\n",
            "Epoch # 7\n",
            "The loss calculated:  0.9911503195762634\n",
            "Epoch # 8\n",
            "The loss calculated:  0.9777063131332397\n",
            "Epoch # 9\n",
            "The loss calculated:  0.9621816873550415\n",
            "Epoch # 10\n",
            "The loss calculated:  0.9460344314575195\n",
            "Epoch # 11\n",
            "The loss calculated:  0.9277940988540649\n",
            "Epoch # 12\n",
            "The loss calculated:  0.9067267179489136\n",
            "Epoch # 13\n",
            "The loss calculated:  0.8843229413032532\n",
            "Epoch # 14\n",
            "The loss calculated:  0.8643943667411804\n",
            "Epoch # 15\n",
            "The loss calculated:  0.8442254066467285\n",
            "Epoch # 16\n",
            "The loss calculated:  0.8225359916687012\n",
            "Epoch # 17\n",
            "The loss calculated:  0.8021125793457031\n",
            "Epoch # 18\n",
            "The loss calculated:  0.7844581007957458\n",
            "Epoch # 19\n",
            "The loss calculated:  0.7673386335372925\n",
            "Epoch # 20\n",
            "The loss calculated:  0.751693606376648\n",
            "Epoch # 21\n",
            "The loss calculated:  0.7359040975570679\n",
            "Epoch # 22\n",
            "The loss calculated:  0.7208331227302551\n",
            "Epoch # 23\n",
            "The loss calculated:  0.7060930132865906\n",
            "Epoch # 24\n",
            "The loss calculated:  0.6922144889831543\n",
            "Epoch # 25\n",
            "The loss calculated:  0.6785418391227722\n",
            "Epoch # 26\n",
            "The loss calculated:  0.6663733720779419\n",
            "Epoch # 27\n",
            "The loss calculated:  0.6546120047569275\n",
            "Epoch # 28\n",
            "The loss calculated:  0.6444071531295776\n",
            "Epoch # 29\n",
            "The loss calculated:  0.6354398131370544\n",
            "Epoch # 30\n",
            "The loss calculated:  0.6270779371261597\n",
            "Epoch # 31\n",
            "The loss calculated:  0.6200775504112244\n",
            "Epoch # 32\n",
            "The loss calculated:  0.6141730546951294\n",
            "Epoch # 33\n",
            "The loss calculated:  0.6090380549430847\n",
            "Epoch # 34\n",
            "The loss calculated:  0.6050668954849243\n",
            "Epoch # 35\n",
            "The loss calculated:  0.6015648245811462\n",
            "Epoch # 36\n",
            "The loss calculated:  0.5985231995582581\n",
            "Epoch # 37\n",
            "The loss calculated:  0.5961698293685913\n",
            "Epoch # 38\n",
            "The loss calculated:  0.5939550399780273\n",
            "Epoch # 39\n",
            "The loss calculated:  0.5921365022659302\n",
            "Epoch # 40\n",
            "The loss calculated:  0.5906482934951782\n",
            "Epoch # 41\n",
            "The loss calculated:  0.5892044305801392\n",
            "Epoch # 42\n",
            "The loss calculated:  0.5880915522575378\n",
            "Epoch # 43\n",
            "The loss calculated:  0.5870484113693237\n",
            "Epoch # 44\n",
            "The loss calculated:  0.5861077904701233\n",
            "Epoch # 45\n",
            "The loss calculated:  0.5853537321090698\n",
            "Epoch # 46\n",
            "The loss calculated:  0.5845850706100464\n",
            "Epoch # 47\n",
            "The loss calculated:  0.5839589238166809\n",
            "Epoch # 48\n",
            "The loss calculated:  0.583386242389679\n",
            "Epoch # 49\n",
            "The loss calculated:  0.5828224420547485\n",
            "Epoch # 50\n",
            "The loss calculated:  0.5823763012886047\n",
            "Epoch # 51\n",
            "The loss calculated:  0.5819211006164551\n",
            "Epoch # 52\n",
            "The loss calculated:  0.5815110206604004\n",
            "Epoch # 53\n",
            "The loss calculated:  0.5811687111854553\n",
            "Epoch # 54\n",
            "The loss calculated:  0.5808075666427612\n",
            "Epoch # 55\n",
            "The loss calculated:  0.5805018544197083\n",
            "Epoch # 56\n",
            "The loss calculated:  0.5802251696586609\n",
            "Epoch # 57\n",
            "The loss calculated:  0.5799396634101868\n",
            "Epoch # 58\n",
            "The loss calculated:  0.5797009468078613\n",
            "Epoch # 59\n",
            "The loss calculated:  0.579470694065094\n",
            "Epoch # 60\n",
            "The loss calculated:  0.579241156578064\n",
            "Epoch # 61\n",
            "The loss calculated:  0.579046368598938\n",
            "Epoch # 62\n",
            "The loss calculated:  0.5788528323173523\n",
            "Epoch # 63\n",
            "The loss calculated:  0.5786652565002441\n",
            "Epoch # 64\n",
            "The loss calculated:  0.5785025358200073\n",
            "Epoch # 65\n",
            "The loss calculated:  0.5783392786979675\n",
            "Epoch # 66\n",
            "The loss calculated:  0.5781839489936829\n",
            "Epoch # 67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The loss calculated:  0.5780459642410278\n",
            "Epoch # 68\n",
            "The loss calculated:  0.5779093503952026\n",
            "Epoch # 69\n",
            "The loss calculated:  0.5777782201766968\n",
            "Epoch # 70\n",
            "The loss calculated:  0.5776605606079102\n",
            "Epoch # 71\n",
            "The loss calculated:  0.5775436758995056\n",
            "Epoch # 72\n",
            "The loss calculated:  0.5774310231208801\n",
            "Epoch # 73\n",
            "The loss calculated:  0.5773281455039978\n",
            "Epoch # 74\n",
            "The loss calculated:  0.577226996421814\n",
            "Epoch # 75\n",
            "The loss calculated:  0.5771282315254211\n",
            "Epoch # 76\n",
            "The loss calculated:  0.5770367383956909\n",
            "Epoch # 77\n",
            "The loss calculated:  0.5769478678703308\n",
            "Epoch # 78\n",
            "The loss calculated:  0.5768599510192871\n",
            "Epoch # 79\n",
            "The loss calculated:  0.5767773389816284\n",
            "Epoch # 80\n",
            "The loss calculated:  0.5766981840133667\n",
            "Epoch # 81\n",
            "The loss calculated:  0.5766196250915527\n",
            "Epoch # 82\n",
            "The loss calculated:  0.5765438079833984\n",
            "Epoch # 83\n",
            "The loss calculated:  0.5764716267585754\n",
            "Epoch # 84\n",
            "The loss calculated:  0.576400637626648\n",
            "Epoch # 85\n",
            "The loss calculated:  0.576330840587616\n",
            "Epoch # 86\n",
            "The loss calculated:  0.5762637257575989\n",
            "Epoch # 87\n",
            "The loss calculated:  0.5761985778808594\n",
            "Epoch # 88\n",
            "The loss calculated:  0.5761343240737915\n",
            "Epoch # 89\n",
            "The loss calculated:  0.5760713219642639\n",
            "Epoch # 90\n",
            "The loss calculated:  0.5760102868080139\n",
            "Epoch # 91\n",
            "The loss calculated:  0.575950562953949\n",
            "Epoch # 92\n",
            "The loss calculated:  0.5758914947509766\n",
            "Epoch # 93\n",
            "The loss calculated:  0.5758331418037415\n",
            "Epoch # 94\n",
            "The loss calculated:  0.5757763981819153\n",
            "Epoch # 95\n",
            "The loss calculated:  0.5757207274436951\n",
            "Epoch # 96\n",
            "The loss calculated:  0.5756655335426331\n",
            "Epoch # 97\n",
            "The loss calculated:  0.5756108164787292\n",
            "Epoch # 98\n",
            "The loss calculated:  0.5755574107170105\n",
            "Epoch # 99\n",
            "The loss calculated:  0.575504720211029\n",
            "Epoch # 100\n",
            "The loss calculated:  0.5754523873329163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggyan-KriU8p",
        "colab_type": "code",
        "outputId": "dca22831-d66c-44f7-c081-d9a652f700a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Prediction\n",
        "x_test = Variable(torch.from_numpy(features_test)).float()\n",
        "pred = model(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp9JrASOiVDf",
        "colab_type": "code",
        "outputId": "42f24264-b74f-432c-aa76-8dd06be412b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "pred = pred.detach().numpy()\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.71068108e-07, 9.98977542e-01, 1.02176191e-03],\n",
              "       [9.99828815e-01, 1.71128122e-04, 1.05351840e-16],\n",
              "       [2.20493325e-26, 7.26701414e-14, 1.00000000e+00],\n",
              "       [7.24835388e-07, 9.97749269e-01, 2.24997592e-03],\n",
              "       [2.26433897e-07, 9.99396443e-01, 6.03261986e-04],\n",
              "       [9.99618292e-01, 3.81666119e-04, 1.58755034e-15],\n",
              "       [7.77456735e-04, 9.99221087e-01, 1.40017630e-06],\n",
              "       [3.66494092e-11, 6.80960435e-03, 9.93190348e-01],\n",
              "       [1.59609481e-08, 2.08413810e-01, 7.91586161e-01],\n",
              "       [6.92179383e-05, 9.99918342e-01, 1.24090157e-05],\n",
              "       [2.28212182e-09, 1.08372748e-01, 8.91627192e-01],\n",
              "       [9.99428689e-01, 5.71303419e-04, 2.10474711e-14],\n",
              "       [9.99838352e-01, 1.61597258e-04, 2.76472983e-16],\n",
              "       [9.99473989e-01, 5.26012620e-04, 1.17016340e-14],\n",
              "       [9.99817193e-01, 1.82890595e-04, 6.99242797e-16],\n",
              "       [9.73253009e-07, 9.99714911e-01, 2.84114038e-04],\n",
              "       [8.71233605e-18, 4.15782182e-08, 1.00000000e+00],\n",
              "       [3.25020119e-05, 9.99910235e-01, 5.72621866e-05],\n",
              "       [1.03826176e-06, 9.96645868e-01, 3.35300155e-03],\n",
              "       [6.97765641e-18, 2.58867168e-08, 1.00000000e+00],\n",
              "       [9.99327779e-01, 6.72207389e-04, 2.70295108e-14],\n",
              "       [8.12784240e-09, 1.51013613e-01, 8.48986387e-01],\n",
              "       [9.99448121e-01, 5.51828649e-04, 8.04354938e-15],\n",
              "       [4.26656187e-17, 1.09551316e-07, 9.99999881e-01],\n",
              "       [2.88595554e-11, 6.10155724e-02, 9.38984454e-01],\n",
              "       [3.08528729e-13, 1.30074230e-04, 9.99869943e-01],\n",
              "       [1.79958784e-17, 9.73689609e-08, 9.99999881e-01],\n",
              "       [4.37865046e-17, 2.37166262e-07, 9.99999762e-01],\n",
              "       [9.99182045e-01, 8.17903958e-04, 4.22073197e-14],\n",
              "       [9.99215484e-01, 7.84440141e-04, 2.86149312e-14],\n",
              "       [9.99827504e-01, 1.72504762e-04, 3.68787007e-15],\n",
              "       [9.99945283e-01, 5.47132622e-05, 1.08993106e-17],\n",
              "       [1.59841438e-05, 9.99980450e-01, 3.52204938e-06],\n",
              "       [9.99574482e-01, 4.25509003e-04, 8.21977120e-15],\n",
              "       [9.99479234e-01, 5.20734640e-04, 5.04566109e-14],\n",
              "       [3.76150012e-13, 7.90583144e-05, 9.99920964e-01],\n",
              "       [5.06142669e-06, 9.99964476e-01, 3.04902405e-05],\n",
              "       [9.99756634e-01, 2.43328192e-04, 1.08574737e-15]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJtMU2uBiVKN",
        "colab_type": "code",
        "outputId": "aae05a50-211d-4abd-cbfb-48be7d375e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print (\"The accuracy is\", accuracy_score(labels_test, np.argmax(pred, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.9736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}