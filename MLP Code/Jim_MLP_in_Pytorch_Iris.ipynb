{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jim MLP in Pytorch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R61kSqpDNC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "058ada08-55ef-4359-dc92-a919afd95ffb"
      },
      "source": [
        "#Iris dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.utils import to_categorical\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2RLRZ_UiF8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create MLP architecture using nn.Linear\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim,50)\n",
        "        self.layer2 = nn.Linear(50, 20)\n",
        "        self.layer3 = nn.Linear(20, 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.softmax(self.layer3(x)) # To check with the loss function\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVQ4gv0biKes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Features and Labels for Iris dataset\n",
        "features, labels = load_iris(return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OInrnhiciPYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Perform test train split\n",
        "features_train,features_test, labels_train, labels_test = train_test_split(features, labels, random_state=1, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8erGb6_DQ3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "9b2954e7-551a-4ebf-89cb-f44ebc839d85"
      },
      "source": [
        "labels_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 0, 2, 2, 1, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 2, 2, 1, 2, 1,\n",
              "       0, 2, 2, 0, 0, 2, 0, 2, 2, 1, 1, 2, 2, 0, 1, 1, 2, 1, 2, 1, 0, 0,\n",
              "       0, 2, 0, 1, 2, 2, 0, 0, 1, 0, 2, 1, 2, 2, 1, 2, 2, 1, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 2, 2, 2, 0, 0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 1, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 2, 1, 2, 1, 2, 2, 1,\n",
              "       2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqirJNXgiSsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize model and set optimizer (Adam) and loss function (Cross Entropy)\n",
        "model = Model(features_train.shape[1])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "epochs = 100\n",
        "\n",
        "#Create function to print loss at each epoch\n",
        "def print_(loss):\n",
        "    print (\"The loss calculated: \", loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaL9QKsQiUpO",
        "colab_type": "code",
        "outputId": "3b874254-eb6f-48cc-89e1-cfa1bf8c5a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Load x and y training sets as tensors and begin modeling\n",
        "x_train, y_train = Variable(torch.from_numpy(features_train)).float(), Variable(torch.from_numpy(labels_train)).long()\n",
        "for epoch in range(1, epochs+1):\n",
        "    print (\"Epoch #\",epoch)\n",
        "    y_pred = model(x_train)\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "    print_(loss.item())\n",
        "    \n",
        "    # Zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # Backpropogation to obtain new gradients\n",
        "    optimizer.step() # Update weights and biases before next epoch"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch # 1\n",
            "The loss calculated:  1.1092793941497803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch # 2\n",
            "The loss calculated:  1.066132664680481\n",
            "Epoch # 3\n",
            "The loss calculated:  1.0352528095245361\n",
            "Epoch # 4\n",
            "The loss calculated:  1.013745903968811\n",
            "Epoch # 5\n",
            "The loss calculated:  0.994579017162323\n",
            "Epoch # 6\n",
            "The loss calculated:  0.9694333672523499\n",
            "Epoch # 7\n",
            "The loss calculated:  0.9364279508590698\n",
            "Epoch # 8\n",
            "The loss calculated:  0.9029765725135803\n",
            "Epoch # 9\n",
            "The loss calculated:  0.8755249977111816\n",
            "Epoch # 10\n",
            "The loss calculated:  0.8533567190170288\n",
            "Epoch # 11\n",
            "The loss calculated:  0.8307565450668335\n",
            "Epoch # 12\n",
            "The loss calculated:  0.8084360957145691\n",
            "Epoch # 13\n",
            "The loss calculated:  0.7895728349685669\n",
            "Epoch # 14\n",
            "The loss calculated:  0.7726094126701355\n",
            "Epoch # 15\n",
            "The loss calculated:  0.7586269378662109\n",
            "Epoch # 16\n",
            "The loss calculated:  0.745472252368927\n",
            "Epoch # 17\n",
            "The loss calculated:  0.7301193475723267\n",
            "Epoch # 18\n",
            "The loss calculated:  0.7137005925178528\n",
            "Epoch # 19\n",
            "The loss calculated:  0.6979542970657349\n",
            "Epoch # 20\n",
            "The loss calculated:  0.6828629374504089\n",
            "Epoch # 21\n",
            "The loss calculated:  0.6691451668739319\n",
            "Epoch # 22\n",
            "The loss calculated:  0.6559482216835022\n",
            "Epoch # 23\n",
            "The loss calculated:  0.6445356607437134\n",
            "Epoch # 24\n",
            "The loss calculated:  0.6343656778335571\n",
            "Epoch # 25\n",
            "The loss calculated:  0.625679612159729\n",
            "Epoch # 26\n",
            "The loss calculated:  0.6183272004127502\n",
            "Epoch # 27\n",
            "The loss calculated:  0.612076461315155\n",
            "Epoch # 28\n",
            "The loss calculated:  0.6071359515190125\n",
            "Epoch # 29\n",
            "The loss calculated:  0.6027689576148987\n",
            "Epoch # 30\n",
            "The loss calculated:  0.5994267463684082\n",
            "Epoch # 31\n",
            "The loss calculated:  0.5964568853378296\n",
            "Epoch # 32\n",
            "The loss calculated:  0.5941651463508606\n",
            "Epoch # 33\n",
            "The loss calculated:  0.5921179056167603\n",
            "Epoch # 34\n",
            "The loss calculated:  0.5904777646064758\n",
            "Epoch # 35\n",
            "The loss calculated:  0.5890359282493591\n",
            "Epoch # 36\n",
            "The loss calculated:  0.5878135561943054\n",
            "Epoch # 37\n",
            "The loss calculated:  0.5867680907249451\n",
            "Epoch # 38\n",
            "The loss calculated:  0.5858222842216492\n",
            "Epoch # 39\n",
            "The loss calculated:  0.5850372910499573\n",
            "Epoch # 40\n",
            "The loss calculated:  0.5842824578285217\n",
            "Epoch # 41\n",
            "The loss calculated:  0.5836769938468933\n",
            "Epoch # 42\n",
            "The loss calculated:  0.5830695033073425\n",
            "Epoch # 43\n",
            "The loss calculated:  0.5825892090797424\n",
            "Epoch # 44\n",
            "The loss calculated:  0.582095205783844\n",
            "Epoch # 45\n",
            "The loss calculated:  0.5816973447799683\n",
            "Epoch # 46\n",
            "The loss calculated:  0.5812979340553284\n",
            "Epoch # 47\n",
            "The loss calculated:  0.5809599161148071\n",
            "Epoch # 48\n",
            "The loss calculated:  0.5806373953819275\n",
            "Epoch # 49\n",
            "The loss calculated:  0.5803412795066833\n",
            "Epoch # 50\n",
            "The loss calculated:  0.5800774693489075\n",
            "Epoch # 51\n",
            "The loss calculated:  0.5798162817955017\n",
            "Epoch # 52\n",
            "The loss calculated:  0.5795963406562805\n",
            "Epoch # 53\n",
            "The loss calculated:  0.5793690085411072\n",
            "Epoch # 54\n",
            "The loss calculated:  0.579179584980011\n",
            "Epoch # 55\n",
            "The loss calculated:  0.578984260559082\n",
            "Epoch # 56\n",
            "The loss calculated:  0.5788143277168274\n",
            "Epoch # 57\n",
            "The loss calculated:  0.5786489248275757\n",
            "Epoch # 58\n",
            "The loss calculated:  0.5784942507743835\n",
            "Epoch # 59\n",
            "The loss calculated:  0.5783535838127136\n",
            "Epoch # 60\n",
            "The loss calculated:  0.5782129168510437\n",
            "Epoch # 61\n",
            "The loss calculated:  0.5780903100967407\n",
            "Epoch # 62\n",
            "The loss calculated:  0.5779643058776855\n",
            "Epoch # 63\n",
            "The loss calculated:  0.5778523683547974\n",
            "Epoch # 64\n",
            "The loss calculated:  0.5777416825294495\n",
            "Epoch # 65\n",
            "The loss calculated:  0.5776371955871582\n",
            "Epoch # 66\n",
            "The loss calculated:  0.577539324760437\n",
            "Epoch # 67\n",
            "The loss calculated:  0.5774412751197815\n",
            "Epoch # 68\n",
            "The loss calculated:  0.5773525238037109\n",
            "Epoch # 69\n",
            "The loss calculated:  0.5772623419761658\n",
            "Epoch # 70\n",
            "The loss calculated:  0.5771781802177429\n",
            "Epoch # 71\n",
            "The loss calculated:  0.5770959854125977\n",
            "Epoch # 72\n",
            "The loss calculated:  0.5770149230957031\n",
            "Epoch # 73\n",
            "The loss calculated:  0.576938807964325\n",
            "Epoch # 74\n",
            "The loss calculated:  0.5768617391586304\n",
            "Epoch # 75\n",
            "The loss calculated:  0.5767887234687805\n",
            "Epoch # 76\n",
            "The loss calculated:  0.5767162442207336\n",
            "Epoch # 77\n",
            "The loss calculated:  0.5766445398330688\n",
            "Epoch # 78\n",
            "The loss calculated:  0.576575756072998\n",
            "Epoch # 79\n",
            "The loss calculated:  0.5765063166618347\n",
            "Epoch # 80\n",
            "The loss calculated:  0.5764385461807251\n",
            "Epoch # 81\n",
            "The loss calculated:  0.5763718485832214\n",
            "Epoch # 82\n",
            "The loss calculated:  0.5763048529624939\n",
            "Epoch # 83\n",
            "The loss calculated:  0.5762392282485962\n",
            "Epoch # 84\n",
            "The loss calculated:  0.5761736631393433\n",
            "Epoch # 85\n",
            "The loss calculated:  0.5761080384254456\n",
            "Epoch # 86\n",
            "The loss calculated:  0.5760436058044434\n",
            "Epoch # 87\n",
            "The loss calculated:  0.5759783983230591\n",
            "Epoch # 88\n",
            "The loss calculated:  0.5759134292602539\n",
            "Epoch # 89\n",
            "The loss calculated:  0.5758489370346069\n",
            "Epoch # 90\n",
            "The loss calculated:  0.5757836103439331\n",
            "Epoch # 91\n",
            "The loss calculated:  0.5757185220718384\n",
            "Epoch # 92\n",
            "The loss calculated:  0.575653612613678\n",
            "Epoch # 93\n",
            "The loss calculated:  0.5755879282951355\n",
            "Epoch # 94\n",
            "The loss calculated:  0.5755223035812378\n",
            "Epoch # 95\n",
            "The loss calculated:  0.5754566192626953\n",
            "Epoch # 96\n",
            "The loss calculated:  0.5753903985023499\n",
            "Epoch # 97\n",
            "The loss calculated:  0.5753237009048462\n",
            "Epoch # 98\n",
            "The loss calculated:  0.5752571821212769\n",
            "Epoch # 99\n",
            "The loss calculated:  0.5751901865005493\n",
            "Epoch # 100\n",
            "The loss calculated:  0.5751228332519531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggyan-KriU8p",
        "colab_type": "code",
        "outputId": "d25b09f7-8555-41cd-d9e0-c59f7c2bd64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "#Perform prediction on test set\n",
        "x_test = Variable(torch.from_numpy(features_test)).float()\n",
        "pred = model(x_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp9JrASOiVDf",
        "colab_type": "code",
        "outputId": "d16df87a-c99d-4604-c2d0-ad759d35c3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "pred = pred.detach().numpy()\n",
        "pred"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9999571e-01, 4.2568017e-06, 7.0259835e-22],\n",
              "       [7.6665282e-03, 9.9233353e-01, 1.2143558e-08],\n",
              "       [8.0252903e-06, 9.9999177e-01, 2.7676731e-07],\n",
              "       [9.9998796e-01, 1.1987506e-05, 2.6570574e-20],\n",
              "       [1.0995460e-10, 1.5204744e-03, 9.9847955e-01],\n",
              "       [6.1142405e-06, 9.9773139e-01, 2.2626310e-03],\n",
              "       [4.4515125e-09, 5.2480218e-03, 9.9475199e-01],\n",
              "       [9.9986660e-01, 1.3333594e-04, 5.0828110e-18],\n",
              "       [9.9981683e-01, 1.8314274e-04, 4.0921240e-17],\n",
              "       [3.3158727e-13, 1.0929560e-06, 9.9999893e-01],\n",
              "       [1.8328745e-05, 9.9950004e-01, 4.8156417e-04],\n",
              "       [9.9996209e-01, 3.7876856e-05, 2.0114627e-19],\n",
              "       [3.7715802e-13, 2.5013467e-06, 9.9999750e-01],\n",
              "       [8.0177042e-06, 9.9998057e-01, 1.1495558e-05],\n",
              "       [7.9095998e-06, 9.9525082e-01, 4.7412636e-03],\n",
              "       [9.9991703e-01, 8.2948769e-05, 3.3217787e-17],\n",
              "       [2.4627170e-05, 9.9997401e-01, 1.3091827e-06],\n",
              "       [9.9846629e-06, 8.0323851e-01, 1.9675151e-01],\n",
              "       [9.9992621e-01, 7.3733441e-05, 7.9765357e-19],\n",
              "       [9.9995160e-01, 4.8390844e-05, 1.3891885e-18],\n",
              "       [1.2505541e-05, 9.8724967e-01, 1.2737855e-02],\n",
              "       [4.4770914e-06, 3.0488518e-01, 6.9511032e-01],\n",
              "       [1.5001243e-06, 9.3757123e-01, 6.2427305e-02],\n",
              "       [9.9996591e-01, 3.4133951e-05, 7.6106365e-19],\n",
              "       [5.3305464e-11, 3.2100658e-04, 9.9967897e-01],\n",
              "       [2.3206292e-05, 9.9995720e-01, 1.9500587e-05],\n",
              "       [9.9999499e-01, 4.9883242e-06, 2.5910425e-21],\n",
              "       [9.9997294e-01, 2.7020942e-05, 3.0625295e-19],\n",
              "       [5.2722676e-06, 9.9933976e-01, 6.5494026e-04],\n",
              "       [6.7840761e-10, 6.1004871e-04, 9.9938989e-01],\n",
              "       [7.1324625e-06, 9.9900478e-01, 9.8814350e-04],\n",
              "       [1.8176617e-13, 3.4455832e-06, 9.9999654e-01],\n",
              "       [3.7551865e-05, 9.9996126e-01, 1.1824566e-06],\n",
              "       [5.6037115e-14, 1.2435012e-07, 9.9999988e-01],\n",
              "       [2.6944145e-14, 1.4463830e-08, 1.0000000e+00],\n",
              "       [9.9997461e-01, 2.5402174e-05, 1.1796904e-19],\n",
              "       [1.8884002e-05, 9.9948418e-01, 4.9702171e-04],\n",
              "       [9.9995589e-01, 4.4156590e-05, 5.1495628e-19]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJtMU2uBiVKN",
        "colab_type": "code",
        "outputId": "7aadfadb-98fe-4620-b65d-c767757667dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#Print accuracy score\n",
        "print (\"The accuracy is\", accuracy_score(labels_test, np.argmax(pred, axis=1)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.9736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsdv3fgrDhKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a530b7f0-c5ef-46cc-c4cb-0780b3d11153"
      },
      "source": [
        "#Print classification report for multi-class predictions (0,1,2)\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(classification_report(labels_test, np.argmax(pred, axis=1)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.98      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}