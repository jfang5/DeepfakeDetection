{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understandably that without the activation function it becomes a linear problem. Can you give me an example of a complicated NN problem that would require a activation function? Would this be something for monte carlo is my guess?\n",
    "2. Difference between pytorch variable and tensor - does this still exist?\n",
    "\n",
    "3. When you do that first layer process - from 4 to x # whats the determination for this?\n",
    "4. What is Autograd for?https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "5. BCELoss versus Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier Initalization?\n",
    "\n",
    "https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "\n",
    " Instead of initializing our weights with small numbers which are distributed randomly we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)\n",
    " \n",
    "  the weights in a network start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful.\n",
    "If the weights in a network start too large, then the signal grows as it passes through each layer until it’s too massive to be useful.\n",
    "Xavier initialization makes sure the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets #batching and working with data is already done for us so it has prepared data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn import datasets as skdata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-layer-perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor is a multi dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  3.])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])\n",
    "\n",
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1969, 0.4761, 0.6599, 0.2518, 0.8377],\n",
      "        [0.9987, 0.6410, 0.1174, 0.2234, 0.4336]])\n",
      "\n",
      " you want to do a flatten operation so you can do nn aka turn this 2 by 5 into a 1 by 10\n",
      "\n",
      "tensor([[0.1969, 0.4761, 0.6599, 0.2518, 0.8377, 0.9987, 0.6410, 0.1174, 0.2234,\n",
      "         0.4336]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand([2,5])\n",
    "print(y)\n",
    "print('\\n you want to do a flatten operation so you can do nn aka turn this 2 by 5 into a 1 by 10\\n')\n",
    "y = y.view([1,10])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "#handdrawn digits from 0 to 9 - 28 by 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True) #size is huge so ram can only handle so much. need a batch size to split it up to get to the full epoch - base 8 - bigger batch size = faster training\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)\n",
    "\n",
    "#shuffle lets you figure out generalized principles not just 1 ,2 ,3 ,4 ,5 ,6\n",
    "\n",
    "#returns a list of x and y tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 9, 1, 7, 3, 0, 3, 5, 7, 1])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:  #prints 1 batch and the 10 tensors of the output\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balancing = as the optimizer is trying to decrease the loss - does not know how godo it can get but trying to decrease it as easy as possible. 0-9. if class imbalance and 60% are 3s in the dataset; then adjust weights that quickly predict 3's and get stuck. can add in momentum and modify weights of specific weights (but want it as balanced as possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 12000, 1: 12000, 2: 0, 3: 12000, 4: 0, 5: 6000, 6: 0, 7: 12000, 8: 0, 9: 6000}\n"
     ]
    }
   ],
   "source": [
    "total = 0 \n",
    "counter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0 }\n",
    "\n",
    "for i in trainset:\n",
    "    xs, ys = data\n",
    "    for y in ys:\n",
    "        counter[int(y)] +=1 \n",
    "        total +=1\n",
    "        \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 20.0\n",
      "1: 20.0\n",
      "2: 0.0\n",
      "3: 20.0\n",
      "4: 0.0\n",
      "5: 10.0\n",
      "6: 0.0\n",
      "7: 20.0\n",
      "8: 0.0\n",
      "9: 10.0\n"
     ]
    }
   ],
   "source": [
    "for i in counter:\n",
    "    print(f'{i}: {counter[i]/total*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 20])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Linear(20, 30) #THE INPUT is 20 the output is 30\n",
    "input = torch.randn(128, 20) #you put in a torch with random #'s of a size of 128x20\n",
    "output = m(input) #transforming this 128x20 and \n",
    "print(output.size())\n",
    "torch.Size([128, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps for Week 4:\n",
    "Multilayer Perceptron using Iris dataset (1-2 hours) \n",
    "\n",
    "## Iris Dataset to make a multilayer preceptrion / image net 5-10 classes and build a CNN architecture\n",
    "### for multi-layer perceptron - simple classifier --> Iris Dataset\n",
    "\n",
    "\n",
    "## CNN from ImageNet prelabeled dataset (3-5 days) using PyTorch on Google Collab\n",
    "### Weight = Kernal in CNN after traversing the image\n",
    "\n",
    "striding, kernel learning ,residual block (res net became famous because of this) - incorporte new info \n",
    "from the original information\n",
    "travesing the image of the kernel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Adam - variant of stocastic descent - momentum - instances where stuck at a minima but no the global minimum - extra coefficient to get out of the local minimas - saddle point\n",
    "\n",
    "multi-layer perceptron and basic CNN classifier + RNN's classifier -> thus use gated RNN's LSTMS - prevent gradients exploding or diminish GLU or LSTMS- fundamental understanding \n",
    "- use PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - Iris Example - Base Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = skdata.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)\n",
    "print(iris.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.target.shape)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = iris.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.shape\n",
    "#has data on 150 irises of the 4 features above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "features = iris.feature_names\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names #one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data[:5] # shows first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1 4.9\n"
     ]
    }
   ],
   "source": [
    "x, y  = iris_data[0][0], iris_data[1][0]\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(iris_data, aspect = 'auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(iris_data, iris_y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4) (120,) (30,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heoun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/heoun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_pred = lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_pred_prob = lr.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['              precision    recall  f1-score   support',\n",
       " '',\n",
       " '           0      1.000     1.000     1.000        11',\n",
       " '           1      1.000     0.923     0.960        13',\n",
       " '           2      0.857     1.000     0.923         6',\n",
       " '',\n",
       " '    accuracy                          0.967        30',\n",
       " '   macro avg      0.952     0.974     0.961        30',\n",
       " 'weighted avg      0.971     0.967     0.967        30',\n",
       " '']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = classification_report(ytest, ytest_pred, digits = 3)\n",
    "report.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - Iris Example - NN\n",
    "\n",
    "Convert data from .csv file to a numpy array (Now this is your input data of size 150 x 4)\n",
    "Now make a random split of 120 samples as train and rest as test ( Note it has to be random)\n",
    "Now, your train data is of size 120 x 4 and test is 30 x 4.\n",
    "Resize the train and test array to a tensor of size N x N. You can use some pre-built methods in TF / keras to resize the arrays.\n",
    "Now you have a input (train and test) of size N X N, apply consecutive conv layers + relu + pool and a couple of fc layers in the end.\n",
    "Use a mean squared error as you have real-valued data and a good optimizer like Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(iris_data, iris_y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# iris_dim = int(iris_data.shape[0]*.8)\n",
    "# c = numpy.c_[iris_data.reshape(len(a), -1), b.reshape(len(b), -1)]\n",
    "\n",
    "# #np.random.shuffle(iris_data)\n",
    "# xtrain, xtest = iris_data[:iris_dim], iris_data[iris_dim+1:] #instead of doing this convert this to a tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9, 3. , 1.4, 0.2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is using the One Hot Encoding Approach - It does not work because of the issue of it being greater than the bounds of 0-1. Ask Shantanu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y_categ = to_categorical(iris_y) #need to remove onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_y_categ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_iris_tensor = torch.from_numpy(iris_data).float()\n",
    "y_iris_tensor = torch.from_numpy(iris_y_categ).float()\n",
    "#y_iris_tensor = torch.from_numpy(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 4])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_iris_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_iris_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module): #inheriting the Module from torch.nn.Module\n",
    "    ''' \n",
    "    This has the basic components of the NN\n",
    "    '''\n",
    "    def __init__(self, batch):\n",
    "        super(net, self).__init__()\n",
    "        self.batch = 10\n",
    "\n",
    "        self.relu = torch.nn.ReLU() #adds the relu function - relu(output)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.mlp1 = nn.Linear(4, 64) #image is 28x28 or in this case 4 columns of dependent variables #input, output, bias is auto True y = xA^t + b\n",
    "        self.mlp2 = nn.Linear(64, 128)\n",
    "        self.mlp3 = nn.Linear(128, 3) #need final results to be the 3 different flowers availble\n",
    "    def forward(self, x):\n",
    "        layer1 = self.relu(self.mlp1(x))\n",
    "        layer11 = self.drop(layer1)\n",
    "        layer2 = self.relu(self.mlp2(layer11))\n",
    "        layer22 = self.drop(layer2)\n",
    "        y_prediction = self.relu(self.mlp3(layer22))\n",
    "        return y_prediction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binay cross entropy loss function - backprop - how we measure our loss\n",
    "loss_fun=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#BCELoss(size_average=True)\n",
    "mlp = net(10).float()\n",
    "#SGD optimizer\n",
    "#this is the sgd optimizer that then gets to the loss function learning rate of 0.01, momentum is  #learning rate\n",
    "opt= torch.optim.Adam(mlp.parameters(), lr = 0.01)\n",
    "#torch.optim.SGD(mlp.parameters(),lr=0.01, momentum = 0.9) \n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = torch.randperm(x_iris_tensor.size()[0]) #Returns a random permutation of integers from 0 to n - 1 or in this case the 150 available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input, target in dataset:\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(input)\n",
    "#     loss = loss_fn(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "torch.Size([150, 3])\n",
      "torch.Size([150, 3])\n",
      "tensor([[0.0191, 0.0000, 0.0000],\n",
      "        [0.1348, 0.4057, 0.0000],\n",
      "        [0.0000, 0.2105, 0.0000],\n",
      "        [0.3385, 0.0042, 0.0000],\n",
      "        [0.0837, 0.3435, 0.0000],\n",
      "        [0.0000, 0.7128, 0.0000],\n",
      "        [0.0000, 0.5377, 0.0000],\n",
      "        [0.0798, 0.3776, 0.0000],\n",
      "        [0.0000, 0.3944, 0.0000],\n",
      "        [0.0056, 0.0000, 0.0944],\n",
      "        [0.1504, 0.1633, 0.0000],\n",
      "        [0.0000, 0.0050, 0.0000],\n",
      "        [0.7247, 0.2447, 0.0000],\n",
      "        [0.0000, 0.2127, 0.0000],\n",
      "        [0.9271, 0.2957, 0.0000],\n",
      "        [0.0000, 0.4526, 0.0000],\n",
      "        [0.4679, 0.5142, 0.0000],\n",
      "        [1.0226, 1.4789, 0.0000],\n",
      "        [0.0103, 0.0133, 0.0000],\n",
      "        [0.0000, 0.0646, 0.0000],\n",
      "        [0.0028, 0.1291, 0.0000],\n",
      "        [0.1869, 0.3605, 0.0000],\n",
      "        [0.2845, 0.2483, 0.0000],\n",
      "        [0.0740, 0.0000, 0.0000],\n",
      "        [0.6838, 0.9579, 0.0000],\n",
      "        [0.2973, 0.2497, 0.0000],\n",
      "        [0.1129, 0.2170, 0.0000],\n",
      "        [0.5016, 0.5606, 0.0000],\n",
      "        [0.0049, 0.2931, 0.0000],\n",
      "        [0.0746, 0.3492, 0.0000],\n",
      "        [0.7252, 0.0000, 0.0000],\n",
      "        [0.4095, 0.4973, 0.0000],\n",
      "        [0.9519, 0.4808, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.3742, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0370, 0.0000, 0.0000],\n",
      "        [0.4898, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5261, 0.0000],\n",
      "        [0.0000, 0.2773, 0.0000],\n",
      "        [0.1203, 0.0000, 0.0000],\n",
      "        [0.1956, 0.1013, 0.0000],\n",
      "        [0.0570, 0.0000, 0.0000],\n",
      "        [0.0828, 0.1600, 0.3360],\n",
      "        [0.1228, 0.1943, 0.0000],\n",
      "        [0.5445, 0.4130, 0.0000],\n",
      "        [0.1457, 0.4584, 0.0000],\n",
      "        [0.0047, 0.0438, 0.0000],\n",
      "        [0.4101, 0.0000, 0.0000],\n",
      "        [0.0453, 0.0000, 0.0000],\n",
      "        [0.3796, 0.0000, 0.0000],\n",
      "        [0.2519, 0.5789, 0.0000],\n",
      "        [0.4582, 0.2752, 0.0000],\n",
      "        [0.1846, 0.6406, 0.0000],\n",
      "        [0.3212, 0.2120, 0.0000],\n",
      "        [0.4675, 0.2329, 0.0000],\n",
      "        [0.0000, 0.5802, 0.0000],\n",
      "        [0.8380, 0.4658, 0.0000],\n",
      "        [0.2435, 0.8357, 0.0000],\n",
      "        [0.5522, 0.5699, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.3511, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2605, 0.0000],\n",
      "        [0.0356, 0.0000, 0.0000],\n",
      "        [1.0106, 0.0000, 0.0000],\n",
      "        [0.4280, 0.2257, 0.0000],\n",
      "        [0.4600, 0.4635, 0.0000],\n",
      "        [0.0454, 0.4415, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.6953, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4263, 0.0000],\n",
      "        [0.6339, 0.0000, 0.0000],\n",
      "        [0.4934, 0.3071, 0.0000],\n",
      "        [0.1222, 0.2827, 0.0000],\n",
      "        [0.8353, 0.9486, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.3860, 0.5276, 0.0000],\n",
      "        [0.9096, 0.0000, 0.0000],\n",
      "        [0.0089, 0.4580, 0.0000],\n",
      "        [0.3060, 0.4422, 0.2211],\n",
      "        [0.6929, 1.0677, 0.0477],\n",
      "        [0.0000, 0.7545, 0.8235],\n",
      "        [0.9294, 0.8267, 0.0000],\n",
      "        [0.6627, 0.1638, 0.0000],\n",
      "        [0.0772, 0.2450, 0.0000],\n",
      "        [0.9099, 0.8576, 0.1037],\n",
      "        [0.2462, 0.7677, 0.0198],\n",
      "        [0.7747, 0.0000, 0.0000],\n",
      "        [0.5309, 0.0404, 0.0000],\n",
      "        [1.3701, 0.5716, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.2049, 0.0413, 0.0000],\n",
      "        [0.6179, 0.2102, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0035, 0.0000, 0.0000],\n",
      "        [1.1014, 0.1877, 0.0000],\n",
      "        [1.1005, 0.2908, 0.1441],\n",
      "        [0.1453, 0.0000, 0.0000],\n",
      "        [0.3491, 0.0000, 0.0000],\n",
      "        [0.3852, 0.1015, 0.0000],\n",
      "        [0.4122, 1.8122, 0.2034],\n",
      "        [0.2484, 0.1832, 0.0000],\n",
      "        [1.0307, 0.0000, 0.0000],\n",
      "        [0.9360, 0.9943, 0.0000],\n",
      "        [0.0000, 0.4836, 0.0000],\n",
      "        [1.2401, 0.6553, 0.0000],\n",
      "        [0.4249, 0.7709, 0.0000],\n",
      "        [0.0000, 0.1051, 0.0000],\n",
      "        [0.0150, 0.5683, 0.0000],\n",
      "        [0.0000, 0.3908, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [1.3725, 0.0055, 0.0000],\n",
      "        [0.0000, 0.8980, 0.0000],\n",
      "        [0.3397, 0.3737, 0.0000],\n",
      "        [0.2195, 0.2679, 0.0000],\n",
      "        [0.0000, 0.7610, 0.0969],\n",
      "        [0.6032, 0.2586, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0492, 0.0000, 0.0000],\n",
      "        [0.9992, 0.7083, 0.2378],\n",
      "        [0.7201, 1.2252, 0.0000],\n",
      "        [0.4151, 0.2359, 0.0000],\n",
      "        [0.5362, 0.9062, 0.0000],\n",
      "        [0.6099, 0.0000, 0.0000],\n",
      "        [0.2878, 0.0000, 0.0000],\n",
      "        [0.0044, 0.0158, 0.6000],\n",
      "        [0.1343, 0.7857, 0.0000],\n",
      "        [0.2814, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0944],\n",
      "        [0.8466, 0.0466, 0.0000],\n",
      "        [0.1866, 0.8099, 0.0000],\n",
      "        [0.4695, 0.0000, 0.0000],\n",
      "        [0.1390, 0.7169, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3362, 0.0000],\n",
      "        [0.0000, 0.7204, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.4721, 0.6138, 0.0000],\n",
      "        [0.1216, 0.4279, 0.0000],\n",
      "        [1.5965, 0.0000, 0.6351],\n",
      "        [0.0000, 0.1180, 0.0000],\n",
      "        [0.2775, 0.6097, 0.0000],\n",
      "        [1.0177, 1.1600, 0.0000],\n",
      "        [0.8726, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-aef9599d177b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_iris_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 932\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "#training\n",
    "loss_array = [] \n",
    "for epoch in range(100):\n",
    "    print(\"Epoch: \" + str(epoch))\n",
    "    #for i in range(0,x_iris_tensor.size()[0],batch_size):\n",
    "        #print(\"batch: \" + str(i))\n",
    "        #indices = permutation[i:i+batch_size]\n",
    "       # batch_x, batch_y = x_iris_tensor[indices], y_iris_tensor[indices]\n",
    "    y_pred_val=mlp.forward(x_iris_tensor)\n",
    "\n",
    "    print(y_pred_val.shape)\n",
    "    print(y_iris_tensor.shape)\n",
    "    print(y_pred_val)\n",
    "\n",
    "    loss=loss_fun(y_pred_val,y_iris_tensor)\n",
    "    loss_array.append(loss)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch: {i} Loss: {loss}')\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is using the One column with different vars Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_iris_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris_y_formated = np.expand_dims(iris_y, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_iris_tensor = torch.from_numpy(iris_data).float()\n",
    "# y_iris_tensor = torch.from_numpy(iris_y_categ).float()\n",
    "#y_iris_tensor = np.squeeze(y_iris_tensor)\n",
    "y_iris_tensor = torch.LongTensor(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 4])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_iris_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_iris_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module): #inheriting the Module from torch.nn.Module\n",
    "    ''' \n",
    "    This has the basic components of the NN\n",
    "    '''\n",
    "    def __init__(self, batch):\n",
    "        super(net, self).__init__()\n",
    "        self.batch = 10\n",
    "\n",
    "        self.relu = torch.nn.ReLU() #adds the relu function - relu(output)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.mlp1 = nn.Linear(4, 128) #image is 28x28 or in this case 4 columns of dependent variables #input, output, bias is auto True y = xA^t + b\n",
    "        self.mlp2 = nn.Linear(128, 12)\n",
    "        self.mlp3 = nn.Linear(12, 3) #need final results to be the 3 different flowers availble\n",
    "    def forward(self, x):\n",
    "        layer1 = self.relu(self.mlp1(x))\n",
    "        layer11 = self.drop(layer1)\n",
    "        layer2 = self.relu(self.mlp2(layer11))\n",
    "        layer22 = self.drop(layer2)\n",
    "        y_prediction = self.relu(self.mlp3(layer2))\n",
    "        return y_prediction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binay cross entropy loss function - backprop - how we measure our loss\n",
    "#loss_fun=torch.nn.BCELoss(reduce = False)\n",
    "loss_fun=torch.nn.CrossEntropyLoss()\n",
    "mlp = net(10).float()\n",
    "#SGD optimizer\n",
    "#this is the sgd optimizer that then gets to the loss function learning rate of 0.01, momentum is  #learning rate\n",
    "opt= torch.optim.SGD(mlp.parameters(),lr=0.01, momentum = 0.9) \n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = torch.randperm(x_iris_tensor.size()[0]) #Returns a random permutation of integers from 0 to n - 1 or in this case the 150 available\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input, target in dataset:\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(input)\n",
    "#     loss = loss_fn(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.1238269805908203\n",
      "Epoch: 10 Loss: 1.019579529762268\n",
      "Epoch: 20 Loss: 0.8482896685600281\n",
      "Epoch: 30 Loss: 0.6058668494224548\n",
      "Epoch: 40 Loss: 0.4494011402130127\n",
      "Epoch: 50 Loss: 0.4307299256324768\n",
      "Epoch: 60 Loss: 0.37817659974098206\n",
      "Epoch: 70 Loss: 0.29017290472984314\n",
      "Epoch: 80 Loss: 0.2940356731414795\n",
      "Epoch: 90 Loss: 0.28316938877105713\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "loss_array = [] \n",
    "for epoch in range(100):\n",
    "\n",
    "#     for i in range(0,x_iris_tensor.size()[0],batch_size):\n",
    "#         print(\"batch: \" + str(i))\n",
    "#         indices = permutation[i:i+batch_size]\n",
    "#         batch_x, batch_y = x_iris_tensor[indices], y_iris_tensor[indices]\n",
    "        y_pred_val=mlp.forward(x_iris_tensor)\n",
    "\n",
    "        loss=loss_fun(y_pred_val,y_iris_tensor)\n",
    "        loss_array.append(loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch} Loss: {loss}')\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.1287, grad_fn=<NllLossBackward>),\n",
       " tensor(1.1261, grad_fn=<NllLossBackward>),\n",
       " tensor(1.1212, grad_fn=<NllLossBackward>),\n",
       " tensor(1.1148, grad_fn=<NllLossBackward>),\n",
       " tensor(1.1074, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0998, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0919, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0838, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0757, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0680, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0608, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0538, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0472, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0409, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0369, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0381, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0388, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0373, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0336, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0279, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0210, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0141, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0091, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0058, grad_fn=<NllLossBackward>),\n",
       " tensor(1.0026, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9985, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9931, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9865, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9789, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9711, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9639, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9576, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9518, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9456, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9384, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9303, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9218, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9137, grad_fn=<NllLossBackward>),\n",
       " tensor(0.9059, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8981, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8900, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8814, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8724, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8632, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8542, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8454, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8367, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8279, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8189, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8098, grad_fn=<NllLossBackward>),\n",
       " tensor(0.8008, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7921, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7837, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7754, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7671, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7589, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7507, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7426, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7346, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7269, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7194, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7122, grad_fn=<NllLossBackward>),\n",
       " tensor(0.7050, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6979, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6912, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6848, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6789, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6731, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6675, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6620, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6566, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6516, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6468, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6423, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6380, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6338, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6297, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6257, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6220, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6184, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6149, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6114, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6078, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6043, grad_fn=<NllLossBackward>),\n",
       " tensor(0.6006, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5970, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5935, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5900, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5863, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5826, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5788, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5753, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5721, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5691, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5662, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5634, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5605, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5577, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5549, grad_fn=<NllLossBackward>),\n",
       " tensor(0.5521, grad_fn=<NllLossBackward>)]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
