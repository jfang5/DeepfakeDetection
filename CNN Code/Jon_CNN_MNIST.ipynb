{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn, \\\n",
    "    torch.nn.functional as F, \\\n",
    "    torch.optim as optim, \\\n",
    "    torch.autograd as autograd, \\\n",
    "    torch.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\cuda\\__init__.py:87: UserWarning: \n",
      "    Found GPU1 GeForce GTX 650 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device('cuda:0')\n",
    "cpu = torch.device('cpu')\n",
    "torch.cuda.get_device_name(device=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = torch.tensor(mnist.train_images()), torch.tensor(mnist.train_labels())\n",
    "X_test, y_test = torch.tensor(mnist.test_images()), torch.tensor(mnist.test_labels())\n",
    "\n",
    "X_train = (X_train.float()/255).unsqueeze(dim=1).to(gpu)\n",
    "X_test = (X_test.float()/255).unsqueeze(dim=1).to(gpu)\n",
    "\n",
    "y_train_onehot = F.one_hot(y_train.to(torch.int64)).float().to(gpu)\n",
    "y_test_onehot = F.one_hot(y_test.to(torch.int64)).float().to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([60000, 10])\n",
      "torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,'\\n',\n",
    "     X_test.shape,'\\n',\n",
    "     y_train_onehot.shape,'\\n',\n",
    "     y_test_onehot.shape, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train.to(cpu)[0][0], 'gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dataloader object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert len(X) == len(y), 'X, y have different lengths'\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchData(X_train, y_train_onehot)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Design network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [4, 8]\n",
    "pool_kernels = [3, 3]\n",
    "out_dim = 28 - 2*len(channels) - sum([i-1 for i in pool_kernels])\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Inheriting nn.Module to convert this to a pytorch NN\n",
    "        # All functionality of nn.Module; module base class keeps track of all weights\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = channels[0], kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = channels[0], out_channels = channels[1], kernel_size=3, stride=1)\n",
    "#         self.conv3 = nn.Conv2d(in_channels = channels[1], out_channels = channels[2], kernel_size=3, stride=1)\n",
    "#         self.conv4 = nn.Conv2d(in_channels = channels[2], out_channels = channels[3], kernel_size=3, stride=1)\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = pool_kernels[0], stride=1, padding=0, ceil_mode=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = pool_kernels[1], stride=1, padding=0, ceil_mode=True)\n",
    "#         self.pool3 = nn.MaxPool2d(kernel_size = pool_kernels[2], stride=1, padding=0, ceil_mode=True)\n",
    "#         self.pool4 = nn.MaxPool2d(kernel_size = pool_kernels[3], stride=1, padding=0, ceil_mode=True)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout2d(.25)\n",
    "        self.dropout2 = nn.Dropout2d(.25)\n",
    "#         self.dropout3 = nn.Dropout2d(.25)\n",
    "#         self.dropout4 = nn.Dropout2d(p=.25)\n",
    "        self.dropout_dense = nn.Dropout2d(.25)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm2d(channels[0])\n",
    "        self.batchnorm2 = nn.BatchNorm2d(channels[1])\n",
    "#         self.batchnorm3 = nn.BatchNorm2d(channels[2])\n",
    "#         self.batchnorm4 = nn.BatchNorm2d(channels[3])\n",
    "        \n",
    "        self.dense1 = nn.Linear(in_features = (channels[-1]*out_dim*out_dim), out_features = 32)\n",
    "        self.dense2 = nn.Linear(in_features=32, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.batchnorm1(self.dropout1(self.conv1(x)))))\n",
    "        x = self.pool2(F.relu(self.batchnorm2(self.dropout2(self.conv2(x)))))\n",
    "#         x = self.pool3(F.relu(self.batchnorm3(self.dropout3(self.conv3(x)))))\n",
    "#         x = self.pool4(F.relu(self.batchnorm4(self.dropout1(self.conv4(x)))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.dropout_dense(self.dense1(x)))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        output = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation A (MSE Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define loss function and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = model().to(gpu)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100 \n",
      "Epoch Loss: 0.08562730520963668 \n",
      "Total Loss: 0.08562730520963668 \n",
      "\n",
      "Epoch: 1, Batch: 200 \n",
      "Epoch Loss: 0.07593702983111143 \n",
      "Total Loss: 0.07593702983111143 \n",
      "\n",
      "Epoch: 1, Batch: 300 \n",
      "Epoch Loss: 0.07055833879858256 \n",
      "Total Loss: 0.07055833879858256 \n",
      "\n",
      "Epoch: 1, Batch: 400 \n",
      "Epoch Loss: 0.06647306262515486 \n",
      "Total Loss: 0.06647306262515486 \n",
      "\n",
      "Epoch: 1, Batch: 500 \n",
      "Epoch Loss: 0.06356062172353268 \n",
      "Total Loss: 0.06356062172353268 \n",
      "\n",
      "Epoch: 1, Batch: 600 \n",
      "Epoch Loss: 0.061036350329717 \n",
      "Total Loss: 0.061036350329717 \n",
      "\n",
      "Epoch: 1, Batch: 700 \n",
      "Epoch Loss: 0.05895128623183284 \n",
      "Total Loss: 0.05895128623183284 \n",
      "\n",
      "Epoch: 1, Batch: 800 \n",
      "Epoch Loss: 0.0572998766368255 \n",
      "Total Loss: 0.0572998766368255 \n",
      "\n",
      "Epoch: 1, Batch: 900 \n",
      "Epoch Loss: 0.05537891140000688 \n",
      "Total Loss: 0.05537891140000688 \n",
      "\n",
      "Epoch: 2, Batch: 100 \n",
      "Epoch Loss: 0.03644070323556661 \n",
      "Total Loss: 0.27417651682160793 \n",
      "\n",
      "Epoch: 2, Batch: 200 \n",
      "Epoch Loss: 0.03636531507596374 \n",
      "Total Loss: 0.1461607401398942 \n",
      "\n",
      "Epoch: 2, Batch: 300 \n",
      "Epoch Loss: 0.036478367944558464 \n",
      "Total Loss: 0.10355790570688744 \n",
      "\n",
      "Epoch: 2, Batch: 400 \n",
      "Epoch Loss: 0.03582084316760301 \n",
      "Total Loss: 0.08189946288475766 \n",
      "\n",
      "Epoch: 2, Batch: 500 \n",
      "Epoch Loss: 0.03549838170409202 \n",
      "Total Loss: 0.06894042389281094 \n",
      "\n",
      "Epoch: 2, Batch: 600 \n",
      "Epoch Loss: 0.03499879068074127 \n",
      "Total Loss: 0.06015875620767474 \n",
      "\n",
      "Epoch: 2, Batch: 700 \n",
      "Epoch Loss: 0.03468846696828093 \n",
      "Total Loss: 0.0539093999418297 \n",
      "\n",
      "Epoch: 2, Batch: 800 \n",
      "Epoch Loss: 0.03441455072723329 \n",
      "Total Loss: 0.049201796014094724 \n",
      "\n",
      "Epoch: 2, Batch: 900 \n",
      "Epoch Loss: 0.033967358263002505 \n",
      "Total Loss: 0.04542325304303732 \n",
      "\n",
      "Epoch: 3, Batch: 100 \n",
      "Epoch Loss: 0.030235820077359678 \n",
      "Total Loss: 0.2860693504350881 \n",
      "\n",
      "Epoch: 3, Batch: 200 \n",
      "Epoch Loss: 0.030570816639810802 \n",
      "Total Loss: 0.1481856440845877 \n",
      "\n",
      "Epoch: 3, Batch: 300 \n",
      "Epoch Loss: 0.030773048661649226 \n",
      "Total Loss: 0.10225459746809469 \n",
      "\n",
      "Epoch: 3, Batch: 400 \n",
      "Epoch Loss: 0.03048041193280369 \n",
      "Total Loss: 0.07915782324659328 \n",
      "\n",
      "Epoch: 3, Batch: 500 \n",
      "Epoch Loss: 0.03040804838016629 \n",
      "Total Loss: 0.06533416487524907 \n",
      "\n",
      "Epoch: 3, Batch: 600 \n",
      "Epoch Loss: 0.03010862254227201 \n",
      "Total Loss: 0.056034664804529816 \n",
      "\n",
      "Epoch: 3, Batch: 700 \n",
      "Epoch Loss: 0.02991294880530664 \n",
      "Total Loss: 0.049398232041192904 \n",
      "\n",
      "Epoch: 3, Batch: 800 \n",
      "Epoch Loss: 0.029820607320871204 \n",
      "Total Loss: 0.04443904540811976 \n",
      "\n",
      "Epoch: 3, Batch: 900 \n",
      "Epoch Loss: 0.029495682112044758 \n",
      "Total Loss: 0.04049753223097435 \n",
      "\n",
      "Epoch: 4, Batch: 100 \n",
      "Epoch Loss: 0.027409839164465665 \n",
      "Total Loss: 0.28257018273929135 \n",
      "\n",
      "Epoch: 4, Batch: 200 \n",
      "Epoch Loss: 0.027931947875767946 \n",
      "Total Loss: 0.14484184844302944 \n",
      "\n",
      "Epoch: 4, Batch: 300 \n",
      "Epoch Loss: 0.02803066322890421 \n",
      "Total Loss: 0.09891357345661769 \n",
      "\n",
      "Epoch: 4, Batch: 400 \n",
      "Epoch Loss: 0.027611958174966277 \n",
      "Total Loss: 0.0758324202807853 \n",
      "\n",
      "Epoch: 4, Batch: 500 \n",
      "Epoch Loss: 0.02757245635613799 \n",
      "Total Loss: 0.06203665867866948 \n",
      "\n",
      "Epoch: 4, Batch: 600 \n",
      "Epoch Loss: 0.027486805381874244 \n",
      "Total Loss: 0.05282465517016438 \n",
      "\n",
      "Epoch: 4, Batch: 700 \n",
      "Epoch Loss: 0.027322465806667293 \n",
      "Total Loss: 0.046218862587120386 \n",
      "\n",
      "Epoch: 4, Batch: 800 \n",
      "Epoch Loss: 0.027287413412705065 \n",
      "Total Loss: 0.041286568721698134 \n",
      "\n",
      "Epoch: 4, Batch: 900 \n",
      "Epoch Loss: 0.027032448367940054 \n",
      "Total Loss: 0.037393414641782224 \n",
      "\n",
      "Epoch: 5, Batch: 100 \n",
      "Epoch Loss: 0.024827153254300356 \n",
      "Total Loss: 0.276008331598714 \n",
      "\n",
      "Epoch: 5, Batch: 200 \n",
      "Epoch Loss: 0.02554867586120963 \n",
      "Total Loss: 0.1406311856461689 \n",
      "\n",
      "Epoch: 5, Batch: 300 \n",
      "Epoch Loss: 0.025628595678135753 \n",
      "Total Loss: 0.09547335278491179 \n",
      "\n",
      "Epoch: 5, Batch: 400 \n",
      "Epoch Loss: 0.02543426723917946 \n",
      "Total Loss: 0.07284757868479938 \n",
      "\n",
      "Epoch: 5, Batch: 500 \n",
      "Epoch Loss: 0.025490557704120873 \n",
      "Total Loss: 0.05930669173039496 \n",
      "\n",
      "Epoch: 5, Batch: 600 \n",
      "Epoch Loss: 0.025363934327227375 \n",
      "Total Loss: 0.05024660369008779 \n",
      "\n",
      "Epoch: 5, Batch: 700 \n",
      "Epoch Loss: 0.02529617310900773 \n",
      "Total Loss: 0.04377964904292354 \n",
      "\n",
      "Epoch: 5, Batch: 800 \n",
      "Epoch Loss: 0.02533081949688494 \n",
      "Total Loss: 0.03894652651785873 \n",
      "\n",
      "Epoch: 5, Batch: 900 \n",
      "Epoch Loss: 0.025129868974909188 \n",
      "Total Loss: 0.035141851678076715 \n",
      "\n",
      "Epoch: 6, Batch: 100 \n",
      "Epoch Loss: 0.02348991245031357 \n",
      "Total Loss: 0.2688894200247402 \n",
      "\n",
      "Epoch: 6, Batch: 200 \n",
      "Epoch Loss: 0.024124540612101553 \n",
      "Total Loss: 0.13650797407686088 \n",
      "\n",
      "Epoch: 6, Batch: 300 \n",
      "Epoch Loss: 0.02430573107674718 \n",
      "Total Loss: 0.09237576671824273 \n",
      "\n",
      "Epoch: 6, Batch: 400 \n",
      "Epoch Loss: 0.024187842982355504 \n",
      "Total Loss: 0.07027491581781457 \n",
      "\n",
      "Epoch: 6, Batch: 500 \n",
      "Epoch Loss: 0.024179100807756187 \n",
      "Total Loss: 0.05702473705789695 \n",
      "\n",
      "Epoch: 6, Batch: 600 \n",
      "Epoch Loss: 0.02410412402047465 \n",
      "Total Loss: 0.04817975977280488 \n",
      "\n",
      "Epoch: 6, Batch: 700 \n",
      "Epoch Loss: 0.024029078762978315 \n",
      "Total Loss: 0.041858337119975615 \n",
      "\n",
      "Epoch: 6, Batch: 800 \n",
      "Epoch Loss: 0.024076898059574886 \n",
      "Total Loss: 0.03713462067030681 \n",
      "\n",
      "Epoch: 6, Batch: 900 \n",
      "Epoch Loss: 0.02390332348230812 \n",
      "Total Loss: 0.03342549109331298 \n",
      "\n",
      "Epoch: 7, Batch: 100 \n",
      "Epoch Loss: 0.022511143181473018 \n",
      "Total Loss: 0.26224495700427464 \n",
      "\n",
      "Epoch: 7, Batch: 200 \n",
      "Epoch Loss: 0.022924123462289572 \n",
      "Total Loss: 0.13278941448378775 \n",
      "\n",
      "Epoch: 7, Batch: 300 \n",
      "Epoch Loss: 0.022937785039345425 \n",
      "Total Loss: 0.08961985290316599 \n",
      "\n",
      "Epoch: 7, Batch: 400 \n",
      "Epoch Loss: 0.022777196196839215 \n",
      "Total Loss: 0.06801115502270737 \n",
      "\n",
      "Epoch: 7, Batch: 500 \n",
      "Epoch Loss: 0.022834707161411644 \n",
      "Total Loss: 0.05506791690444308 \n",
      "\n",
      "Epoch: 7, Batch: 600 \n",
      "Epoch Loss: 0.022758804143716893 \n",
      "Total Loss: 0.04642277096930359 \n",
      "\n",
      "Epoch: 7, Batch: 700 \n",
      "Epoch Loss: 0.022788585975233998 \n",
      "Total Loss: 0.04025966648316505 \n",
      "\n",
      "Epoch: 7, Batch: 800 \n",
      "Epoch Loss: 0.022878997843945398 \n",
      "Total Loss: 0.03564706318928594 \n",
      "\n",
      "Epoch: 7, Batch: 900 \n",
      "Epoch Loss: 0.022791917983235585 \n",
      "Total Loss: 0.03203699710631063 \n",
      "\n",
      "Epoch: 8, Batch: 100 \n",
      "Epoch Loss: 0.02157117336988449 \n",
      "Total Loss: 0.2559427891683299 \n",
      "\n",
      "Epoch: 8, Batch: 200 \n",
      "Epoch Loss: 0.022193765826523305 \n",
      "Total Loss: 0.12939741697686258 \n",
      "\n",
      "Epoch: 8, Batch: 300 \n",
      "Epoch Loss: 0.02241145580696563 \n",
      "Total Loss: 0.08721689614156881 \n",
      "\n",
      "Epoch: 8, Batch: 400 \n",
      "Epoch Loss: 0.022185294029768557 \n",
      "Total Loss: 0.06608475987799466 \n",
      "\n",
      "Epoch: 8, Batch: 500 \n",
      "Epoch Loss: 0.0221840682644397 \n",
      "Total Loss: 0.053422287032473835 \n",
      "\n",
      "Epoch: 8, Batch: 600 \n",
      "Epoch Loss: 0.02208656371105462 \n",
      "Total Loss: 0.044968552546730885 \n",
      "\n",
      "Epoch: 8, Batch: 700 \n",
      "Epoch Loss: 0.0220600828129266 \n",
      "Total Loss: 0.038935566422629304 \n",
      "\n",
      "Epoch: 8, Batch: 800 \n",
      "Epoch Loss: 0.0221041518507991 \n",
      "Total Loss: 0.03441881804348668 \n",
      "\n",
      "Epoch: 8, Batch: 900 \n",
      "Epoch Loss: 0.021917110627724064 \n",
      "Total Loss: 0.030878126883698214 \n",
      "\n",
      "Epoch: 9, Batch: 100 \n",
      "Epoch Loss: 0.020911049908027054 \n",
      "Total Loss: 0.25015251907933916 \n",
      "\n",
      "Epoch: 9, Batch: 200 \n",
      "Epoch Loss: 0.021294346204958858 \n",
      "Total Loss: 0.12628057301199686 \n",
      "\n",
      "Epoch: 9, Batch: 300 \n",
      "Epoch Loss: 0.021397301762675246 \n",
      "Total Loss: 0.08498716767015005 \n",
      "\n",
      "Epoch: 9, Batch: 400 \n",
      "Epoch Loss: 0.02128031717147678 \n",
      "Total Loss: 0.06432174695810924 \n",
      "\n",
      "Epoch: 9, Batch: 500 \n",
      "Epoch Loss: 0.02131994678452611 \n",
      "Total Loss: 0.051934696793970135 \n",
      "\n",
      "Epoch: 9, Batch: 600 \n",
      "Epoch Loss: 0.021337168311389786 \n",
      "Total Loss: 0.04367564132730304 \n",
      "\n",
      "Epoch: 9, Batch: 700 \n",
      "Epoch Loss: 0.021285810437319534 \n",
      "Total Loss: 0.03776924277567083 \n",
      "\n",
      "Epoch: 9, Batch: 800 \n",
      "Epoch Loss: 0.02139577210182324 \n",
      "Total Loss: 0.03335594164750849 \n",
      "\n",
      "Epoch: 9, Batch: 900 \n",
      "Epoch Loss: 0.021205529171145626 \n",
      "Total Loss: 0.029892733140078225 \n",
      "\n",
      "Epoch: 10, Batch: 100 \n",
      "Epoch Loss: 0.020350260315462946 \n",
      "Total Loss: 0.2448521332461387 \n",
      "\n",
      "Epoch: 10, Batch: 200 \n",
      "Epoch Loss: 0.020674751498736443 \n",
      "Total Loss: 0.12347602875716984 \n",
      "\n",
      "Epoch: 10, Batch: 300 \n",
      "Epoch Loss: 0.020900539389501014 \n",
      "Total Loss: 0.08302908967714756 \n",
      "\n",
      "Epoch: 10, Batch: 400 \n",
      "Epoch Loss: 0.02068633888615295 \n",
      "Total Loss: 0.0627729106922634 \n",
      "\n",
      "Epoch: 10, Batch: 500 \n",
      "Epoch Loss: 0.020579745166003704 \n",
      "Total Loss: 0.05062139595951885 \n",
      "\n",
      "Epoch: 10, Batch: 600 \n",
      "Epoch Loss: 0.020567336568298438 \n",
      "Total Loss: 0.04252625152592858 \n",
      "\n",
      "Epoch: 10, Batch: 700 \n",
      "Epoch Loss: 0.020601828836702876 \n",
      "Total Loss: 0.036748341057183484 \n",
      "\n",
      "Epoch: 10, Batch: 800 \n",
      "Epoch Loss: 0.02067107004695572 \n",
      "Total Loss: 0.03241924540651962 \n",
      "\n",
      "Epoch: 10, Batch: 900 \n",
      "Epoch Loss: 0.02055615207594302 \n",
      "Total Loss: 0.029035293786993457 \n",
      "\n",
      "Epoch: 11, Batch: 100 \n",
      "Epoch Loss: 0.01902767867781222 \n",
      "Total Loss: 0.23991029606962747 \n",
      "\n",
      "Epoch: 11, Batch: 200 \n",
      "Epoch Loss: 0.019633036623708904 \n",
      "Total Loss: 0.1208750750607049 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Batch: 300 \n",
      "Epoch Loss: 0.019881258591388663 \n",
      "Total Loss: 0.08120088951097745 \n",
      "\n",
      "Epoch: 11, Batch: 400 \n",
      "Epoch Loss: 0.019762383261695503 \n",
      "Total Loss: 0.06134170707124709 \n",
      "\n",
      "Epoch: 11, Batch: 500 \n",
      "Epoch Loss: 0.019819039393216373 \n",
      "Total Loss: 0.049437832273712214 \n",
      "\n",
      "Epoch: 11, Batch: 600 \n",
      "Epoch Loss: 0.019804058127726117 \n",
      "Total Loss: 0.04149712010385525 \n",
      "\n",
      "Epoch: 11, Batch: 700 \n",
      "Epoch Loss: 0.019878739496426923 \n",
      "Total Loss: 0.03583294486445556 \n",
      "\n",
      "Epoch: 11, Batch: 800 \n",
      "Epoch Loss: 0.01998932843678631 \n",
      "Total Loss: 0.03158977506343614 \n",
      "\n",
      "Epoch: 11, Batch: 900 \n",
      "Epoch Loss: 0.01984323381860223 \n",
      "Total Loss: 0.028268431135914242 \n",
      "\n",
      "Epoch: 12, Batch: 100 \n",
      "Epoch Loss: 0.018772097621113062 \n",
      "Total Loss: 0.2353348688253512 \n",
      "\n",
      "Epoch: 12, Batch: 200 \n",
      "Epoch Loss: 0.019335021069273352 \n",
      "Total Loss: 0.11849651543423534 \n",
      "\n",
      "Epoch: 12, Batch: 300 \n",
      "Epoch Loss: 0.019462189295639594 \n",
      "Total Loss: 0.079545358226945 \n",
      "\n",
      "Epoch: 12, Batch: 400 \n",
      "Epoch Loss: 0.019300339496694505 \n",
      "Total Loss: 0.06005099346395582 \n",
      "\n",
      "Epoch: 12, Batch: 500 \n",
      "Epoch Loss: 0.019397968225181104 \n",
      "Total Loss: 0.04837060282348345 \n",
      "\n",
      "Epoch: 12, Batch: 600 \n",
      "Epoch Loss: 0.0194809183323135 \n",
      "Total Loss: 0.040585164420513645 \n",
      "\n",
      "Epoch: 12, Batch: 700 \n",
      "Epoch Loss: 0.01952705676135208 \n",
      "Total Loss: 0.03502304435253055 \n",
      "\n",
      "Epoch: 12, Batch: 800 \n",
      "Epoch Loss: 0.019668573657982052 \n",
      "Total Loss: 0.030860363724447476 \n",
      "\n",
      "Epoch: 12, Batch: 900 \n",
      "Epoch Loss: 0.019571495336583918 \n",
      "Total Loss: 0.02760546098437367 \n",
      "\n",
      "Epoch: 13, Batch: 100 \n",
      "Epoch Loss: 0.018616882693022488 \n",
      "Total Loss: 0.23126116090812363 \n",
      "\n",
      "Epoch: 13, Batch: 200 \n",
      "Epoch Loss: 0.019266393687576055 \n",
      "Total Loss: 0.11639657678798987 \n",
      "\n",
      "Epoch: 13, Batch: 300 \n",
      "Epoch Loss: 0.019221880476300916 \n",
      "Total Loss: 0.07808830386003814 \n",
      "\n",
      "Epoch: 13, Batch: 400 \n",
      "Epoch Loss: 0.019029383705928923 \n",
      "Total Loss: 0.058921071998775006 \n",
      "\n",
      "Epoch: 13, Batch: 500 \n",
      "Epoch Loss: 0.019045575702562927 \n",
      "Total Loss: 0.047430862886544606 \n",
      "\n",
      "Epoch: 13, Batch: 600 \n",
      "Epoch Loss: 0.018984253434464336 \n",
      "Total Loss: 0.039765176022043214 \n",
      "\n",
      "Epoch: 13, Batch: 700 \n",
      "Epoch Loss: 0.01900150435710592 \n",
      "Total Loss: 0.034294381753981605 \n",
      "\n",
      "Epoch: 13, Batch: 800 \n",
      "Epoch Loss: 0.019157613125862553 \n",
      "Total Loss: 0.030202299174225816 \n",
      "\n",
      "Epoch: 13, Batch: 900 \n",
      "Epoch Loss: 0.01899828091574212 \n",
      "Total Loss: 0.026997972114139006 \n",
      "\n",
      "Epoch: 14, Batch: 100 \n",
      "Epoch Loss: 0.018361742682754992 \n",
      "Total Loss: 0.22739602402850453 \n",
      "\n",
      "Epoch: 14, Batch: 200 \n",
      "Epoch Loss: 0.018642293722368777 \n",
      "Total Loss: 0.11437382789860878 \n",
      "\n",
      "Epoch: 14, Batch: 300 \n",
      "Epoch Loss: 0.018732073654731115 \n",
      "Total Loss: 0.07669949558763099 \n",
      "\n",
      "Epoch: 14, Batch: 400 \n",
      "Epoch Loss: 0.018587285329122096 \n",
      "Total Loss: 0.05784878098272851 \n",
      "\n",
      "Epoch: 14, Batch: 500 \n",
      "Epoch Loss: 0.018658634575083852 \n",
      "Total Loss: 0.04654965380845325 \n",
      "\n",
      "Epoch: 14, Batch: 600 \n",
      "Epoch Loss: 0.018616607623795668 \n",
      "Total Loss: 0.039010502850703364 \n",
      "\n",
      "Epoch: 14, Batch: 700 \n",
      "Epoch Loss: 0.018639210313558578 \n",
      "Total Loss: 0.03362915373378794 \n",
      "\n",
      "Epoch: 14, Batch: 800 \n",
      "Epoch Loss: 0.01878015634487383 \n",
      "Total Loss: 0.0296019986113866 \n",
      "\n",
      "Epoch: 14, Batch: 900 \n",
      "Epoch Loss: 0.018617629670641487 \n",
      "Total Loss: 0.026450327466286363 \n",
      "\n",
      "Epoch: 15, Batch: 100 \n",
      "Epoch Loss: 0.017777632493525743 \n",
      "Total Loss: 0.22377306028828026 \n",
      "\n",
      "Epoch: 15, Batch: 200 \n",
      "Epoch Loss: 0.018284226451069117 \n",
      "Total Loss: 0.11251289082442721 \n",
      "\n",
      "Epoch: 15, Batch: 300 \n",
      "Epoch Loss: 0.01842187650501728 \n",
      "Total Loss: 0.07542408669657177 \n",
      "\n",
      "Epoch: 15, Batch: 400 \n",
      "Epoch Loss: 0.018185870645102114 \n",
      "Total Loss: 0.056859362573518105 \n",
      "\n",
      "Epoch: 15, Batch: 500 \n",
      "Epoch Loss: 0.01831245370581746 \n",
      "Total Loss: 0.04573840720479687 \n",
      "\n",
      "Epoch: 15, Batch: 600 \n",
      "Epoch Loss: 0.01826933560272058 \n",
      "Total Loss: 0.03831593650496668 \n",
      "\n",
      "Epoch: 15, Batch: 700 \n",
      "Epoch Loss: 0.01824817248620093 \n",
      "Total Loss: 0.03301481408794366 \n",
      "\n",
      "Epoch: 15, Batch: 800 \n",
      "Epoch Loss: 0.018341171264182776 \n",
      "Total Loss: 0.0290462303495345 \n",
      "\n",
      "Epoch: 15, Batch: 900 \n",
      "Epoch Loss: 0.018249767834527624 \n",
      "Total Loss: 0.02594863838771427 \n",
      "\n",
      "Epoch: 16, Batch: 100 \n",
      "Epoch Loss: 0.017139922538772227 \n",
      "Total Loss: 0.2203854515281273 \n",
      "\n",
      "Epoch: 16, Batch: 200 \n",
      "Epoch Loss: 0.01782253132201731 \n",
      "Total Loss: 0.1107710113923531 \n",
      "\n",
      "Epoch: 16, Batch: 300 \n",
      "Epoch Loss: 0.017980031274879973 \n",
      "Total Loss: 0.07422848741116468 \n",
      "\n",
      "Epoch: 16, Batch: 400 \n",
      "Epoch Loss: 0.017846785478759557 \n",
      "Total Loss: 0.055943975684785985 \n",
      "\n",
      "Epoch: 16, Batch: 500 \n",
      "Epoch Loss: 0.017947078021243214 \n",
      "Total Loss: 0.04498453365021851 \n",
      "\n",
      "Epoch: 16, Batch: 600 \n",
      "Epoch Loss: 0.017973462650552392 \n",
      "Total Loss: 0.037675709143901864 \n",
      "\n",
      "Epoch: 16, Batch: 700 \n",
      "Epoch Loss: 0.017966801991154042 \n",
      "Total Loss: 0.032453526034369136 \n",
      "\n",
      "Epoch: 16, Batch: 800 \n",
      "Epoch Loss: 0.018040635696379467 \n",
      "Total Loss: 0.028541815527205472 \n",
      "\n",
      "Epoch: 16, Batch: 900 \n",
      "Epoch Loss: 0.01796306276176539 \n",
      "Total Loss: 0.025490936574771896 \n",
      "\n",
      "Epoch: 17, Batch: 100 \n",
      "Epoch Loss: 0.017270858129486442 \n",
      "Total Loss: 0.2173019205343307 \n",
      "\n",
      "Epoch: 17, Batch: 200 \n",
      "Epoch Loss: 0.017712500265333803 \n",
      "Total Loss: 0.10918490563190597 \n",
      "\n",
      "Epoch: 17, Batch: 300 \n",
      "Epoch Loss: 0.017764744334854185 \n",
      "Total Loss: 0.07314031419526859 \n",
      "\n",
      "Epoch: 17, Batch: 400 \n",
      "Epoch Loss: 0.017472260562935844 \n",
      "Total Loss: 0.05509927695890998 \n",
      "\n",
      "Epoch: 17, Batch: 500 \n",
      "Epoch Loss: 0.01759688889887184 \n",
      "Total Loss: 0.04429230865233523 \n",
      "\n",
      "Epoch: 17, Batch: 600 \n",
      "Epoch Loss: 0.017509273496301225 \n",
      "Total Loss: 0.037077621881685716 \n",
      "\n",
      "Epoch: 17, Batch: 700 \n",
      "Epoch Loss: 0.017525221521167884 \n",
      "Total Loss: 0.03192889362691017 \n",
      "\n",
      "Epoch: 17, Batch: 800 \n",
      "Epoch Loss: 0.017641168040572665 \n",
      "Total Loss: 0.028073464229990564 \n",
      "\n",
      "Epoch: 17, Batch: 900 \n",
      "Epoch Loss: 0.017511388132762578 \n",
      "Total Loss: 0.025061858066333324 \n",
      "\n",
      "Epoch: 18, Batch: 100 \n",
      "Epoch Loss: 0.016743399831466377 \n",
      "Total Loss: 0.21427316461224108 \n",
      "\n",
      "Epoch: 18, Batch: 200 \n",
      "Epoch Loss: 0.01728357872227207 \n",
      "Total Loss: 0.1076316866842616 \n",
      "\n",
      "Epoch: 18, Batch: 300 \n",
      "Epoch Loss: 0.017351900598344704 \n",
      "Total Loss: 0.07207831972192422 \n",
      "\n",
      "Epoch: 18, Batch: 400 \n",
      "Epoch Loss: 0.017066725859185682 \n",
      "Total Loss: 0.054283895369800224 \n",
      "\n",
      "Epoch: 18, Batch: 500 \n",
      "Epoch Loss: 0.017203685370273888 \n",
      "Total Loss: 0.043624355444891585 \n",
      "\n",
      "Epoch: 18, Batch: 600 \n",
      "Epoch Loss: 0.017178432291839272 \n",
      "Total Loss: 0.03651151997166582 \n",
      "\n",
      "Epoch: 18, Batch: 700 \n",
      "Epoch Loss: 0.01714348550021116 \n",
      "Total Loss: 0.031429983822939295 \n",
      "\n",
      "Epoch: 18, Batch: 800 \n",
      "Epoch Loss: 0.01721559990372043 \n",
      "Total Loss: 0.027624294183462756 \n",
      "\n",
      "Epoch: 18, Batch: 900 \n",
      "Epoch Loss: 0.017113735219981108 \n",
      "Total Loss: 0.024655538149189526 \n",
      "\n",
      "Epoch: 19, Batch: 100 \n",
      "Epoch Loss: 0.0165326162148267 \n",
      "Total Loss: 0.21136735147723046 \n",
      "\n",
      "Epoch: 19, Batch: 200 \n",
      "Epoch Loss: 0.017200286681763828 \n",
      "Total Loss: 0.10615388513726526 \n",
      "\n",
      "Epoch: 19, Batch: 300 \n",
      "Epoch Loss: 0.017202353129784267 \n",
      "Total Loss: 0.0710711249340685 \n",
      "\n",
      "Epoch: 19, Batch: 400 \n",
      "Epoch Loss: 0.016846873403992504 \n",
      "Total Loss: 0.05351098099300687 \n",
      "\n",
      "Epoch: 19, Batch: 500 \n",
      "Epoch Loss: 0.01690940784290433 \n",
      "Total Loss: 0.04298941159017972 \n",
      "\n",
      "Epoch: 19, Batch: 600 \n",
      "Epoch Loss: 0.016892611622655145 \n",
      "Total Loss: 0.03597195378586388 \n",
      "\n",
      "Epoch: 19, Batch: 700 \n",
      "Epoch Loss: 0.01683705110169415 \n",
      "Total Loss: 0.030957191124544443 \n",
      "\n",
      "Epoch: 19, Batch: 800 \n",
      "Epoch Loss: 0.01692708696529735 \n",
      "Total Loss: 0.027203051036677178 \n",
      "\n",
      "Epoch: 19, Batch: 900 \n",
      "Epoch Loss: 0.01683586868509236 \n",
      "Total Loss: 0.024274677660926218 \n",
      "\n",
      "Epoch: 20, Batch: 100 \n",
      "Epoch Loss: 0.015912145497277378 \n",
      "Total Loss: 0.20861382135143502 \n",
      "\n",
      "Epoch: 20, Batch: 200 \n",
      "Epoch Loss: 0.016337713694665582 \n",
      "Total Loss: 0.10472599272301886 \n",
      "\n",
      "Epoch: 20, Batch: 300 \n",
      "Epoch Loss: 0.016490558742855987 \n",
      "Total Loss: 0.07009726596266652 \n",
      "\n",
      "Epoch: 20, Batch: 400 \n",
      "Epoch Loss: 0.016291715116240085 \n",
      "Total Loss: 0.052769139274954795 \n",
      "\n",
      "Epoch: 20, Batch: 500 \n",
      "Epoch Loss: 0.016500337231904267 \n",
      "Total Loss: 0.04238865967690945 \n",
      "\n",
      "Epoch: 20, Batch: 600 \n",
      "Epoch Loss: 0.01644631513239195 \n",
      "Total Loss: 0.03545868476938146 \n",
      "\n",
      "Epoch: 20, Batch: 700 \n",
      "Epoch Loss: 0.016487678347953727 \n",
      "Total Loss: 0.03051270021405071 \n",
      "\n",
      "Epoch: 20, Batch: 800 \n",
      "Epoch Loss: 0.016591966178966685 \n",
      "Total Loss: 0.02680687506851973 \n",
      "\n",
      "Epoch: 20, Batch: 900 \n",
      "Epoch Loss: 0.01649971079909139 \n",
      "Total Loss: 0.023915898215129145 \n",
      "\n",
      "Epoch: 21, Batch: 100 \n",
      "Epoch Loss: 0.015751020852476358 \n",
      "Total Loss: 0.2059985134916912 \n",
      "\n",
      "Epoch: 21, Batch: 200 \n",
      "Epoch Loss: 0.016035786457359792 \n",
      "Total Loss: 0.10338784131875617 \n",
      "\n",
      "Epoch: 21, Batch: 300 \n",
      "Epoch Loss: 0.016365332528948784 \n",
      "Total Loss: 0.06919545650888707 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Batch: 400 \n",
      "Epoch Loss: 0.016169039299711585 \n",
      "Total Loss: 0.05208207047228435 \n",
      "\n",
      "Epoch: 21, Batch: 500 \n",
      "Epoch Loss: 0.016159675138071178 \n",
      "Total Loss: 0.04181920131584186 \n",
      "\n",
      "Epoch: 21, Batch: 600 \n",
      "Epoch Loss: 0.016047940747812392 \n",
      "Total Loss: 0.03497226513460249 \n",
      "\n",
      "Epoch: 21, Batch: 700 \n",
      "Epoch Loss: 0.016115303570404648 \n",
      "Total Loss: 0.030088604676638583 \n",
      "\n",
      "Epoch: 21, Batch: 800 \n",
      "Epoch Loss: 0.016298252200940624 \n",
      "Total Loss: 0.026432165357622408 \n",
      "\n",
      "Epoch: 21, Batch: 900 \n",
      "Epoch Loss: 0.016153116068906254 \n",
      "Total Loss: 0.02357458098991109 \n",
      "\n",
      "Epoch: 22, Batch: 100 \n",
      "Epoch Loss: 0.01538117203861475 \n",
      "Total Loss: 0.20345448197246613 \n",
      "\n",
      "Epoch: 22, Batch: 200 \n",
      "Epoch Loss: 0.016082462312188 \n",
      "Total Loss: 0.10210868990863674 \n",
      "\n",
      "Epoch: 22, Batch: 300 \n",
      "Epoch Loss: 0.016012881852996846 \n",
      "Total Loss: 0.06831297086234289 \n",
      "\n",
      "Epoch: 22, Batch: 400 \n",
      "Epoch Loss: 0.01585817292914726 \n",
      "Total Loss: 0.05140966048945715 \n",
      "\n",
      "Epoch: 22, Batch: 500 \n",
      "Epoch Loss: 0.015893180892802776 \n",
      "Total Loss: 0.041273484871087764 \n",
      "\n",
      "Epoch: 22, Batch: 600 \n",
      "Epoch Loss: 0.015739292832246673 \n",
      "Total Loss: 0.03450797869961455 \n",
      "\n",
      "Epoch: 22, Batch: 700 \n",
      "Epoch Loss: 0.01575244554983718 \n",
      "Total Loss: 0.029681068507821432 \n",
      "\n",
      "Epoch: 22, Batch: 800 \n",
      "Epoch Loss: 0.015901669109007345 \n",
      "Total Loss: 0.02606722036493011 \n",
      "\n",
      "Epoch: 22, Batch: 900 \n",
      "Epoch Loss: 0.015816916002788478 \n",
      "Total Loss: 0.023247321592832004 \n",
      "\n",
      "Epoch: 23, Batch: 100 \n",
      "Epoch Loss: 0.01489611622877419 \n",
      "Total Loss: 0.20099147854109659 \n",
      "\n",
      "Epoch: 23, Batch: 200 \n",
      "Epoch Loss: 0.015619963025674224 \n",
      "Total Loss: 0.10085103948408251 \n",
      "\n",
      "Epoch: 23, Batch: 300 \n",
      "Epoch Loss: 0.015748707922175528 \n",
      "Total Loss: 0.0674660002026518 \n",
      "\n",
      "Epoch: 23, Batch: 400 \n",
      "Epoch Loss: 0.015500315665267408 \n",
      "Total Loss: 0.05075988209649475 \n",
      "\n",
      "Epoch: 23, Batch: 500 \n",
      "Epoch Loss: 0.01552667603455484 \n",
      "Total Loss: 0.04074383713381932 \n",
      "\n",
      "Epoch: 23, Batch: 600 \n",
      "Epoch Loss: 0.01561345773593833 \n",
      "Total Loss: 0.03406948287414549 \n",
      "\n",
      "Epoch: 23, Batch: 700 \n",
      "Epoch Loss: 0.01560688180516341 \n",
      "Total Loss: 0.02929910598045088 \n",
      "\n",
      "Epoch: 23, Batch: 800 \n",
      "Epoch Loss: 0.015732941043679602 \n",
      "Total Loss: 0.025727018579162413 \n",
      "\n",
      "Epoch: 23, Batch: 900 \n",
      "Epoch Loss: 0.01565099126814554 \n",
      "Total Loss: 0.02294090247164134 \n",
      "\n",
      "Epoch: 24, Batch: 100 \n",
      "Epoch Loss: 0.01534765211865306 \n",
      "Total Loss: 0.19870923655670292 \n",
      "\n",
      "Epoch: 24, Batch: 200 \n",
      "Epoch Loss: 0.015580362165346742 \n",
      "Total Loss: 0.09968405728276897 \n",
      "\n",
      "Epoch: 24, Batch: 300 \n",
      "Epoch Loss: 0.015672527640126645 \n",
      "Total Loss: 0.06667627233559162 \n",
      "\n",
      "Epoch: 24, Batch: 400 \n",
      "Epoch Loss: 0.015588218672201037 \n",
      "Total Loss: 0.05016694687428147 \n",
      "\n",
      "Epoch: 24, Batch: 500 \n",
      "Epoch Loss: 0.01564963522553444 \n",
      "Total Loss: 0.04026601834474908 \n",
      "\n",
      "Epoch: 24, Batch: 600 \n",
      "Epoch Loss: 0.015599091875677307 \n",
      "Total Loss: 0.03366158733677973 \n",
      "\n",
      "Epoch: 24, Batch: 700 \n",
      "Epoch Loss: 0.015528990040932384 \n",
      "Total Loss: 0.028942719973385378 \n",
      "\n",
      "Epoch: 24, Batch: 800 \n",
      "Epoch Loss: 0.01564434802450705 \n",
      "Total Loss: 0.02541056671582434 \n",
      "\n",
      "Epoch: 24, Batch: 900 \n",
      "Epoch Loss: 0.015554778925660583 \n",
      "Total Loss: 0.022655865905431307 \n",
      "\n",
      "Epoch: 25, Batch: 100 \n",
      "Epoch Loss: 0.014905447969213129 \n",
      "Total Loss: 0.1965394534541294 \n",
      "\n",
      "Epoch: 25, Batch: 200 \n",
      "Epoch Loss: 0.015208786830771714 \n",
      "Total Loss: 0.09857996924091131 \n",
      "\n",
      "Epoch: 25, Batch: 300 \n",
      "Epoch Loss: 0.015392910111695529 \n",
      "Total Loss: 0.06593012824958812 \n",
      "\n",
      "Epoch: 25, Batch: 400 \n",
      "Epoch Loss: 0.015290269658435137 \n",
      "Total Loss: 0.049597419670177625 \n",
      "\n",
      "Epoch: 25, Batch: 500 \n",
      "Epoch Loss: 0.015419611868448556 \n",
      "Total Loss: 0.039805431581810116 \n",
      "\n",
      "Epoch: 25, Batch: 600 \n",
      "Epoch Loss: 0.015346760040459534 \n",
      "Total Loss: 0.03327107632417853 \n",
      "\n",
      "Epoch: 25, Batch: 700 \n",
      "Epoch Loss: 0.015363204201151217 \n",
      "Total Loss: 0.02860641895881189 \n",
      "\n",
      "Epoch: 25, Batch: 800 \n",
      "Epoch Loss: 0.015510369435651228 \n",
      "Total Loss: 0.025113319219346158 \n",
      "\n",
      "Epoch: 25, Batch: 900 \n",
      "Epoch Loss: 0.015418276589674254 \n",
      "Total Loss: 0.022388201678627068 \n",
      "\n",
      "Epoch: 26, Batch: 100 \n",
      "Epoch Loss: 0.01428966426756233 \n",
      "Total Loss: 0.1944862951237995 \n",
      "\n",
      "Epoch: 26, Batch: 200 \n",
      "Epoch Loss: 0.01473305266816169 \n",
      "Total Loss: 0.09753500219783745 \n",
      "\n",
      "Epoch: 26, Batch: 300 \n",
      "Epoch Loss: 0.015062719726314148 \n",
      "Total Loss: 0.0652248995914124 \n",
      "\n",
      "Epoch: 26, Batch: 400 \n",
      "Epoch Loss: 0.014909670863999054 \n",
      "Total Loss: 0.04905762204237712 \n",
      "\n",
      "Epoch: 26, Batch: 500 \n",
      "Epoch Loss: 0.01502177659329027 \n",
      "Total Loss: 0.039365099168597505 \n",
      "\n",
      "Epoch: 26, Batch: 600 \n",
      "Epoch Loss: 0.014985191780918589 \n",
      "Total Loss: 0.03289913563869702 \n",
      "\n",
      "Epoch: 26, Batch: 700 \n",
      "Epoch Loss: 0.015024653789587318 \n",
      "Total Loss: 0.028283113107023822 \n",
      "\n",
      "Epoch: 26, Batch: 800 \n",
      "Epoch Loss: 0.015152487715822645 \n",
      "Total Loss: 0.024824874570566372 \n",
      "\n",
      "Epoch: 26, Batch: 900 \n",
      "Epoch Loss: 0.015057530081313518 \n",
      "Total Loss: 0.022127657178132674 \n",
      "\n",
      "Epoch: 27, Batch: 100 \n",
      "Epoch Loss: 0.014172620847821235 \n",
      "Total Loss: 0.19247344787922446 \n",
      "\n",
      "Epoch: 27, Batch: 200 \n",
      "Epoch Loss: 0.014703950074035674 \n",
      "Total Loss: 0.09651885874146872 \n",
      "\n",
      "Epoch: 27, Batch: 300 \n",
      "Epoch Loss: 0.014756865341526766 \n",
      "Total Loss: 0.0645293959001953 \n",
      "\n",
      "Epoch: 27, Batch: 400 \n",
      "Epoch Loss: 0.014521211233222857 \n",
      "Total Loss: 0.04852495663726047 \n",
      "\n",
      "Epoch: 27, Batch: 500 \n",
      "Epoch Loss: 0.014663252144120634 \n",
      "Total Loss: 0.03893279061193957 \n",
      "\n",
      "Epoch: 27, Batch: 600 \n",
      "Epoch Loss: 0.014707428287559499 \n",
      "Total Loss: 0.032536142232201214 \n",
      "\n",
      "Epoch: 27, Batch: 700 \n",
      "Epoch Loss: 0.014755005465288247 \n",
      "Total Loss: 0.02796770111189554 \n",
      "\n",
      "Epoch: 27, Batch: 800 \n",
      "Epoch Loss: 0.014888752335100435 \n",
      "Total Loss: 0.024545002271166863 \n",
      "\n",
      "Epoch: 27, Batch: 900 \n",
      "Epoch Loss: 0.014816168093950385 \n",
      "Total Loss: 0.021876362077106144 \n",
      "\n",
      "Epoch: 28, Batch: 100 \n",
      "Epoch Loss: 0.01428535684943199 \n",
      "Total Loss: 0.19053903602030395 \n",
      "\n",
      "Epoch: 28, Batch: 200 \n",
      "Epoch Loss: 0.014664172765333206 \n",
      "Total Loss: 0.09553814280803116 \n",
      "\n",
      "Epoch: 28, Batch: 300 \n",
      "Epoch Loss: 0.01482803805731237 \n",
      "Total Loss: 0.06387252102251209 \n",
      "\n",
      "Epoch: 28, Batch: 400 \n",
      "Epoch Loss: 0.01472952990909107 \n",
      "Total Loss: 0.0480332658156736 \n",
      "\n",
      "Epoch: 28, Batch: 500 \n",
      "Epoch Loss: 0.014790085735730827 \n",
      "Total Loss: 0.038533986288555235 \n",
      "\n",
      "Epoch: 28, Batch: 600 \n",
      "Epoch Loss: 0.014764917636445413 \n",
      "Total Loss: 0.032198792604391374 \n",
      "\n",
      "Epoch: 28, Batch: 700 \n",
      "Epoch Loss: 0.014830728473940066 \n",
      "Total Loss: 0.027676646688962546 \n",
      "\n",
      "Epoch: 28, Batch: 800 \n",
      "Epoch Loss: 0.014902886145864613 \n",
      "Total Loss: 0.02428585152181248 \n",
      "\n",
      "Epoch: 28, Batch: 900 \n",
      "Epoch Loss: 0.014769856767315003 \n",
      "Total Loss: 0.021641810962797277 \n",
      "\n",
      "Epoch: 29, Batch: 100 \n",
      "Epoch Loss: 0.0144688755646348 \n",
      "Total Loss: 0.1887198750696402 \n",
      "\n",
      "Epoch: 29, Batch: 200 \n",
      "Epoch Loss: 0.01483100889250636 \n",
      "Total Loss: 0.09462188826275765 \n",
      "\n",
      "Epoch: 29, Batch: 300 \n",
      "Epoch Loss: 0.014760685795918107 \n",
      "Total Loss: 0.06324930527405385 \n",
      "\n",
      "Epoch: 29, Batch: 400 \n",
      "Epoch Loss: 0.014582067930605262 \n",
      "Total Loss: 0.047558067010149585 \n",
      "\n",
      "Epoch: 29, Batch: 500 \n",
      "Epoch Loss: 0.014600759012624621 \n",
      "Total Loss: 0.038147664113917616 \n",
      "\n",
      "Epoch: 29, Batch: 600 \n",
      "Epoch Loss: 0.014583691670559347 \n",
      "Total Loss: 0.031873043974013145 \n",
      "\n",
      "Epoch: 29, Batch: 700 \n",
      "Epoch Loss: 0.01461359948651599 \n",
      "Total Loss: 0.027392624127391837 \n",
      "\n",
      "Epoch: 29, Batch: 800 \n",
      "Epoch Loss: 0.01476400975836441 \n",
      "Total Loss: 0.024036722325525197 \n",
      "\n",
      "Epoch: 29, Batch: 900 \n",
      "Epoch Loss: 0.014685027276993626 \n",
      "Total Loss: 0.02141981895382327 \n",
      "\n",
      "Epoch: 30, Batch: 100 \n",
      "Epoch Loss: 0.014060924616642297 \n",
      "Total Loss: 0.18697419502469712 \n",
      "\n",
      "Epoch: 30, Batch: 200 \n",
      "Epoch Loss: 0.014632344767451287 \n",
      "Total Loss: 0.09374049359431956 \n",
      "\n",
      "Epoch: 30, Batch: 300 \n",
      "Epoch Loss: 0.014890945963561535 \n",
      "Total Loss: 0.06266486404461062 \n",
      "\n",
      "Epoch: 30, Batch: 400 \n",
      "Epoch Loss: 0.014699264550581575 \n",
      "Total Loss: 0.04711634986938831 \n",
      "\n",
      "Epoch: 30, Batch: 500 \n",
      "Epoch Loss: 0.014757168523967266 \n",
      "Total Loss: 0.03779300512496071 \n",
      "\n",
      "Epoch: 30, Batch: 600 \n",
      "Epoch Loss: 0.014692721917914847 \n",
      "Total Loss: 0.03157400698684311 \n",
      "\n",
      "Epoch: 30, Batch: 700 \n",
      "Epoch Loss: 0.014676866182791336 \n",
      "Total Loss: 0.027132871378113384 \n",
      "\n",
      "Epoch: 30, Batch: 800 \n",
      "Epoch Loss: 0.014742046511964873 \n",
      "Total Loss: 0.02380458874258329 \n",
      "\n",
      "Epoch: 30, Batch: 900 \n",
      "Epoch Loss: 0.014621286031065715 \n",
      "Total Loss: 0.021210209260755047 \n",
      "\n",
      "Epoch: 31, Batch: 100 \n",
      "Epoch Loss: 0.013967839665710927 \n",
      "Total Loss: 0.18534060246483874 \n",
      "\n",
      "Epoch: 31, Batch: 200 \n",
      "Epoch Loss: 0.014492940532509237 \n",
      "Total Loss: 0.09291252770660174 \n",
      "\n",
      "Epoch: 31, Batch: 300 \n",
      "Epoch Loss: 0.01462409924560537 \n",
      "Total Loss: 0.06210175413420544 \n",
      "\n",
      "Epoch: 31, Batch: 400 \n",
      "Epoch Loss: 0.014351423030020669 \n",
      "Total Loss: 0.046685455877938487 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Batch: 500 \n",
      "Epoch Loss: 0.014483622153289617 \n",
      "Total Loss: 0.03744521901619831 \n",
      "\n",
      "Epoch: 31, Batch: 600 \n",
      "Epoch Loss: 0.014455382369148234 \n",
      "Total Loss: 0.031281307155694514 \n",
      "\n",
      "Epoch: 31, Batch: 700 \n",
      "Epoch Loss: 0.014453606279566884 \n",
      "Total Loss: 0.02687910636267861 \n",
      "\n",
      "Epoch: 31, Batch: 800 \n",
      "Epoch Loss: 0.014558981549344025 \n",
      "Total Loss: 0.023580897940076784 \n",
      "\n",
      "Epoch: 31, Batch: 900 \n",
      "Epoch Loss: 0.014429801714399622 \n",
      "Total Loss: 0.021008813807074863 \n",
      "\n",
      "Epoch: 32, Batch: 100 \n",
      "Epoch Loss: 0.013744338061660527 \n",
      "Total Loss: 0.18374931592588836 \n",
      "\n",
      "Epoch: 32, Batch: 200 \n",
      "Epoch Loss: 0.014524659926537424 \n",
      "Total Loss: 0.09211379830343504 \n",
      "\n",
      "Epoch: 32, Batch: 300 \n",
      "Epoch Loss: 0.014551559234969319 \n",
      "Total Loss: 0.06156133801324662 \n",
      "\n",
      "Epoch: 32, Batch: 400 \n",
      "Epoch Loss: 0.014258413420757279 \n",
      "Total Loss: 0.046275526759764034 \n",
      "\n",
      "Epoch: 32, Batch: 500 \n",
      "Epoch Loss: 0.014356448165141046 \n",
      "Total Loss: 0.037112600077452956 \n",
      "\n",
      "Epoch: 32, Batch: 600 \n",
      "Epoch Loss: 0.014232050147062789 \n",
      "Total Loss: 0.030998052460672623 \n",
      "\n",
      "Epoch: 32, Batch: 700 \n",
      "Epoch Loss: 0.014291517832981689 \n",
      "Total Loss: 0.026635153555346604 \n",
      "\n",
      "Epoch: 32, Batch: 800 \n",
      "Epoch Loss: 0.014418759749969468 \n",
      "Total Loss: 0.02336556191236923 \n",
      "\n",
      "Epoch: 32, Batch: 900 \n",
      "Epoch Loss: 0.01429158724999676 \n",
      "Total Loss: 0.020815479363947006 \n",
      "\n",
      "Epoch: 33, Batch: 100 \n",
      "Epoch Loss: 0.013223836054094136 \n",
      "Total Loss: 0.1822039338810172 \n",
      "\n",
      "Epoch: 33, Batch: 200 \n",
      "Epoch Loss: 0.013756286853458732 \n",
      "Total Loss: 0.09131846296555136 \n",
      "\n",
      "Epoch: 33, Batch: 300 \n",
      "Epoch Loss: 0.014068582006730139 \n",
      "Total Loss: 0.061027391192319834 \n",
      "\n",
      "Epoch: 33, Batch: 400 \n",
      "Epoch Loss: 0.013823669744888321 \n",
      "Total Loss: 0.04586970197726535 \n",
      "\n",
      "Epoch: 33, Batch: 500 \n",
      "Epoch Loss: 0.013991652609780431 \n",
      "Total Loss: 0.03678463178829318 \n",
      "\n",
      "Epoch: 33, Batch: 600 \n",
      "Epoch Loss: 0.013959415630282213 \n",
      "Total Loss: 0.03072354785758165 \n",
      "\n",
      "Epoch: 33, Batch: 700 \n",
      "Epoch Loss: 0.013998233165725002 \n",
      "Total Loss: 0.026396076251859513 \n",
      "\n",
      "Epoch: 33, Batch: 800 \n",
      "Epoch Loss: 0.01415224396274425 \n",
      "Total Loss: 0.023154257324702374 \n",
      "\n",
      "Epoch: 33, Batch: 900 \n",
      "Epoch Loss: 0.014061205211198993 \n",
      "Total Loss: 0.0206264539694285 \n",
      "\n",
      "Epoch: 34, Batch: 100 \n",
      "Epoch Loss: 0.013922294154763221 \n",
      "Total Loss: 0.18071509060203372 \n",
      "\n",
      "Epoch: 34, Batch: 200 \n",
      "Epoch Loss: 0.014300008355639875 \n",
      "Total Loss: 0.0905733941621421 \n",
      "\n",
      "Epoch: 34, Batch: 300 \n",
      "Epoch Loss: 0.014305049801866214 \n",
      "Total Loss: 0.060522607212941 \n",
      "\n",
      "Epoch: 34, Batch: 400 \n",
      "Epoch Loss: 0.014136062571778893 \n",
      "Total Loss: 0.04549216938677573 \n",
      "\n",
      "Epoch: 34, Batch: 500 \n",
      "Epoch Loss: 0.014297750627622008 \n",
      "Total Loss: 0.036481644349720545 \n",
      "\n",
      "Epoch: 34, Batch: 600 \n",
      "Epoch Loss: 0.01425711755718415 \n",
      "Total Loss: 0.030470262214007296 \n",
      "\n",
      "Epoch: 34, Batch: 700 \n",
      "Epoch Loss: 0.014176453802335474 \n",
      "Total Loss: 0.026174899003910637 \n",
      "\n",
      "Epoch: 34, Batch: 800 \n",
      "Epoch Loss: 0.014337784705567173 \n",
      "Total Loss: 0.022959900970437208 \n",
      "\n",
      "Epoch: 34, Batch: 900 \n",
      "Epoch Loss: 0.014214595720275409 \n",
      "Total Loss: 0.02045203316273484 \n",
      "\n",
      "Epoch: 35, Batch: 100 \n",
      "Epoch Loss: 0.013369967751204968 \n",
      "Total Loss: 0.17932228294320937 \n",
      "\n",
      "Epoch: 35, Batch: 200 \n",
      "Epoch Loss: 0.014044744670391083 \n",
      "Total Loss: 0.08987142035145579 \n",
      "\n",
      "Epoch: 35, Batch: 300 \n",
      "Epoch Loss: 0.014194688987142096 \n",
      "Total Loss: 0.060052323830690944 \n",
      "\n",
      "Epoch: 35, Batch: 400 \n",
      "Epoch Loss: 0.013950939804199152 \n",
      "Total Loss: 0.04513366924627085 \n",
      "\n",
      "Epoch: 35, Batch: 500 \n",
      "Epoch Loss: 0.014038361947517843 \n",
      "Total Loss: 0.03618915282856407 \n",
      "\n",
      "Epoch: 35, Batch: 600 \n",
      "Epoch Loss: 0.013996302536300693 \n",
      "Total Loss: 0.030223275002280604 \n",
      "\n",
      "Epoch: 35, Batch: 700 \n",
      "Epoch Loss: 0.013956169443138476 \n",
      "Total Loss: 0.025961645393318744 \n",
      "\n",
      "Epoch: 35, Batch: 800 \n",
      "Epoch Loss: 0.014137920202629176 \n",
      "Total Loss: 0.022771476060293415 \n",
      "\n",
      "Epoch: 35, Batch: 900 \n",
      "Epoch Loss: 0.014016339450091538 \n",
      "Total Loss: 0.02028272060416491 \n",
      "\n",
      "Epoch: 36, Batch: 100 \n",
      "Epoch Loss: 0.013099008044227957 \n",
      "Total Loss: 0.17795923510779782 \n",
      "\n",
      "Epoch: 36, Batch: 200 \n",
      "Epoch Loss: 0.0133103893045336 \n",
      "Total Loss: 0.08916741992285501 \n",
      "\n",
      "Epoch: 36, Batch: 300 \n",
      "Epoch Loss: 0.013756382383095721 \n",
      "Total Loss: 0.05958057965727575 \n",
      "\n",
      "Epoch: 36, Batch: 400 \n",
      "Epoch Loss: 0.013523554370040073 \n",
      "Total Loss: 0.04477449773136565 \n",
      "\n",
      "Epoch: 36, Batch: 500 \n",
      "Epoch Loss: 0.01366962440032512 \n",
      "Total Loss: 0.03589878654354511 \n",
      "\n",
      "Epoch: 36, Batch: 600 \n",
      "Epoch Loss: 0.01356844448018819 \n",
      "Total Loss: 0.029976130197766772 \n",
      "\n",
      "Epoch: 36, Batch: 700 \n",
      "Epoch Loss: 0.01366576603613794 \n",
      "Total Loss: 0.02575037229400579 \n",
      "\n",
      "Epoch: 36, Batch: 800 \n",
      "Epoch Loss: 0.013747329026809894 \n",
      "Total Loss: 0.02258129197239921 \n",
      "\n",
      "Epoch: 36, Batch: 900 \n",
      "Epoch Loss: 0.013679570871301823 \n",
      "Total Loss: 0.02011280738789571 \n",
      "\n",
      "Epoch: 37, Batch: 100 \n",
      "Epoch Loss: 0.012762711667455733 \n",
      "Total Loss: 0.17658611678839284 \n",
      "\n",
      "Epoch: 37, Batch: 200 \n",
      "Epoch Loss: 0.013610435584560036 \n",
      "Total Loss: 0.08848843892800269 \n",
      "\n",
      "Epoch: 37, Batch: 300 \n",
      "Epoch Loss: 0.014024953028808037 \n",
      "Total Loss: 0.05912611232963517 \n",
      "\n",
      "Epoch: 37, Batch: 400 \n",
      "Epoch Loss: 0.013839805234456435 \n",
      "Total Loss: 0.04443434344892503 \n",
      "\n",
      "Epoch: 37, Batch: 500 \n",
      "Epoch Loss: 0.01381240816321224 \n",
      "Total Loss: 0.035621544055779136 \n",
      "\n",
      "Epoch: 37, Batch: 600 \n",
      "Epoch Loss: 0.013773199917438129 \n",
      "Total Loss: 0.02974577841895364 \n",
      "\n",
      "Epoch: 37, Batch: 700 \n",
      "Epoch Loss: 0.013693750202655793 \n",
      "Total Loss: 0.025547412590431157 \n",
      "\n",
      "Epoch: 37, Batch: 800 \n",
      "Epoch Loss: 0.013768797552911565 \n",
      "Total Loss: 0.022402276992994498 \n",
      "\n",
      "Epoch: 37, Batch: 900 \n",
      "Epoch Loss: 0.013664289294328127 \n",
      "Total Loss: 0.01995165829775385 \n",
      "\n",
      "Epoch: 38, Batch: 100 \n",
      "Epoch Loss: 0.012630221266299486 \n",
      "Total Loss: 0.17528465476983815 \n",
      "\n",
      "Epoch: 38, Batch: 200 \n",
      "Epoch Loss: 0.013247990731615573 \n",
      "Total Loss: 0.0878247715980366 \n",
      "\n",
      "Epoch: 38, Batch: 300 \n",
      "Epoch Loss: 0.013474238778774936 \n",
      "Total Loss: 0.05867201207301645 \n",
      "\n",
      "Epoch: 38, Batch: 400 \n",
      "Epoch Loss: 0.013262357953935862 \n",
      "Total Loss: 0.044087079551337456 \n",
      "\n",
      "Epoch: 38, Batch: 500 \n",
      "Epoch Loss: 0.013451705211773515 \n",
      "Total Loss: 0.035344448347612724 \n",
      "\n",
      "Epoch: 38, Batch: 600 \n",
      "Epoch Loss: 0.013476367046435674 \n",
      "Total Loss: 0.029513354659062123 \n",
      "\n",
      "Epoch: 38, Batch: 700 \n",
      "Epoch Loss: 0.013449755617018257 \n",
      "Total Loss: 0.025347123869573978 \n",
      "\n",
      "Epoch: 38, Batch: 800 \n",
      "Epoch Loss: 0.013574358972837217 \n",
      "Total Loss: 0.02222625503871792 \n",
      "\n",
      "Epoch: 38, Batch: 900 \n",
      "Epoch Loss: 0.013516510558935503 \n",
      "Total Loss: 0.019794839926953127 \n",
      "\n",
      "Epoch: 39, Batch: 100 \n",
      "Epoch Loss: 0.012915113242343068 \n",
      "Total Loss: 0.17402829743044165 \n",
      "\n",
      "Epoch: 39, Batch: 200 \n",
      "Epoch Loss: 0.013321435446850955 \n",
      "Total Loss: 0.08719014560818696 \n",
      "\n",
      "Epoch: 39, Batch: 300 \n",
      "Epoch Loss: 0.0135568132220457 \n",
      "Total Loss: 0.05824665748898306 \n",
      "\n",
      "Epoch: 39, Batch: 400 \n",
      "Epoch Loss: 0.013323301039636136 \n",
      "Total Loss: 0.043765908273739905 \n",
      "\n",
      "Epoch: 39, Batch: 500 \n",
      "Epoch Loss: 0.013514094844460487 \n",
      "Total Loss: 0.03508594338854966 \n",
      "\n",
      "Epoch: 39, Batch: 600 \n",
      "Epoch Loss: 0.013471114436009278 \n",
      "Total Loss: 0.0292949366374399 \n",
      "\n",
      "Epoch: 39, Batch: 700 \n",
      "Epoch Loss: 0.013494319345669022 \n",
      "Total Loss: 0.025159885428441626 \n",
      "\n",
      "Epoch: 39, Batch: 800 \n",
      "Epoch Loss: 0.013642068532062694 \n",
      "Total Loss: 0.022061939214106995 \n",
      "\n",
      "Epoch: 39, Batch: 900 \n",
      "Epoch Loss: 0.013614456285722554 \n",
      "Total Loss: 0.0196487709205595 \n",
      "\n",
      "Epoch: 40, Batch: 100 \n",
      "Epoch Loss: 0.01215871449559927 \n",
      "Total Loss: 0.17283351188048254 \n",
      "\n",
      "Epoch: 40, Batch: 200 \n",
      "Epoch Loss: 0.012956212828867138 \n",
      "Total Loss: 0.08658867732976797 \n",
      "\n",
      "Epoch: 40, Batch: 300 \n",
      "Epoch Loss: 0.01331215310220917 \n",
      "Total Loss: 0.057842651833586084 \n",
      "\n",
      "Epoch: 40, Batch: 400 \n",
      "Epoch Loss: 0.01317752571310848 \n",
      "Total Loss: 0.04346182414735085 \n",
      "\n",
      "Epoch: 40, Batch: 500 \n",
      "Epoch Loss: 0.013268767674453556 \n",
      "Total Loss: 0.034837627995479854 \n",
      "\n",
      "Epoch: 40, Batch: 600 \n",
      "Epoch Loss: 0.013229056123333673 \n",
      "Total Loss: 0.02908565040609877 \n",
      "\n",
      "Epoch: 40, Batch: 700 \n",
      "Epoch Loss: 0.01326436518358865 \n",
      "Total Loss: 0.024978686846460083 \n",
      "\n",
      "Epoch: 40, Batch: 800 \n",
      "Epoch Loss: 0.013406793737667613 \n",
      "Total Loss: 0.02190136284570326 \n",
      "\n",
      "Epoch: 40, Batch: 900 \n",
      "Epoch Loss: 0.013314168656555315 \n",
      "Total Loss: 0.0195028035517575 \n",
      "\n",
      "Epoch: 41, Batch: 100 \n",
      "Epoch Loss: 0.012736340081319212 \n",
      "Total Loss: 0.17165578625596514 \n",
      "\n",
      "Epoch: 41, Batch: 200 \n",
      "Epoch Loss: 0.013334073458099737 \n",
      "Total Loss: 0.08599779321133477 \n",
      "\n",
      "Epoch: 41, Batch: 300 \n",
      "Epoch Loss: 0.0135903696444196 \n",
      "Total Loss: 0.05744652036867082 \n",
      "\n",
      "Epoch: 41, Batch: 400 \n",
      "Epoch Loss: 0.013373790111509153 \n",
      "Total Loss: 0.04316247595645908 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Batch: 500 \n",
      "Epoch Loss: 0.013432567716110498 \n",
      "Total Loss: 0.03459665236582343 \n",
      "\n",
      "Epoch: 41, Batch: 600 \n",
      "Epoch Loss: 0.01342495329095982 \n",
      "Total Loss: 0.028884961854304923 \n",
      "\n",
      "Epoch: 41, Batch: 700 \n",
      "Epoch Loss: 0.013469759087144796 \n",
      "Total Loss: 0.024806408397293606 \n",
      "\n",
      "Epoch: 41, Batch: 800 \n",
      "Epoch Loss: 0.01356330170208821 \n",
      "Total Loss: 0.021748955213505967 \n",
      "\n",
      "Epoch: 41, Batch: 900 \n",
      "Epoch Loss: 0.013509608838034587 \n",
      "Total Loss: 0.01936785196735925 \n",
      "\n",
      "Epoch: 42, Batch: 100 \n",
      "Epoch Loss: 0.013070436539128422 \n",
      "Total Loss: 0.17057394791045227 \n",
      "\n",
      "Epoch: 42, Batch: 200 \n",
      "Epoch Loss: 0.013476349092088639 \n",
      "Total Loss: 0.08545223897481005 \n",
      "\n",
      "Epoch: 42, Batch: 300 \n",
      "Epoch Loss: 0.013649798769814273 \n",
      "Total Loss: 0.057079244222296106 \n",
      "\n",
      "Epoch: 42, Batch: 400 \n",
      "Epoch Loss: 0.013525205057812855 \n",
      "Total Loss: 0.04288771545197094 \n",
      "\n",
      "Epoch: 42, Batch: 500 \n",
      "Epoch Loss: 0.01348577475361526 \n",
      "Total Loss: 0.03437363928318068 \n",
      "\n",
      "Epoch: 42, Batch: 600 \n",
      "Epoch Loss: 0.013437191746973743 \n",
      "Total Loss: 0.02869705764357821 \n",
      "\n",
      "Epoch: 42, Batch: 700 \n",
      "Epoch Loss: 0.013421319482981094 \n",
      "Total Loss: 0.024642804802995695 \n",
      "\n",
      "Epoch: 42, Batch: 800 \n",
      "Epoch Loss: 0.0134999680519104 \n",
      "Total Loss: 0.021604271190818897 \n",
      "\n",
      "Epoch: 42, Batch: 900 \n",
      "Epoch Loss: 0.013441930326840115 \n",
      "Total Loss: 0.019238128964659863 \n",
      "\n",
      "Epoch: 43, Batch: 100 \n",
      "Epoch Loss: 0.012814080961979925 \n",
      "Total Loss: 0.16950972833237501 \n",
      "\n",
      "Epoch: 43, Batch: 200 \n",
      "Epoch Loss: 0.013330729107838123 \n",
      "Total Loss: 0.08491588018076537 \n",
      "\n",
      "Epoch: 43, Batch: 300 \n",
      "Epoch Loss: 0.01344626075743387 \n",
      "Total Loss: 0.056716612555057735 \n",
      "\n",
      "Epoch: 43, Batch: 400 \n",
      "Epoch Loss: 0.013182862903922796 \n",
      "Total Loss: 0.042609509819452544 \n",
      "\n",
      "Epoch: 43, Batch: 500 \n",
      "Epoch Loss: 0.013273487824946643 \n",
      "Total Loss: 0.03415103105327851 \n",
      "\n",
      "Epoch: 43, Batch: 600 \n",
      "Epoch Loss: 0.013184551475181555 \n",
      "Total Loss: 0.028508571884423392 \n",
      "\n",
      "Epoch: 43, Batch: 700 \n",
      "Epoch Loss: 0.013206344015696751 \n",
      "Total Loss: 0.024480228057940277 \n",
      "\n",
      "Epoch: 43, Batch: 800 \n",
      "Epoch Loss: 0.013337971638829913 \n",
      "Total Loss: 0.021461651193141817 \n",
      "\n",
      "Epoch: 43, Batch: 900 \n",
      "Epoch Loss: 0.013227384926285595 \n",
      "Total Loss: 0.019108916541774462 \n",
      "\n",
      "Epoch: 44, Batch: 100 \n",
      "Epoch Loss: 0.012393596991896629 \n",
      "Total Loss: 0.16844495550522962 \n",
      "\n",
      "Epoch: 44, Batch: 200 \n",
      "Epoch Loss: 0.013103944887407125 \n",
      "Total Loss: 0.0843794583524207 \n",
      "\n",
      "Epoch: 44, Batch: 300 \n",
      "Epoch Loss: 0.013189799708003798 \n",
      "Total Loss: 0.05635419579062286 \n",
      "\n",
      "Epoch: 44, Batch: 400 \n",
      "Epoch Loss: 0.01301472082035616 \n",
      "Total Loss: 0.04233660982113427 \n",
      "\n",
      "Epoch: 44, Batch: 500 \n",
      "Epoch Loss: 0.013200016500428319 \n",
      "Total Loss: 0.03393265694427431 \n",
      "\n",
      "Epoch: 44, Batch: 600 \n",
      "Epoch Loss: 0.013126571682902674 \n",
      "Total Loss: 0.028325544982331904 \n",
      "\n",
      "Epoch: 44, Batch: 700 \n",
      "Epoch Loss: 0.013094098657768751 \n",
      "Total Loss: 0.024320919272216194 \n",
      "\n",
      "Epoch: 44, Batch: 800 \n",
      "Epoch Loss: 0.013214220640948042 \n",
      "Total Loss: 0.021320733552175543 \n",
      "\n",
      "Epoch: 44, Batch: 900 \n",
      "Epoch Loss: 0.013119284799839887 \n",
      "Total Loss: 0.01898297476877971 \n",
      "\n",
      "Epoch: 45, Batch: 100 \n",
      "Epoch Loss: 0.012091226736083626 \n",
      "Total Loss: 0.1674097906431804 \n",
      "\n",
      "Epoch: 45, Batch: 200 \n",
      "Epoch Loss: 0.013096648941282183 \n",
      "Total Loss: 0.08386158500099555 \n",
      "\n",
      "Epoch: 45, Batch: 300 \n",
      "Epoch Loss: 0.013147924571918945 \n",
      "Total Loss: 0.056005875006835494 \n",
      "\n",
      "Epoch: 45, Batch: 400 \n",
      "Epoch Loss: 0.013004575439845211 \n",
      "Total Loss: 0.042074264744257864 \n",
      "\n",
      "Epoch: 45, Batch: 500 \n",
      "Epoch Loss: 0.013048860428389162 \n",
      "Total Loss: 0.0337181940193288 \n",
      "\n",
      "Epoch: 45, Batch: 600 \n",
      "Epoch Loss: 0.013042080640249575 \n",
      "Total Loss: 0.028146673466846416 \n",
      "\n",
      "Epoch: 45, Batch: 700 \n",
      "Epoch Loss: 0.013054879104518996 \n",
      "Total Loss: 0.02416740795536085 \n",
      "\n",
      "Epoch: 45, Batch: 800 \n",
      "Epoch Loss: 0.013161264889931771 \n",
      "Total Loss: 0.02118510964257358 \n",
      "\n",
      "Epoch: 45, Batch: 900 \n",
      "Epoch Loss: 0.013132897885257585 \n",
      "Total Loss: 0.018863075143640377 \n",
      "\n",
      "Epoch: 46, Batch: 100 \n",
      "Epoch Loss: 0.01246959537267685 \n",
      "Total Loss: 0.16643952856179448 \n",
      "\n",
      "Epoch: 46, Batch: 200 \n",
      "Epoch Loss: 0.012963373840320855 \n",
      "Total Loss: 0.08336603767554904 \n",
      "\n",
      "Epoch: 46, Batch: 300 \n",
      "Epoch Loss: 0.013039662384738525 \n",
      "Total Loss: 0.055672954388580324 \n",
      "\n",
      "Epoch: 46, Batch: 400 \n",
      "Epoch Loss: 0.012832560036331415 \n",
      "Total Loss: 0.04182108129682171 \n",
      "\n",
      "Epoch: 46, Batch: 500 \n",
      "Epoch Loss: 0.012951596684753895 \n",
      "Total Loss: 0.03351524652997234 \n",
      "\n",
      "Epoch: 46, Batch: 600 \n",
      "Epoch Loss: 0.012970591873551408 \n",
      "Total Loss: 0.027976711122141947 \n",
      "\n",
      "Epoch: 46, Batch: 700 \n",
      "Epoch Loss: 0.012984667739032636 \n",
      "Total Loss: 0.024020625442990988 \n",
      "\n",
      "Epoch: 46, Batch: 800 \n",
      "Epoch Loss: 0.01308949519705493 \n",
      "Total Loss: 0.0210556105436041 \n",
      "\n",
      "Epoch: 46, Batch: 900 \n",
      "Epoch Loss: 0.013029985491496822 \n",
      "Total Loss: 0.018746421709887297 \n",
      "\n",
      "Epoch: 47, Batch: 100 \n",
      "Epoch Loss: 0.012110861288383602 \n",
      "Total Loss: 0.16547469719635086 \n",
      "\n",
      "Epoch: 47, Batch: 200 \n",
      "Epoch Loss: 0.012512984385248274 \n",
      "Total Loss: 0.08287474335862344 \n",
      "\n",
      "Epoch: 47, Batch: 300 \n",
      "Epoch Loss: 0.01279607082096239 \n",
      "Total Loss: 0.05534459659151059 \n",
      "\n",
      "Epoch: 47, Batch: 400 \n",
      "Epoch Loss: 0.01271398080396466 \n",
      "Total Loss: 0.04157476505402109 \n",
      "\n",
      "Epoch: 47, Batch: 500 \n",
      "Epoch Loss: 0.012901910598389804 \n",
      "Total Loss: 0.03331791259545556 \n",
      "\n",
      "Epoch: 47, Batch: 600 \n",
      "Epoch Loss: 0.012763907983123015 \n",
      "Total Loss: 0.027807742393045547 \n",
      "\n",
      "Epoch: 47, Batch: 700 \n",
      "Epoch Loss: 0.01280257529845195 \n",
      "Total Loss: 0.02387482654720143 \n",
      "\n",
      "Epoch: 47, Batch: 800 \n",
      "Epoch Loss: 0.012976255334215238 \n",
      "Total Loss: 0.020928217951100607 \n",
      "\n",
      "Epoch: 47, Batch: 900 \n",
      "Epoch Loss: 0.012918777141927016 \n",
      "Total Loss: 0.018632314187275293 \n",
      "\n",
      "Epoch: 48, Batch: 100 \n",
      "Epoch Loss: 0.012174750682897867 \n",
      "Total Loss: 0.16454255500798656 \n",
      "\n",
      "Epoch: 48, Batch: 200 \n",
      "Epoch Loss: 0.012593065662076696 \n",
      "Total Loss: 0.08240681271900636 \n",
      "\n",
      "Epoch: 48, Batch: 300 \n",
      "Epoch Loss: 0.012813371456383418 \n",
      "Total Loss: 0.05502991669492783 \n",
      "\n",
      "Epoch: 48, Batch: 400 \n",
      "Epoch Loss: 0.012788146815146319 \n",
      "Total Loss: 0.04133864831750543 \n",
      "\n",
      "Epoch: 48, Batch: 500 \n",
      "Epoch Loss: 0.012815222133416683 \n",
      "Total Loss: 0.03312476666819809 \n",
      "\n",
      "Epoch: 48, Batch: 600 \n",
      "Epoch Loss: 0.012778406004654243 \n",
      "Total Loss: 0.027647702519890217 \n",
      "\n",
      "Epoch: 48, Batch: 700 \n",
      "Epoch Loss: 0.012789285181955035 \n",
      "Total Loss: 0.02373628835111352 \n",
      "\n",
      "Epoch: 48, Batch: 800 \n",
      "Epoch Loss: 0.01295041049219435 \n",
      "Total Loss: 0.020805914514682323 \n",
      "\n",
      "Epoch: 48, Batch: 900 \n",
      "Epoch Loss: 0.012838769911840145 \n",
      "Total Loss: 0.018521798191914397 \n",
      "\n",
      "Epoch: 49, Batch: 100 \n",
      "Epoch Loss: 0.012203810545615852 \n",
      "Total Loss: 0.16362467303625022 \n",
      "\n",
      "Epoch: 49, Batch: 200 \n",
      "Epoch Loss: 0.012795269014313817 \n",
      "Total Loss: 0.08194893577815585 \n",
      "\n",
      "Epoch: 49, Batch: 300 \n",
      "Epoch Loss: 0.012967914302522938 \n",
      "Total Loss: 0.05472318987168853 \n",
      "\n",
      "Epoch: 49, Batch: 400 \n",
      "Epoch Loss: 0.012692053383216263 \n",
      "Total Loss: 0.04110292541716077 \n",
      "\n",
      "Epoch: 49, Batch: 500 \n",
      "Epoch Loss: 0.012844619972631335 \n",
      "Total Loss: 0.03293725823711756 \n",
      "\n",
      "Epoch: 49, Batch: 600 \n",
      "Epoch Loss: 0.012782641398565223 \n",
      "Total Loss: 0.027490139512319853 \n",
      "\n",
      "Epoch: 49, Batch: 700 \n",
      "Epoch Loss: 0.012812646771781146 \n",
      "Total Loss: 0.023600856255490126 \n",
      "\n",
      "Epoch: 49, Batch: 800 \n",
      "Epoch Loss: 0.01297646580147557 \n",
      "Total Loss: 0.02068677779245523 \n",
      "\n",
      "Epoch: 49, Batch: 900 \n",
      "Epoch Loss: 0.012889996747900215 \n",
      "Total Loss: 0.01841590734458446 \n",
      "\n",
      "Epoch: 50, Batch: 100 \n",
      "Epoch Loss: 0.012184678893536329 \n",
      "Total Loss: 0.1627511353035923 \n",
      "\n",
      "Epoch: 50, Batch: 200 \n",
      "Epoch Loss: 0.01276038080919534 \n",
      "Total Loss: 0.0815089284790447 \n",
      "\n",
      "Epoch: 50, Batch: 300 \n",
      "Epoch Loss: 0.01271103243653973 \n",
      "Total Loss: 0.05442336789063799 \n",
      "\n",
      "Epoch: 50, Batch: 400 \n",
      "Epoch Loss: 0.012525272241909989 \n",
      "Total Loss: 0.0408773658762686 \n",
      "\n",
      "Epoch: 50, Batch: 500 \n",
      "Epoch Loss: 0.012688738284166902 \n",
      "Total Loss: 0.03275526311082765 \n",
      "\n",
      "Epoch: 50, Batch: 600 \n",
      "Epoch Loss: 0.012648956044965113 \n",
      "Total Loss: 0.0273375527418529 \n",
      "\n",
      "Epoch: 50, Batch: 700 \n",
      "Epoch Loss: 0.012671252897208822 \n",
      "Total Loss: 0.023468773875904403 \n",
      "\n",
      "Epoch: 50, Batch: 800 \n",
      "Epoch Loss: 0.012809011380595621 \n",
      "Total Loss: 0.02056961044332711 \n",
      "\n",
      "Epoch: 50, Batch: 900 \n",
      "Epoch Loss: 0.012670316382621725 \n",
      "Total Loss: 0.0183097887416215 \n",
      "\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1} \\n',f'Epoch Loss: {epoch_loss/(i+1)} \\n',\n",
    "                  f'Total Loss: {total_loss/((i+1)*(epoch+1))} \\n', sep='')\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation B (Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define loss function and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = model().to(gpu)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn2.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100 \n",
      "Epoch Loss: 2.0997710359096526 \n",
      "Total Loss: 2.0997710359096526 \n",
      "\n",
      "Epoch: 1, Batch: 200 \n",
      "Epoch Loss: 1.9590588796138764 \n",
      "Total Loss: 1.9590588796138764 \n",
      "\n",
      "Epoch: 1, Batch: 300 \n",
      "Epoch Loss: 1.8169571097691855 \n",
      "Total Loss: 1.8169571097691855 \n",
      "\n",
      "Epoch: 1, Batch: 400 \n",
      "Epoch Loss: 1.7041906115412713 \n",
      "Total Loss: 1.7041906115412713 \n",
      "\n",
      "Epoch: 1, Batch: 500 \n",
      "Epoch Loss: 1.61875761449337 \n",
      "Total Loss: 1.61875761449337 \n",
      "\n",
      "Epoch: 1, Batch: 600 \n",
      "Epoch Loss: 1.5461543948451677 \n",
      "Total Loss: 1.5461543948451677 \n",
      "\n",
      "Epoch: 1, Batch: 700 \n",
      "Epoch Loss: 1.4828795105218888 \n",
      "Total Loss: 1.4828795105218888 \n",
      "\n",
      "Epoch: 1, Batch: 800 \n",
      "Epoch Loss: 1.4217936097085475 \n",
      "Total Loss: 1.4217936097085475 \n",
      "\n",
      "Epoch: 1, Batch: 900 \n",
      "Epoch Loss: 1.3624378943443298 \n",
      "Total Loss: 1.3624378943443298 \n",
      "\n",
      "Epoch: 2, Batch: 100 \n",
      "Epoch Loss: 0.801836286187172 \n",
      "Total Loss: 6.678309253305197 \n",
      "\n",
      "Epoch: 2, Batch: 200 \n",
      "Epoch Loss: 0.8056947042047977 \n",
      "Total Loss: 3.5415429072082043 \n",
      "\n",
      "Epoch: 2, Batch: 300 \n",
      "Epoch Loss: 0.7837616195281346 \n",
      "Total Loss: 2.4843445131679376 \n",
      "\n",
      "Epoch: 2, Batch: 400 \n",
      "Epoch Loss: 0.74452442497015 \n",
      "Total Loss: 1.9416099900379777 \n",
      "\n",
      "Epoch: 2, Batch: 500 \n",
      "Epoch Loss: 0.7196445455551147 \n",
      "Total Loss: 1.6153004948198795 \n",
      "\n",
      "Epoch: 2, Batch: 600 \n",
      "Epoch Loss: 0.6955802063643932 \n",
      "Total Loss: 1.3940219548841317 \n",
      "\n",
      "Epoch: 2, Batch: 700 \n",
      "Epoch Loss: 0.6725198409387043 \n",
      "Total Loss: 1.233030079071011 \n",
      "\n",
      "Epoch: 2, Batch: 800 \n",
      "Epoch Loss: 0.6547517159581184 \n",
      "Total Loss: 1.1120497467555106 \n",
      "\n",
      "Epoch: 2, Batch: 900 \n",
      "Epoch Loss: 0.6341014672981368 \n",
      "Total Loss: 1.0145386347836918 \n",
      "\n",
      "Epoch: 3, Batch: 100 \n",
      "Epoch Loss: 0.4493460714817047 \n",
      "Total Loss: 6.287303974926472 \n",
      "\n",
      "Epoch: 3, Batch: 200 \n",
      "Epoch Loss: 0.46211455821990965 \n",
      "Total Loss: 3.2227991616229215 \n",
      "\n",
      "Epoch: 3, Batch: 300 \n",
      "Epoch Loss: 0.46060164834062256 \n",
      "Total Loss: 2.19937453314662 \n",
      "\n",
      "Epoch: 3, Batch: 400 \n",
      "Epoch Loss: 0.4542417880520225 \n",
      "Total Loss: 1.6857944171254833 \n",
      "\n",
      "Epoch: 3, Batch: 500 \n",
      "Epoch Loss: 0.45028385454416275 \n",
      "Total Loss: 1.3775990084012348 \n",
      "\n",
      "Epoch: 3, Batch: 600 \n",
      "Epoch Loss: 0.4448231235643228 \n",
      "Total Loss: 1.171194699704647 \n",
      "\n",
      "Epoch: 3, Batch: 700 \n",
      "Epoch Loss: 0.4429024652498109 \n",
      "Total Loss: 1.024423005240304 \n",
      "\n",
      "Epoch: 3, Batch: 800 \n",
      "Epoch Loss: 0.4414376103132963 \n",
      "Total Loss: 0.9143361139918367 \n",
      "\n",
      "Epoch: 3, Batch: 900 \n",
      "Epoch Loss: 0.43398509916332034 \n",
      "Total Loss: 0.8266085831765775 \n",
      "\n",
      "Epoch: 4, Batch: 100 \n",
      "Epoch Loss: 0.372039345651865 \n",
      "Total Loss: 5.700993330720812 \n",
      "\n",
      "Epoch: 4, Batch: 200 \n",
      "Epoch Loss: 0.38571400709450243 \n",
      "Total Loss: 2.9004202489275484 \n",
      "\n",
      "Epoch: 4, Batch: 300 \n",
      "Epoch Loss: 0.38744188989202183 \n",
      "Total Loss: 1.966188303908954 \n",
      "\n",
      "Epoch: 4, Batch: 400 \n",
      "Epoch Loss: 0.3824640408530831 \n",
      "Total Loss: 1.4976118837902321 \n",
      "\n",
      "Epoch: 4, Batch: 500 \n",
      "Epoch Loss: 0.3822415818572044 \n",
      "Total Loss: 1.2171570943258703 \n",
      "\n",
      "Epoch: 4, Batch: 600 \n",
      "Epoch Loss: 0.3801291196793318 \n",
      "Total Loss: 1.0296961956378072 \n",
      "\n",
      "Epoch: 4, Batch: 700 \n",
      "Epoch Loss: 0.3767752507541861 \n",
      "Total Loss: 0.8953343118753816 \n",
      "\n",
      "Epoch: 4, Batch: 800 \n",
      "Epoch Loss: 0.3753760690800846 \n",
      "Total Loss: 0.7948419540585019 \n",
      "\n",
      "Epoch: 4, Batch: 900 \n",
      "Epoch Loss: 0.3716203378968769 \n",
      "Total Loss: 0.7160143616195355 \n",
      "\n",
      "Epoch: 5, Batch: 100 \n",
      "Epoch Loss: 0.3425206668674946 \n",
      "Total Loss: 5.244945058628916 \n",
      "\n",
      "Epoch: 5, Batch: 200 \n",
      "Epoch Loss: 0.347307113558054 \n",
      "Total Loss: 2.65768188533932 \n",
      "\n",
      "Epoch: 5, Batch: 300 \n",
      "Epoch Loss: 0.3436793947219849 \n",
      "Total Loss: 1.7942161873628695 \n",
      "\n",
      "Epoch: 5, Batch: 400 \n",
      "Epoch Loss: 0.336272648293525 \n",
      "Total Loss: 1.3613647609725594 \n",
      "\n",
      "Epoch: 5, Batch: 500 \n",
      "Epoch Loss: 0.34065028043091294 \n",
      "Total Loss: 1.1034182411372662 \n",
      "\n",
      "Epoch: 5, Batch: 600 \n",
      "Epoch Loss: 0.33957955505698917 \n",
      "Total Loss: 0.9306560652206342 \n",
      "\n",
      "Epoch: 5, Batch: 700 \n",
      "Epoch Loss: 0.33900510591055666 \n",
      "Total Loss: 0.8072925819328853 \n",
      "\n",
      "Epoch: 5, Batch: 800 \n",
      "Epoch Loss: 0.33820394141599536 \n",
      "Total Loss: 0.7146959039401263 \n",
      "\n",
      "Epoch: 5, Batch: 900 \n",
      "Epoch Loss: 0.3348879484915071 \n",
      "Total Loss: 0.642137692504459 \n",
      "\n",
      "Epoch: 6, Batch: 100 \n",
      "Epoch Loss: 0.3078116822987795 \n",
      "Total Loss: 4.883544124575953 \n",
      "\n",
      "Epoch: 6, Batch: 200 \n",
      "Epoch Loss: 0.31809627562761306 \n",
      "Total Loss: 2.4691371347010134 \n",
      "\n",
      "Epoch: 6, Batch: 300 \n",
      "Epoch Loss: 0.3248755122472842 \n",
      "Total Loss: 1.6648933112165993 \n",
      "\n",
      "Epoch: 6, Batch: 400 \n",
      "Epoch Loss: 0.3216066604480147 \n",
      "Total Loss: 1.2616616544562083 \n",
      "\n",
      "Epoch: 6, Batch: 500 \n",
      "Epoch Loss: 0.32162883500754835 \n",
      "Total Loss: 1.0200532413398227 \n",
      "\n",
      "Epoch: 6, Batch: 600 \n",
      "Epoch Loss: 0.31890447344630957 \n",
      "Total Loss: 0.8585244418287443 \n",
      "\n",
      "Epoch: 6, Batch: 700 \n",
      "Epoch Loss: 0.3178743275787149 \n",
      "Total Loss: 0.7432993656716177 \n",
      "\n",
      "Epoch: 6, Batch: 800 \n",
      "Epoch Loss: 0.31909783819690346 \n",
      "Total Loss: 0.6572132452235867 \n",
      "\n",
      "Epoch: 6, Batch: 900 \n",
      "Epoch Loss: 0.3147283681564861 \n",
      "Total Loss: 0.5893705255289873 \n",
      "\n",
      "Epoch: 7, Batch: 100 \n",
      "Epoch Loss: 0.28237820744514464 \n",
      "Total Loss: 4.6006528980657455 \n",
      "\n",
      "Epoch: 7, Batch: 200 \n",
      "Epoch Loss: 0.2947228254377842 \n",
      "Total Loss: 2.3222598378493315 \n",
      "\n",
      "Epoch: 7, Batch: 300 \n",
      "Epoch Loss: 0.2954364676525195 \n",
      "Total Loss: 1.5623095943796492 \n",
      "\n",
      "Epoch: 7, Batch: 400 \n",
      "Epoch Loss: 0.2910595764219761 \n",
      "Total Loss: 1.1816582280251064 \n",
      "\n",
      "Epoch: 7, Batch: 500 \n",
      "Epoch Loss: 0.2917894102782011 \n",
      "Total Loss: 0.9537468322973166 \n",
      "\n",
      "Epoch: 7, Batch: 600 \n",
      "Epoch Loss: 0.28998403950283924 \n",
      "Total Loss: 0.8014784837150503 \n",
      "\n",
      "Epoch: 7, Batch: 700 \n",
      "Epoch Loss: 0.29181997597217557 \n",
      "Total Loss: 0.6931618757310266 \n",
      "\n",
      "Epoch: 7, Batch: 800 \n",
      "Epoch Loss: 0.29391576278954745 \n",
      "Total Loss: 0.6120271103809188 \n",
      "\n",
      "Epoch: 7, Batch: 900 \n",
      "Epoch Loss: 0.29081202091442215 \n",
      "Total Loss: 0.5482460359880139 \n",
      "\n",
      "Epoch: 8, Batch: 100 \n",
      "Epoch Loss: 0.2647798107564449 \n",
      "Total Loss: 4.361287196418271 \n",
      "\n",
      "Epoch: 8, Batch: 200 \n",
      "Epoch Loss: 0.28006512723863125 \n",
      "Total Loss: 2.1991030009416863 \n",
      "\n",
      "Epoch: 8, Batch: 300 \n",
      "Epoch Loss: 0.28331572954853373 \n",
      "Total Loss: 1.478144372884805 \n",
      "\n",
      "Epoch: 8, Batch: 400 \n",
      "Epoch Loss: 0.2792433259636164 \n",
      "Total Loss: 1.1169528457638807 \n",
      "\n",
      "Epoch: 8, Batch: 500 \n",
      "Epoch Loss: 0.28094522215425966 \n",
      "Total Loss: 0.9007560967840255 \n",
      "\n",
      "Epoch: 8, Batch: 600 \n",
      "Epoch Loss: 0.2814734559630354 \n",
      "Total Loss: 0.7565491353409985 \n",
      "\n",
      "Epoch: 8, Batch: 700 \n",
      "Epoch Loss: 0.27976973404841765 \n",
      "Total Loss: 0.65328403390944 \n",
      "\n",
      "Epoch: 8, Batch: 800 \n",
      "Epoch Loss: 0.2807565537840128 \n",
      "Total Loss: 0.576118284232216 \n",
      "\n",
      "Epoch: 8, Batch: 900 \n",
      "Epoch Loss: 0.27799327505545485 \n",
      "Total Loss: 0.5156591282790113 \n",
      "\n",
      "Epoch: 9, Batch: 100 \n",
      "Epoch Loss: 0.2567646791785955 \n",
      "Total Loss: 4.1622910153865815 \n",
      "\n",
      "Epoch: 9, Batch: 200 \n",
      "Epoch Loss: 0.2690158764272928 \n",
      "Total Loss: 2.096771456230846 \n",
      "\n",
      "Epoch: 9, Batch: 300 \n",
      "Epoch Loss: 0.268649273738265 \n",
      "Total Loss: 1.407770454833905 \n",
      "\n",
      "Epoch: 9, Batch: 400 \n",
      "Epoch Loss: 0.2657176188752055 \n",
      "Total Loss: 1.0629645815222628 \n",
      "\n",
      "Epoch: 9, Batch: 500 \n",
      "Epoch Loss: 0.2678073222488165 \n",
      "Total Loss: 0.8565086904565493 \n",
      "\n",
      "Epoch: 9, Batch: 600 \n",
      "Epoch Loss: 0.2676668383926153 \n",
      "Total Loss: 0.7187010275862283 \n",
      "\n",
      "Epoch: 9, Batch: 700 \n",
      "Epoch Loss: 0.2661601203467165 \n",
      "Total Loss: 0.6201107190750421 \n",
      "\n",
      "Epoch: 9, Batch: 800 \n",
      "Epoch Loss: 0.26659730336628856 \n",
      "Total Loss: 0.5463421234198742 \n",
      "\n",
      "Epoch: 9, Batch: 900 \n",
      "Epoch Loss: 0.264666493675775 \n",
      "Total Loss: 0.4887142333627483 \n",
      "\n",
      "Epoch: 10, Batch: 100 \n",
      "Epoch Loss: 0.24775435134768486 \n",
      "Total Loss: 3.990789815016091 \n",
      "\n",
      "Epoch: 10, Batch: 200 \n",
      "Epoch Loss: 0.2599169656634331 \n",
      "Total Loss: 2.0089988865070043 \n",
      "\n",
      "Epoch: 10, Batch: 300 \n",
      "Epoch Loss: 0.26098876987894376 \n",
      "Total Loss: 1.3481036702816684 \n",
      "\n",
      "Epoch: 10, Batch: 400 \n",
      "Epoch Loss: 0.25647789100185037 \n",
      "Total Loss: 1.0171513840705155 \n",
      "\n",
      "Epoch: 10, Batch: 500 \n",
      "Epoch Loss: 0.25813588041067126 \n",
      "Total Loss: 0.8190164640173316 \n",
      "\n",
      "Epoch: 10, Batch: 600 \n",
      "Epoch Loss: 0.25785501590619486 \n",
      "Total Loss: 0.6867878982375065 \n",
      "\n",
      "Epoch: 10, Batch: 700 \n",
      "Epoch Loss: 0.25840999435101236 \n",
      "Total Loss: 0.5924144822752901 \n",
      "\n",
      "Epoch: 10, Batch: 800 \n",
      "Epoch Loss: 0.261217570528388 \n",
      "Total Loss: 0.5218735545380041 \n",
      "\n",
      "Epoch: 10, Batch: 900 \n",
      "Epoch Loss: 0.25871273272153406 \n",
      "Total Loss: 0.46653953770341144 \n",
      "\n",
      "Epoch: 11, Batch: 100 \n",
      "Epoch Loss: 0.23516701333224774 \n",
      "Total Loss: 3.845863481712612 \n",
      "\n",
      "Epoch: 11, Batch: 200 \n",
      "Epoch Loss: 0.24617446724325417 \n",
      "Total Loss: 1.9346218281814997 \n",
      "\n",
      "Epoch: 11, Batch: 300 \n",
      "Epoch Loss: 0.24900981689492863 \n",
      "Total Loss: 1.297465476854281 \n",
      "\n",
      "Epoch: 11, Batch: 400 \n",
      "Epoch Loss: 0.24512382611632347 \n",
      "Total Loss: 0.9784051497720859 \n",
      "\n",
      "Epoch: 11, Batch: 500 \n",
      "Epoch Loss: 0.24671809202432632 \n",
      "Total Loss: 0.7873258499205112 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Batch: 600 \n",
      "Epoch Loss: 0.24742854225138824 \n",
      "Total Loss: 0.6599076142274972 \n",
      "\n",
      "Epoch: 11, Batch: 700 \n",
      "Epoch Loss: 0.24770517086344107 \n",
      "Total Loss: 0.568873603656501 \n",
      "\n",
      "Epoch: 11, Batch: 800 \n",
      "Epoch Loss: 0.25094713548198344 \n",
      "Total Loss: 0.500873958742754 \n",
      "\n",
      "Epoch: 11, Batch: 900 \n",
      "Epoch Loss: 0.2490140143616332 \n",
      "Total Loss: 0.44758037792688066 \n",
      "\n",
      "Epoch: 12, Batch: 100 \n",
      "Epoch Loss: 0.22140891961753367 \n",
      "Total Loss: 3.7170882211253047 \n",
      "\n",
      "Epoch: 12, Batch: 200 \n",
      "Epoch Loss: 0.23673139013350009 \n",
      "Total Loss: 1.86904635475638 \n",
      "\n",
      "Epoch: 12, Batch: 300 \n",
      "Epoch Loss: 0.23910222813487053 \n",
      "Total Loss: 1.2528043449525204 \n",
      "\n",
      "Epoch: 12, Batch: 400 \n",
      "Epoch Loss: 0.2342994378041476 \n",
      "Total Loss: 0.9441843226063065 \n",
      "\n",
      "Epoch: 12, Batch: 500 \n",
      "Epoch Loss: 0.23705354257673025 \n",
      "Total Loss: 0.7594819574461629 \n",
      "\n",
      "Epoch: 12, Batch: 600 \n",
      "Epoch Loss: 0.23870948203528922 \n",
      "Total Loss: 0.6363320364735814 \n",
      "\n",
      "Epoch: 12, Batch: 700 \n",
      "Epoch Loss: 0.23860660845147713 \n",
      "Total Loss: 0.5482606665838865 \n",
      "\n",
      "Epoch: 12, Batch: 800 \n",
      "Epoch Loss: 0.24091899789404125 \n",
      "Total Loss: 0.4824062678858172 \n",
      "\n",
      "Epoch: 12, Batch: 900 \n",
      "Epoch Loss: 0.23798418154733048 \n",
      "Total Loss: 0.43079173488714906 \n",
      "\n",
      "Epoch: 13, Batch: 100 \n",
      "Epoch Loss: 0.2259438482671976 \n",
      "Total Loss: 3.6011799451192985 \n",
      "\n",
      "Epoch: 13, Batch: 200 \n",
      "Epoch Loss: 0.2378822197765112 \n",
      "Total Loss: 1.8101984568398732 \n",
      "\n",
      "Epoch: 13, Batch: 300 \n",
      "Epoch Loss: 0.23581582076847554 \n",
      "Total Loss: 1.212739561553567 \n",
      "\n",
      "Epoch: 13, Batch: 400 \n",
      "Epoch Loss: 0.23313954303972423 \n",
      "Total Loss: 0.9138837232777419 \n",
      "\n",
      "Epoch: 13, Batch: 500 \n",
      "Epoch Loss: 0.23674708928912877 \n",
      "Total Loss: 0.7349712443804511 \n",
      "\n",
      "Epoch: 13, Batch: 600 \n",
      "Epoch Loss: 0.2375156912766397 \n",
      "Total Loss: 0.6155703793069682 \n",
      "\n",
      "Epoch: 13, Batch: 700 \n",
      "Epoch Loss: 0.23774931452636208 \n",
      "Total Loss: 0.530259787252398 \n",
      "\n",
      "Epoch: 13, Batch: 800 \n",
      "Epoch Loss: 0.2393843773333356 \n",
      "Total Loss: 0.46638913900913814 \n",
      "\n",
      "Epoch: 13, Batch: 900 \n",
      "Epoch Loss: 0.23633155066519976 \n",
      "Total Loss: 0.4163793110621409 \n",
      "\n",
      "Epoch: 14, Batch: 100 \n",
      "Epoch Loss: 0.21151820257306098 \n",
      "Total Loss: 3.5005546427864047 \n",
      "\n",
      "Epoch: 14, Batch: 200 \n",
      "Epoch Loss: 0.23077797077596188 \n",
      "Total Loss: 1.7592072406424475 \n",
      "\n",
      "Epoch: 14, Batch: 300 \n",
      "Epoch Loss: 0.2308633109430472 \n",
      "Total Loss: 1.1783056364110893 \n",
      "\n",
      "Epoch: 14, Batch: 400 \n",
      "Epoch Loss: 0.22771450005471705 \n",
      "Total Loss: 0.8876268713688478 \n",
      "\n",
      "Epoch: 14, Batch: 500 \n",
      "Epoch Loss: 0.2300541176944971 \n",
      "Total Loss: 0.7135216769272728 \n",
      "\n",
      "Epoch: 14, Batch: 600 \n",
      "Epoch Loss: 0.22892485853905478 \n",
      "Total Loss: 0.5972594755675111 \n",
      "\n",
      "Epoch: 14, Batch: 700 \n",
      "Epoch Loss: 0.22828569005110433 \n",
      "Total Loss: 0.5142270063958606 \n",
      "\n",
      "Epoch: 14, Batch: 800 \n",
      "Epoch Loss: 0.2299668233981356 \n",
      "Total Loss: 0.4521069766394794 \n",
      "\n",
      "Epoch: 14, Batch: 900 \n",
      "Epoch Loss: 0.22733709005431996 \n",
      "Total Loss: 0.4035101635470277 \n",
      "\n",
      "Epoch: 15, Batch: 100 \n",
      "Epoch Loss: 0.21726019337773322 \n",
      "Total Loss: 3.408804685138166 \n",
      "\n",
      "Epoch: 15, Batch: 200 \n",
      "Epoch Loss: 0.22685749914497136 \n",
      "Total Loss: 1.71228416939949 \n",
      "\n",
      "Epoch: 15, Batch: 300 \n",
      "Epoch Loss: 0.22260531701147557 \n",
      "Total Loss: 1.1462805785495374 \n",
      "\n",
      "Epoch: 15, Batch: 400 \n",
      "Epoch Loss: 0.220655815936625 \n",
      "Total Loss: 0.8632905557906876 \n",
      "\n",
      "Epoch: 15, Batch: 500 \n",
      "Epoch Loss: 0.22276749394834042 \n",
      "Total Loss: 0.6937153007124861 \n",
      "\n",
      "Epoch: 15, Batch: 600 \n",
      "Epoch Loss: 0.22226034300401806 \n",
      "Total Loss: 0.5805374682413207 \n",
      "\n",
      "Epoch: 15, Batch: 700 \n",
      "Epoch Loss: 0.22393999635641065 \n",
      "Total Loss: 0.49983228626847265 \n",
      "\n",
      "Epoch: 15, Batch: 800 \n",
      "Epoch Loss: 0.22599922304973005 \n",
      "Total Loss: 0.4393566989007716 \n",
      "\n",
      "Epoch: 15, Batch: 900 \n",
      "Epoch Loss: 0.22445601013799507 \n",
      "Total Loss: 0.39211047536990157 \n",
      "\n",
      "Epoch: 16, Batch: 100 \n",
      "Epoch Loss: 0.20943472988903522 \n",
      "Total Loss: 3.3259573193150573 \n",
      "\n",
      "Epoch: 16, Batch: 200 \n",
      "Epoch Loss: 0.22132695380598308 \n",
      "Total Loss: 1.6702667589613702 \n",
      "\n",
      "Epoch: 16, Batch: 300 \n",
      "Epoch Loss: 0.224790202130874 \n",
      "Total Loss: 1.1183386038655105 \n",
      "\n",
      "Epoch: 16, Batch: 400 \n",
      "Epoch Loss: 0.2214195312745869 \n",
      "Total Loss: 0.8420556328789098 \n",
      "\n",
      "Epoch: 16, Batch: 500 \n",
      "Epoch Loss: 0.22169976353645324 \n",
      "Total Loss: 0.6764297649604268 \n",
      "\n",
      "Epoch: 16, Batch: 600 \n",
      "Epoch Loss: 0.22200928963099917 \n",
      "Total Loss: 0.5660201887181029 \n",
      "\n",
      "Epoch: 16, Batch: 700 \n",
      "Epoch Loss: 0.22088422160595655 \n",
      "Total Loss: 0.48707207080708553 \n",
      "\n",
      "Epoch: 16, Batch: 800 \n",
      "Epoch Loss: 0.22247128867078572 \n",
      "Total Loss: 0.4280129116290482 \n",
      "\n",
      "Epoch: 16, Batch: 900 \n",
      "Epoch Loss: 0.22007966310613686 \n",
      "Total Loss: 0.3818513843549105 \n",
      "\n",
      "Epoch: 17, Batch: 100 \n",
      "Epoch Loss: 0.19981299314647913 \n",
      "Total Loss: 3.2500818910646965 \n",
      "\n",
      "Epoch: 17, Batch: 200 \n",
      "Epoch Loss: 0.2115152139030397 \n",
      "Total Loss: 1.631606164198807 \n",
      "\n",
      "Epoch: 17, Batch: 300 \n",
      "Epoch Loss: 0.21383594355235497 \n",
      "Total Loss: 1.0920213134433416 \n",
      "\n",
      "Epoch: 17, Batch: 400 \n",
      "Epoch Loss: 0.21418642894364892 \n",
      "Total Loss: 0.8221812480989406 \n",
      "\n",
      "Epoch: 17, Batch: 500 \n",
      "Epoch Loss: 0.2144628154858947 \n",
      "Total Loss: 0.6602810968515628 \n",
      "\n",
      "Epoch: 17, Batch: 600 \n",
      "Epoch Loss: 0.21501626972729962 \n",
      "Total Loss: 0.5523693801305604 \n",
      "\n",
      "Epoch: 17, Batch: 700 \n",
      "Epoch Loss: 0.21575277406722307 \n",
      "Total Loss: 0.47530965187751195 \n",
      "\n",
      "Epoch: 17, Batch: 800 \n",
      "Epoch Loss: 0.21654462329577653 \n",
      "Total Loss: 0.4175289422155851 \n",
      "\n",
      "Epoch: 17, Batch: 900 \n",
      "Epoch Loss: 0.21441172819170687 \n",
      "Total Loss: 0.3724266974423446 \n",
      "\n",
      "Epoch: 18, Batch: 100 \n",
      "Epoch Loss: 0.2009303668141365 \n",
      "Total Loss: 3.1804591640105677 \n",
      "\n",
      "Epoch: 18, Batch: 200 \n",
      "Epoch Loss: 0.20758348204195498 \n",
      "Total Loss: 1.5961805985961108 \n",
      "\n",
      "Epoch: 18, Batch: 300 \n",
      "Epoch Loss: 0.209169887950023 \n",
      "Total Loss: 1.0680526712819658 \n",
      "\n",
      "Epoch: 18, Batch: 400 \n",
      "Epoch Loss: 0.20786751223728062 \n",
      "Total Loss: 0.80387228658785 \n",
      "\n",
      "Epoch: 18, Batch: 500 \n",
      "Epoch Loss: 0.20920571345090866 \n",
      "Total Loss: 0.645481812807007 \n",
      "\n",
      "Epoch: 18, Batch: 600 \n",
      "Epoch Loss: 0.20987924099589386 \n",
      "Total Loss: 0.5398760188088096 \n",
      "\n",
      "Epoch: 18, Batch: 700 \n",
      "Epoch Loss: 0.21239428112017256 \n",
      "Total Loss: 0.46455630597791503 \n",
      "\n",
      "Epoch: 18, Batch: 800 \n",
      "Epoch Loss: 0.215119462874718 \n",
      "Total Loss: 0.40811312700259605 \n",
      "\n",
      "Epoch: 18, Batch: 900 \n",
      "Epoch Loss: 0.21241295693235265 \n",
      "Total Loss: 0.3639447604800448 \n",
      "\n",
      "Epoch: 19, Batch: 100 \n",
      "Epoch Loss: 0.19454000122845172 \n",
      "Total Loss: 3.116681795610409 \n",
      "\n",
      "Epoch: 19, Batch: 200 \n",
      "Epoch Loss: 0.20986478716135026 \n",
      "Total Loss: 1.5642669392024215 \n",
      "\n",
      "Epoch: 19, Batch: 300 \n",
      "Epoch Loss: 0.21132103078067302 \n",
      "Total Loss: 1.0466031089072165 \n",
      "\n",
      "Epoch: 19, Batch: 400 \n",
      "Epoch Loss: 0.208597443215549 \n",
      "Total Loss: 0.7875895248451515 \n",
      "\n",
      "Epoch: 19, Batch: 500 \n",
      "Epoch Loss: 0.21238124038279058 \n",
      "Total Loss: 0.6324665296556135 \n",
      "\n",
      "Epoch: 19, Batch: 600 \n",
      "Epoch Loss: 0.2117397289102276 \n",
      "Total Loss: 0.5288846709546552 \n",
      "\n",
      "Epoch: 19, Batch: 700 \n",
      "Epoch Loss: 0.2129664422305567 \n",
      "Total Loss: 0.45498630985701666 \n",
      "\n",
      "Epoch: 19, Batch: 800 \n",
      "Epoch Loss: 0.2142493622470647 \n",
      "Total Loss: 0.3995816382456963 \n",
      "\n",
      "Epoch: 19, Batch: 900 \n",
      "Epoch Loss: 0.21193609373437033 \n",
      "Total Loss: 0.35631484771332544 \n",
      "\n",
      "Epoch: 20, Batch: 100 \n",
      "Epoch Loss: 0.1926489866897464 \n",
      "Total Loss: 3.0591236995067446 \n",
      "\n",
      "Epoch: 20, Batch: 200 \n",
      "Epoch Loss: 0.20702980207279326 \n",
      "Total Loss: 1.5350971151897683 \n",
      "\n",
      "Epoch: 20, Batch: 300 \n",
      "Epoch Loss: 0.2059596693639954 \n",
      "Total Loss: 1.0267950668589523 \n",
      "\n",
      "Epoch: 20, Batch: 400 \n",
      "Epoch Loss: 0.2044469276536256 \n",
      "Total Loss: 0.7725951589257456 \n",
      "\n",
      "Epoch: 20, Batch: 500 \n",
      "Epoch Loss: 0.2039876768887043 \n",
      "Total Loss: 0.6200976338788867 \n",
      "\n",
      "Epoch: 20, Batch: 600 \n",
      "Epoch Loss: 0.20419074270874263 \n",
      "Total Loss: 0.5184580788308134 \n",
      "\n",
      "Epoch: 20, Batch: 700 \n",
      "Epoch Loss: 0.20447274105357272 \n",
      "Total Loss: 0.445865244220144 \n",
      "\n",
      "Epoch: 20, Batch: 800 \n",
      "Epoch Loss: 0.2064491561567411 \n",
      "Total Loss: 0.39150886407936925 \n",
      "\n",
      "Epoch: 20, Batch: 900 \n",
      "Epoch Loss: 0.20512445451898706 \n",
      "Total Loss: 0.3490885838562002 \n",
      "\n",
      "Epoch: 21, Batch: 100 \n",
      "Epoch Loss: 0.19167730890214443 \n",
      "Total Loss: 3.0042530804127456 \n",
      "\n",
      "Epoch: 21, Batch: 200 \n",
      "Epoch Loss: 0.20315223403275012 \n",
      "Total Loss: 1.5072367106626432 \n",
      "\n",
      "Epoch: 21, Batch: 300 \n",
      "Epoch Loss: 0.20392722388108572 \n",
      "Total Loss: 1.008086016689028 \n",
      "\n",
      "Epoch: 21, Batch: 400 \n",
      "Epoch Loss: 0.19879980166442693 \n",
      "Total Loss: 0.7582480546002764 \n",
      "\n",
      "Epoch: 21, Batch: 500 \n",
      "Epoch Loss: 0.19979350277036428 \n",
      "Total Loss: 0.608539094224927 \n",
      "\n",
      "Epoch: 21, Batch: 600 \n",
      "Epoch Loss: 0.20041502718503276 \n",
      "Total Loss: 0.508731170975204 \n",
      "\n",
      "Epoch: 21, Batch: 700 \n",
      "Epoch Loss: 0.20151028847055777 \n",
      "Total Loss: 0.43747081223849943 \n",
      "\n",
      "Epoch: 21, Batch: 800 \n",
      "Epoch Loss: 0.20487046865746378 \n",
      "Total Loss: 0.3841464352918168 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Batch: 900 \n",
      "Epoch Loss: 0.20366865950740046 \n",
      "Total Loss: 0.342490239763662 \n",
      "\n",
      "Epoch: 22, Batch: 100 \n",
      "Epoch Loss: 0.1934474064782262 \n",
      "Total Loss: 2.9542064213854347 \n",
      "\n",
      "Epoch: 22, Batch: 200 \n",
      "Epoch Loss: 0.2050561302341521 \n",
      "Total Loss: 1.4820274119197645 \n",
      "\n",
      "Epoch: 22, Batch: 300 \n",
      "Epoch Loss: 0.20495798175533614 \n",
      "Total Loss: 0.9911207243828385 \n",
      "\n",
      "Epoch: 22, Batch: 400 \n",
      "Epoch Loss: 0.20395699480548501 \n",
      "Total Loss: 0.7456241118548099 \n",
      "\n",
      "Epoch: 22, Batch: 500 \n",
      "Epoch Loss: 0.2065215060710907 \n",
      "Total Loss: 0.5984700126759708 \n",
      "\n",
      "Epoch: 22, Batch: 600 \n",
      "Epoch Loss: 0.2064864024768273 \n",
      "Total Loss: 0.5002879718095629 \n",
      "\n",
      "Epoch: 22, Batch: 700 \n",
      "Epoch Loss: 0.2045779228795852 \n",
      "Total Loss: 0.43007233327375605 \n",
      "\n",
      "Epoch: 22, Batch: 800 \n",
      "Epoch Loss: 0.20569626858923584 \n",
      "Total Loss: 0.3775265000722456 \n",
      "\n",
      "Epoch: 22, Batch: 900 \n",
      "Epoch Loss: 0.20434932550622356 \n",
      "Total Loss: 0.3365567565331179 \n",
      "\n",
      "Epoch: 23, Batch: 100 \n",
      "Epoch Loss: 0.18837066501379013 \n",
      "Total Loss: 2.908105827803197 \n",
      "\n",
      "Epoch: 23, Batch: 200 \n",
      "Epoch Loss: 0.19320914570242168 \n",
      "Total Loss: 1.4583582970840128 \n",
      "\n",
      "Epoch: 23, Batch: 300 \n",
      "Epoch Loss: 0.19350530696411927 \n",
      "Total Loss: 0.9750518738456826 \n",
      "\n",
      "Epoch: 23, Batch: 400 \n",
      "Epoch Loss: 0.18936686429195107 \n",
      "Total Loss: 0.7332122916481255 \n",
      "\n",
      "Epoch: 23, Batch: 500 \n",
      "Epoch Loss: 0.19047617651522158 \n",
      "Total Loss: 0.588264732669877 \n",
      "\n",
      "Epoch: 23, Batch: 600 \n",
      "Epoch Loss: 0.19130445652951797 \n",
      "Total Loss: 0.4916368848814265 \n",
      "\n",
      "Epoch: 23, Batch: 700 \n",
      "Epoch Loss: 0.19179797055465833 \n",
      "Total Loss: 0.4226127277536792 \n",
      "\n",
      "Epoch: 23, Batch: 800 \n",
      "Epoch Loss: 0.19465111035853624 \n",
      "Total Loss: 0.37095256661591325 \n",
      "\n",
      "Epoch: 23, Batch: 900 \n",
      "Epoch Loss: 0.19408329726507267 \n",
      "Total Loss: 0.33065127077219997 \n",
      "\n",
      "Epoch: 24, Batch: 100 \n",
      "Epoch Loss: 0.18641091521829367 \n",
      "Total Loss: 2.862321685645729 \n",
      "\n",
      "Epoch: 24, Batch: 200 \n",
      "Epoch Loss: 0.19142536712810398 \n",
      "Total Loss: 1.4352533390528213 \n",
      "\n",
      "Epoch: 24, Batch: 300 \n",
      "Epoch Loss: 0.19269701037555934 \n",
      "Total Loss: 0.9595472301584151 \n",
      "\n",
      "Epoch: 24, Batch: 400 \n",
      "Epoch Loss: 0.18925012940540908 \n",
      "Total Loss: 0.7215240631031338 \n",
      "\n",
      "Epoch: 24, Batch: 500 \n",
      "Epoch Loss: 0.19200131314247845 \n",
      "Total Loss: 0.57891096754993 \n",
      "\n",
      "Epoch: 24, Batch: 600 \n",
      "Epoch Loss: 0.19303468462079765 \n",
      "Total Loss: 0.48380220588891665 \n",
      "\n",
      "Epoch: 24, Batch: 700 \n",
      "Epoch Loss: 0.19499657958745956 \n",
      "Total Loss: 0.41591836665113946 \n",
      "\n",
      "Epoch: 24, Batch: 800 \n",
      "Epoch Loss: 0.195675328806974 \n",
      "Total Loss: 0.36497245922257815 \n",
      "\n",
      "Epoch: 24, Batch: 900 \n",
      "Epoch Loss: 0.19444229022910198 \n",
      "Total Loss: 0.3252744914460237 \n",
      "\n",
      "Epoch: 25, Batch: 100 \n",
      "Epoch Loss: 0.19463722068816425 \n",
      "Total Loss: 2.820541996002197 \n",
      "\n",
      "Epoch: 25, Batch: 200 \n",
      "Epoch Loss: 0.19435745630413293 \n",
      "Total Loss: 1.4141525518395006 \n",
      "\n",
      "Epoch: 25, Batch: 300 \n",
      "Epoch Loss: 0.19731725516418616 \n",
      "Total Loss: 0.9454781925981244 \n",
      "\n",
      "Epoch: 25, Batch: 400 \n",
      "Epoch Loss: 0.19556299956515433 \n",
      "Total Loss: 0.7110116467762738 \n",
      "\n",
      "Epoch: 25, Batch: 500 \n",
      "Epoch Loss: 0.19556569044291974 \n",
      "Total Loss: 0.5703739290526509 \n",
      "\n",
      "Epoch: 25, Batch: 600 \n",
      "Epoch Loss: 0.19476473892107607 \n",
      "Total Loss: 0.4765833407526215 \n",
      "\n",
      "Epoch: 25, Batch: 700 \n",
      "Epoch Loss: 0.19367655269269432 \n",
      "Total Loss: 0.40956942027551785 \n",
      "\n",
      "Epoch: 25, Batch: 800 \n",
      "Epoch Loss: 0.19597713642288 \n",
      "Total Loss: 0.359433648853749 \n",
      "\n",
      "Epoch: 25, Batch: 900 \n",
      "Epoch Loss: 0.19374353376527628 \n",
      "Total Loss: 0.3202782421477967 \n",
      "\n",
      "Epoch: 26, Batch: 100 \n",
      "Epoch Loss: 0.1917648668959737 \n",
      "Total Loss: 2.7813620536889023 \n",
      "\n",
      "Epoch: 26, Batch: 200 \n",
      "Epoch Loss: 0.19802259230986238 \n",
      "Total Loss: 1.3946094944929848 \n",
      "\n",
      "Epoch: 26, Batch: 300 \n",
      "Epoch Loss: 0.19603360506395498 \n",
      "Total Loss: 0.932201914669325 \n",
      "\n",
      "Epoch: 26, Batch: 400 \n",
      "Epoch Loss: 0.19292097291909158 \n",
      "Total Loss: 0.7009166578912678 \n",
      "\n",
      "Epoch: 26, Batch: 500 \n",
      "Epoch Loss: 0.19341239861398934 \n",
      "Total Loss: 0.5622362347852725 \n",
      "\n",
      "Epoch: 26, Batch: 600 \n",
      "Epoch Loss: 0.19434558217724165 \n",
      "Total Loss: 0.4698059103979419 \n",
      "\n",
      "Epoch: 26, Batch: 700 \n",
      "Epoch Loss: 0.19315548538097313 \n",
      "Total Loss: 0.4037128402564961 \n",
      "\n",
      "Epoch: 26, Batch: 800 \n",
      "Epoch Loss: 0.19394829244818537 \n",
      "Total Loss: 0.35420785994519693 \n",
      "\n",
      "Epoch: 26, Batch: 900 \n",
      "Epoch Loss: 0.1922294042673376 \n",
      "Total Loss: 0.3156141588351347 \n",
      "\n",
      "Epoch: 27, Batch: 100 \n",
      "Epoch Loss: 0.1862086496502161 \n",
      "Total Loss: 2.7443502232653123 \n",
      "\n",
      "Epoch: 27, Batch: 200 \n",
      "Epoch Loss: 0.18919971790164708 \n",
      "Total Loss: 1.3757342002651205 \n",
      "\n",
      "Epoch: 27, Batch: 300 \n",
      "Epoch Loss: 0.1861943629135688 \n",
      "Total Loss: 0.9193806230525176 \n",
      "\n",
      "Epoch: 27, Batch: 400 \n",
      "Epoch Loss: 0.18507948991842568 \n",
      "Total Loss: 0.6912181975758048 \n",
      "\n",
      "Epoch: 27, Batch: 500 \n",
      "Epoch Loss: 0.18916337884217502 \n",
      "Total Loss: 0.5544967723905488 \n",
      "\n",
      "Epoch: 27, Batch: 600 \n",
      "Epoch Loss: 0.1895166833139956 \n",
      "Total Loss: 0.463261404187637 \n",
      "\n",
      "Epoch: 27, Batch: 700 \n",
      "Epoch Loss: 0.1895652747207454 \n",
      "Total Loss: 0.3980857370452827 \n",
      "\n",
      "Epoch: 27, Batch: 800 \n",
      "Epoch Loss: 0.18993693990632893 \n",
      "Total Loss: 0.34921640230446227 \n",
      "\n",
      "Epoch: 27, Batch: 900 \n",
      "Epoch Loss: 0.18844180901431376 \n",
      "Total Loss: 0.3111408381055228 \n",
      "\n",
      "Epoch: 28, Batch: 100 \n",
      "Epoch Loss: 0.18322415702044964 \n",
      "Total Loss: 2.708810159697064 \n",
      "\n",
      "Epoch: 28, Batch: 200 \n",
      "Epoch Loss: 0.1885697688162327 \n",
      "Total Loss: 1.3578678545023182 \n",
      "\n",
      "Epoch: 28, Batch: 300 \n",
      "Epoch Loss: 0.1870430534084638 \n",
      "Total Loss: 0.9074355889847946 \n",
      "\n",
      "Epoch: 28, Batch: 400 \n",
      "Epoch Loss: 0.18309536156244577 \n",
      "Total Loss: 0.6821057300066709 \n",
      "\n",
      "Epoch: 28, Batch: 500 \n",
      "Epoch Loss: 0.18501732847839594 \n",
      "Total Loss: 0.5470610496920667 \n",
      "\n",
      "Epoch: 28, Batch: 600 \n",
      "Epoch Loss: 0.18377350649485985 \n",
      "Total Loss: 0.4569410794849197 \n",
      "\n",
      "Epoch: 28, Batch: 700 \n",
      "Epoch Loss: 0.18475290450666632 \n",
      "Total Loss: 0.39263638084716335 \n",
      "\n",
      "Epoch: 28, Batch: 800 \n",
      "Epoch Loss: 0.18692030339036136 \n",
      "Total Loss: 0.34445903009651896 \n",
      "\n",
      "Epoch: 28, Batch: 900 \n",
      "Epoch Loss: 0.18624711027161941 \n",
      "Total Loss: 0.30690350915453146 \n",
      "\n",
      "Epoch: 29, Batch: 100 \n",
      "Epoch Loss: 0.1784404483437538 \n",
      "Total Loss: 2.674948788456105 \n",
      "\n",
      "Epoch: 29, Batch: 200 \n",
      "Epoch Loss: 0.18158106856048106 \n",
      "Total Loss: 1.3406592509311077 \n",
      "\n",
      "Epoch: 29, Batch: 300 \n",
      "Epoch Loss: 0.18407212249934674 \n",
      "Total Loss: 0.8959458710848429 \n",
      "\n",
      "Epoch: 29, Batch: 400 \n",
      "Epoch Loss: 0.18349381146021188 \n",
      "Total Loss: 0.6735262901958977 \n",
      "\n",
      "Epoch: 29, Batch: 500 \n",
      "Epoch Loss: 0.1854639411047101 \n",
      "Total Loss: 0.5401544422234954 \n",
      "\n",
      "Epoch: 29, Batch: 600 \n",
      "Epoch Loss: 0.18556245781481265 \n",
      "Total Loss: 0.45119798369984004 \n",
      "\n",
      "Epoch: 29, Batch: 700 \n",
      "Epoch Loss: 0.18618592706641982 \n",
      "Total Loss: 0.38767672860270064 \n",
      "\n",
      "Epoch: 29, Batch: 800 \n",
      "Epoch Loss: 0.1888496056338772 \n",
      "Total Loss: 0.3401115140601479 \n",
      "\n",
      "Epoch: 29, Batch: 900 \n",
      "Epoch Loss: 0.1875214815346731 \n",
      "Total Loss: 0.30299911015592085 \n",
      "\n",
      "Epoch: 30, Batch: 100 \n",
      "Epoch Loss: 0.17572768304497002 \n",
      "Total Loss: 2.6437896820108095 \n",
      "\n",
      "Epoch: 30, Batch: 200 \n",
      "Epoch Loss: 0.18428165126591922 \n",
      "Total Loss: 1.3251087679968525 \n",
      "\n",
      "Epoch: 30, Batch: 300 \n",
      "Epoch Loss: 0.18497121023635069 \n",
      "Total Loss: 0.8854764045332041 \n",
      "\n",
      "Epoch: 30, Batch: 400 \n",
      "Epoch Loss: 0.18113776003941895 \n",
      "Total Loss: 0.6655209484786416 \n",
      "\n",
      "Epoch: 30, Batch: 500 \n",
      "Epoch Loss: 0.18370421300083398 \n",
      "Total Loss: 0.5337098922818899 \n",
      "\n",
      "Epoch: 30, Batch: 600 \n",
      "Epoch Loss: 0.18582329435274006 \n",
      "Total Loss: 0.4458494585744209 \n",
      "\n",
      "Epoch: 30, Batch: 700 \n",
      "Epoch Loss: 0.1862757043221167 \n",
      "Total Loss: 0.3830566319406387 \n",
      "\n",
      "Epoch: 30, Batch: 800 \n",
      "Epoch Loss: 0.188137802737765 \n",
      "Total Loss: 0.336012771663256 \n",
      "\n",
      "Epoch: 30, Batch: 900 \n",
      "Epoch Loss: 0.18674338885893424 \n",
      "Total Loss: 0.2993283454704064 \n",
      "\n",
      "Epoch: 31, Batch: 100 \n",
      "Epoch Loss: 0.1705453882738948 \n",
      "Total Loss: 2.6143040089393335 \n",
      "\n",
      "Epoch: 31, Batch: 200 \n",
      "Epoch Loss: 0.17898432863876224 \n",
      "Total Loss: 1.3101749604213382 \n",
      "\n",
      "Epoch: 31, Batch: 300 \n",
      "Epoch Loss: 0.18183187194168568 \n",
      "Total Loss: 0.8754663925233387 \n",
      "\n",
      "Epoch: 31, Batch: 400 \n",
      "Epoch Loss: 0.17864772516302765 \n",
      "Total Loss: 0.6579634660443351 \n",
      "\n",
      "Epoch: 31, Batch: 500 \n",
      "Epoch Loss: 0.18278682616353034 \n",
      "Total Loss: 0.5276568581913748 \n",
      "\n",
      "Epoch: 31, Batch: 600 \n",
      "Epoch Loss: 0.18220938860749206 \n",
      "Total Loss: 0.44067814634672253 \n",
      "\n",
      "Epoch: 31, Batch: 700 \n",
      "Epoch Loss: 0.18296853523169246 \n",
      "Total Loss: 0.3785882886427064 \n",
      "\n",
      "Epoch: 31, Batch: 800 \n",
      "Epoch Loss: 0.18465178478509187 \n",
      "Total Loss: 0.3320568272867991 \n",
      "\n",
      "Epoch: 31, Batch: 900 \n",
      "Epoch Loss: 0.18237177832259072 \n",
      "Total Loss: 0.2957499100134364 \n",
      "\n",
      "Epoch: 32, Batch: 100 \n",
      "Epoch Loss: 0.17337068162858485 \n",
      "Total Loss: 2.585676212116377 \n",
      "\n",
      "Epoch: 32, Batch: 200 \n",
      "Epoch Loss: 0.1795767766609788 \n",
      "Total Loss: 1.2957409634283976 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Batch: 300 \n",
      "Epoch Loss: 0.17895042213300863 \n",
      "Total Loss: 0.8656783267968179 \n",
      "\n",
      "Epoch: 32, Batch: 400 \n",
      "Epoch Loss: 0.17752181902527808 \n",
      "Total Loss: 0.650612151423411 \n",
      "\n",
      "Epoch: 32, Batch: 500 \n",
      "Epoch Loss: 0.179434325709939 \n",
      "Total Loss: 0.5216589983415324 \n",
      "\n",
      "Epoch: 32, Batch: 600 \n",
      "Epoch Loss: 0.18049874822298687 \n",
      "Total Loss: 0.43568364893454903 \n",
      "\n",
      "Epoch: 32, Batch: 700 \n",
      "Epoch Loss: 0.18103371452007974 \n",
      "Total Loss: 0.37426564333810736 \n",
      "\n",
      "Epoch: 32, Batch: 800 \n",
      "Epoch Loss: 0.18278746984433383 \n",
      "Total Loss: 0.328244405722071 \n",
      "\n",
      "Epoch: 32, Batch: 900 \n",
      "Epoch Loss: 0.18020721435546874 \n",
      "Total Loss: 0.29232685081699555 \n",
      "\n",
      "Epoch: 33, Batch: 100 \n",
      "Epoch Loss: 0.17387826897203923 \n",
      "Total Loss: 2.558299859451751 \n",
      "\n",
      "Epoch: 33, Batch: 200 \n",
      "Epoch Loss: 0.17780718427151443 \n",
      "Total Loss: 1.2819035069921025 \n",
      "\n",
      "Epoch: 33, Batch: 300 \n",
      "Epoch Loss: 0.17657318094124397 \n",
      "Total Loss: 0.8563609761187826 \n",
      "\n",
      "Epoch: 33, Batch: 400 \n",
      "Epoch Loss: 0.17396885519847274 \n",
      "Total Loss: 0.643529488740376 \n",
      "\n",
      "Epoch: 33, Batch: 500 \n",
      "Epoch Loss: 0.17611954566836358 \n",
      "Total Loss: 0.5159431201289549 \n",
      "\n",
      "Epoch: 33, Batch: 600 \n",
      "Epoch Loss: 0.1776842903966705 \n",
      "Total Loss: 0.43088950926927155 \n",
      "\n",
      "Epoch: 33, Batch: 700 \n",
      "Epoch Loss: 0.17903123764055115 \n",
      "Total Loss: 0.37014387773341817 \n",
      "\n",
      "Epoch: 33, Batch: 800 \n",
      "Epoch Loss: 0.18016392834484576 \n",
      "Total Loss: 0.32458836560490345 \n",
      "\n",
      "Epoch: 33, Batch: 900 \n",
      "Epoch Loss: 0.17889681191080145 \n",
      "Total Loss: 0.2890912067344544 \n",
      "\n",
      "Epoch: 34, Batch: 100 \n",
      "Epoch Loss: 0.17155747972428798 \n",
      "Total Loss: 2.5320360159742483 \n",
      "\n",
      "Epoch: 34, Batch: 200 \n",
      "Epoch Loss: 0.17508089516311884 \n",
      "Total Loss: 1.2686445419665646 \n",
      "\n",
      "Epoch: 34, Batch: 300 \n",
      "Epoch Loss: 0.17690100332101186 \n",
      "Total Loss: 0.8475330399349332 \n",
      "\n",
      "Epoch: 34, Batch: 400 \n",
      "Epoch Loss: 0.1772777635511011 \n",
      "Total Loss: 0.6369616038059158 \n",
      "\n",
      "Epoch: 34, Batch: 500 \n",
      "Epoch Loss: 0.17883930091559888 \n",
      "Total Loss: 0.5106580209881068 \n",
      "\n",
      "Epoch: 34, Batch: 600 \n",
      "Epoch Loss: 0.17862703829382856 \n",
      "Total Loss: 0.42641877104491727 \n",
      "\n",
      "Epoch: 34, Batch: 700 \n",
      "Epoch Loss: 0.17840879255639655 \n",
      "Total Loss: 0.3662459185348526 \n",
      "\n",
      "Epoch: 34, Batch: 800 \n",
      "Epoch Loss: 0.17983237355481832 \n",
      "Total Loss: 0.3211629634258776 \n",
      "\n",
      "Epoch: 34, Batch: 900 \n",
      "Epoch Loss: 0.17935720084028112 \n",
      "Total Loss: 0.2860519015390937 \n",
      "\n",
      "Epoch: 35, Batch: 100 \n",
      "Epoch Loss: 0.16442891798913478 \n",
      "Total Loss: 2.507067540912756 \n",
      "\n",
      "Epoch: 35, Batch: 200 \n",
      "Epoch Loss: 0.17355653524398804 \n",
      "Total Loss: 1.2561435440635043 \n",
      "\n",
      "Epoch: 35, Batch: 300 \n",
      "Epoch Loss: 0.17492956185092529 \n",
      "Total Loss: 0.8391211780905724 \n",
      "\n",
      "Epoch: 35, Batch: 400 \n",
      "Epoch Loss: 0.17270251146517693 \n",
      "Total Loss: 0.6305267504272717 \n",
      "\n",
      "Epoch: 35, Batch: 500 \n",
      "Epoch Loss: 0.17340133171528577 \n",
      "Total Loss: 0.5054282381287643 \n",
      "\n",
      "Epoch: 35, Batch: 600 \n",
      "Epoch Loss: 0.17495683358361325 \n",
      "Total Loss: 0.4220603619783762 \n",
      "\n",
      "Epoch: 35, Batch: 700 \n",
      "Epoch Loss: 0.176326533912548 \n",
      "Total Loss: 0.3625192683728374 \n",
      "\n",
      "Epoch: 35, Batch: 800 \n",
      "Epoch Loss: 0.17810898010153323 \n",
      "Total Loss: 0.31788502448131994 \n",
      "\n",
      "Epoch: 35, Batch: 900 \n",
      "Epoch Loss: 0.17712118067261245 \n",
      "Total Loss: 0.2831016686984471 \n",
      "\n",
      "Epoch: 36, Batch: 100 \n",
      "Epoch Loss: 0.1657383843511343 \n",
      "Total Loss: 2.4832431427689476 \n",
      "\n",
      "Epoch: 36, Batch: 200 \n",
      "Epoch Loss: 0.16618273643776774 \n",
      "Total Loss: 1.2439358365028683 \n",
      "\n",
      "Epoch: 36, Batch: 300 \n",
      "Epoch Loss: 0.16776784875740608 \n",
      "Total Loss: 0.830873317607437 \n",
      "\n",
      "Epoch: 36, Batch: 400 \n",
      "Epoch Loss: 0.16865743026137353 \n",
      "Total Loss: 0.62434475330817 \n",
      "\n",
      "Epoch: 36, Batch: 500 \n",
      "Epoch Loss: 0.17007576629519464 \n",
      "Total Loss: 0.500452186593372 \n",
      "\n",
      "Epoch: 36, Batch: 600 \n",
      "Epoch Loss: 0.1690667277574539 \n",
      "Total Loss: 0.4178028477864616 \n",
      "\n",
      "Epoch: 36, Batch: 700 \n",
      "Epoch Loss: 0.17067783255662236 \n",
      "Total Loss: 0.3588323792985212 \n",
      "\n",
      "Epoch: 36, Batch: 800 \n",
      "Epoch Loss: 0.1735243874648586 \n",
      "Total Loss: 0.31465003421892307 \n",
      "\n",
      "Epoch: 36, Batch: 900 \n",
      "Epoch Loss: 0.1732423701799578 \n",
      "Total Loss: 0.2802166545893537 \n",
      "\n",
      "Epoch: 37, Batch: 100 \n",
      "Epoch Loss: 0.1625590667501092 \n",
      "Total Loss: 2.459527397667234 \n",
      "\n",
      "Epoch: 37, Batch: 200 \n",
      "Epoch Loss: 0.16904728781431913 \n",
      "Total Loss: 1.2321358003049485 \n",
      "\n",
      "Epoch: 37, Batch: 300 \n",
      "Epoch Loss: 0.17048624886820712 \n",
      "Total Loss: 0.8229857062481276 \n",
      "\n",
      "Epoch: 37, Batch: 400 \n",
      "Epoch Loss: 0.16876900097355246 \n",
      "Total Loss: 0.6183448016948092 \n",
      "\n",
      "Epoch: 37, Batch: 500 \n",
      "Epoch Loss: 0.172345799587667 \n",
      "Total Loss: 0.49568477645878856 \n",
      "\n",
      "Epoch: 37, Batch: 600 \n",
      "Epoch Loss: 0.1726734720915556 \n",
      "Total Loss: 0.41385583549318417 \n",
      "\n",
      "Epoch: 37, Batch: 700 \n",
      "Epoch Loss: 0.1730122561007738 \n",
      "Total Loss: 0.355409422546884 \n",
      "\n",
      "Epoch: 37, Batch: 800 \n",
      "Epoch Loss: 0.1746124885370955 \n",
      "Total Loss: 0.31161099511903484 \n",
      "\n",
      "Epoch: 37, Batch: 900 \n",
      "Epoch Loss: 0.17425345715963178 \n",
      "Total Loss: 0.2775022094936163 \n",
      "\n",
      "Epoch: 38, Batch: 100 \n",
      "Epoch Loss: 0.1652292102575302 \n",
      "Total Loss: 2.437495331027006 \n",
      "\n",
      "Epoch: 38, Batch: 200 \n",
      "Epoch Loss: 0.17037954417988657 \n",
      "Total Loss: 1.221057269172743 \n",
      "\n",
      "Epoch: 38, Batch: 300 \n",
      "Epoch Loss: 0.16995620110382637 \n",
      "Total Loss: 0.8155215962463173 \n",
      "\n",
      "Epoch: 38, Batch: 400 \n",
      "Epoch Loss: 0.1673175437003374 \n",
      "Total Loss: 0.6126898917340134 \n",
      "\n",
      "Epoch: 38, Batch: 500 \n",
      "Epoch Loss: 0.16790765716135503 \n",
      "Total Loss: 0.4910480613398709 \n",
      "\n",
      "Epoch: 38, Batch: 600 \n",
      "Epoch Loss: 0.16975253626083334 \n",
      "Total Loss: 0.40999170432602494 \n",
      "\n",
      "Epoch: 38, Batch: 700 \n",
      "Epoch Loss: 0.17110899911395142 \n",
      "Total Loss: 0.35209532467130206 \n",
      "\n",
      "Epoch: 38, Batch: 800 \n",
      "Epoch Loss: 0.17343633679207415 \n",
      "Total Loss: 0.3087075133654779 \n",
      "\n",
      "Epoch: 38, Batch: 900 \n",
      "Epoch Loss: 0.17319008257653978 \n",
      "Total Loss: 0.2749073219647882 \n",
      "\n",
      "Epoch: 39, Batch: 100 \n",
      "Epoch Loss: 0.16348485320806502 \n",
      "Total Loss: 2.4162606195809366 \n",
      "\n",
      "Epoch: 39, Batch: 200 \n",
      "Epoch Loss: 0.16528062077239156 \n",
      "Total Loss: 1.2102723147691443 \n",
      "\n",
      "Epoch: 39, Batch: 300 \n",
      "Epoch Loss: 0.1659542033697168 \n",
      "Total Loss: 0.808278136073142 \n",
      "\n",
      "Epoch: 39, Batch: 400 \n",
      "Epoch Loss: 0.1639780745934695 \n",
      "Total Loss: 0.6072217410821945 \n",
      "\n",
      "Epoch: 39, Batch: 500 \n",
      "Epoch Loss: 0.16554985755681992 \n",
      "Total Loss: 0.48665860819606443 \n",
      "\n",
      "Epoch: 39, Batch: 600 \n",
      "Epoch Loss: 0.16721286144728462 \n",
      "Total Loss: 0.4062989593125307 \n",
      "\n",
      "Epoch: 39, Batch: 700 \n",
      "Epoch Loss: 0.16732993040766034 \n",
      "Total Loss: 0.3488717539315095 \n",
      "\n",
      "Epoch: 39, Batch: 800 \n",
      "Epoch Loss: 0.1699163402337581 \n",
      "Total Loss: 0.3058654167702517 \n",
      "\n",
      "Epoch: 39, Batch: 900 \n",
      "Epoch Loss: 0.16947640544838374 \n",
      "Total Loss: 0.2723531822207519 \n",
      "\n",
      "Epoch: 40, Batch: 100 \n",
      "Epoch Loss: 0.1517261616513133 \n",
      "Total Loss: 2.394984349307604 \n",
      "\n",
      "Epoch: 40, Batch: 200 \n",
      "Epoch Loss: 0.16283061185851694 \n",
      "Total Loss: 1.1996663629296236 \n",
      "\n",
      "Epoch: 40, Batch: 300 \n",
      "Epoch Loss: 0.16429523080587388 \n",
      "Total Loss: 0.8011711125255873 \n",
      "\n",
      "Epoch: 40, Batch: 400 \n",
      "Epoch Loss: 0.16311286139301956 \n",
      "Total Loss: 0.6018756203514058 \n",
      "\n",
      "Epoch: 40, Batch: 500 \n",
      "Epoch Loss: 0.1648855941966176 \n",
      "Total Loss: 0.4823603789081797 \n",
      "\n",
      "Epoch: 40, Batch: 600 \n",
      "Epoch Loss: 0.16582246270651618 \n",
      "Total Loss: 0.4026774274453831 \n",
      "\n",
      "Epoch: 40, Batch: 700 \n",
      "Epoch Loss: 0.16693905946399484 \n",
      "Total Loss: 0.3457722186675029 \n",
      "\n",
      "Epoch: 40, Batch: 800 \n",
      "Epoch Loss: 0.16913058580830692 \n",
      "Total Loss: 0.3031271640534978 \n",
      "\n",
      "Epoch: 40, Batch: 900 \n",
      "Epoch Loss: 0.16805123582068418 \n",
      "Total Loss: 0.2698891914806639 \n",
      "\n",
      "Epoch: 41, Batch: 100 \n",
      "Epoch Loss: 0.1707062728330493 \n",
      "Total Loss: 2.375145856131141 \n",
      "\n",
      "Epoch: 41, Batch: 200 \n",
      "Epoch Loss: 0.17354375531896948 \n",
      "Total Loss: 1.189723918770508 \n",
      "\n",
      "Epoch: 41, Batch: 300 \n",
      "Epoch Loss: 0.17299801864971717 \n",
      "Total Loss: 0.7945468933698607 \n",
      "\n",
      "Epoch: 41, Batch: 400 \n",
      "Epoch Loss: 0.1677243120316416 \n",
      "Total Loss: 0.5968364090040871 \n",
      "\n",
      "Epoch: 41, Batch: 500 \n",
      "Epoch Loss: 0.16865785429626703 \n",
      "Total Loss: 0.47831006390256126 \n",
      "\n",
      "Epoch: 41, Batch: 600 \n",
      "Epoch Loss: 0.16825309230635563 \n",
      "Total Loss: 0.3992674487088693 \n",
      "\n",
      "Epoch: 41, Batch: 700 \n",
      "Epoch Loss: 0.16897773271160466 \n",
      "Total Loss: 0.34283316361506955 \n",
      "\n",
      "Epoch: 41, Batch: 800 \n",
      "Epoch Loss: 0.1702191188093275 \n",
      "Total Loss: 0.30052447188725107 \n",
      "\n",
      "Epoch: 41, Batch: 900 \n",
      "Epoch Loss: 0.16888024253149828 \n",
      "Total Loss: 0.2675615068057648 \n",
      "\n",
      "Epoch: 42, Batch: 100 \n",
      "Epoch Loss: 0.15613405022770166 \n",
      "Total Loss: 2.3557250961492815 \n",
      "\n",
      "Epoch: 42, Batch: 200 \n",
      "Epoch Loss: 0.16490654112771153 \n",
      "Total Loss: 1.1799301555987802 \n",
      "\n",
      "Epoch: 42, Batch: 300 \n",
      "Epoch Loss: 0.1625901782140136 \n",
      "Total Loss: 0.7878737343070171 \n",
      "\n",
      "Epoch: 42, Batch: 400 \n",
      "Epoch Loss: 0.16252783012576402 \n",
      "Total Loss: 0.5918716149318165 \n",
      "\n",
      "Epoch: 42, Batch: 500 \n",
      "Epoch Loss: 0.16465924518555403 \n",
      "Total Loss: 0.47432198197128517 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Batch: 600 \n",
      "Epoch Loss: 0.16709133580947916 \n",
      "Total Loss: 0.39597963493213884 \n",
      "\n",
      "Epoch: 42, Batch: 700 \n",
      "Epoch Loss: 0.16712035761880023 \n",
      "Total Loss: 0.339980144528482 \n",
      "\n",
      "Epoch: 42, Batch: 800 \n",
      "Epoch Loss: 0.16694338113535195 \n",
      "Total Loss: 0.297975794753348 \n",
      "\n",
      "Epoch: 42, Batch: 900 \n",
      "Epoch Loss: 0.1664933737864097 \n",
      "Total Loss: 0.26529830780984076 \n",
      "\n",
      "Epoch: 43, Batch: 100 \n",
      "Epoch Loss: 0.15573460537940265 \n",
      "Total Loss: 2.3370689670282396 \n",
      "\n",
      "Epoch: 43, Batch: 200 \n",
      "Epoch Loss: 0.16121398981660603 \n",
      "Total Loss: 1.1704727785635827 \n",
      "\n",
      "Epoch: 43, Batch: 300 \n",
      "Epoch Loss: 0.16620013973365227 \n",
      "Total Loss: 0.7816808635351616 \n",
      "\n",
      "Epoch: 43, Batch: 400 \n",
      "Epoch Loss: 0.16284730790182947 \n",
      "Total Loss: 0.5871489547002454 \n",
      "\n",
      "Epoch: 43, Batch: 500 \n",
      "Epoch Loss: 0.16605919694155455 \n",
      "Total Loss: 0.47055128819322173 \n",
      "\n",
      "Epoch: 43, Batch: 600 \n",
      "Epoch Loss: 0.1668940640675525 \n",
      "Total Loss: 0.3927891293071326 \n",
      "\n",
      "Epoch: 43, Batch: 700 \n",
      "Epoch Loss: 0.16786989834691796 \n",
      "Total Loss: 0.33725355569854926 \n",
      "\n",
      "Epoch: 43, Batch: 800 \n",
      "Epoch Loss: 0.1687131391186267 \n",
      "Total Loss: 0.2956044653772788 \n",
      "\n",
      "Epoch: 43, Batch: 900 \n",
      "Epoch Loss: 0.16600818875763151 \n",
      "Total Loss: 0.2631325702213271 \n",
      "\n",
      "Epoch: 44, Batch: 100 \n",
      "Epoch Loss: 0.14905818562954665 \n",
      "Total Loss: 2.318865535816347 \n",
      "\n",
      "Epoch: 44, Batch: 200 \n",
      "Epoch Loss: 0.157969782166183 \n",
      "Total Loss: 1.1613291472116147 \n",
      "\n",
      "Epoch: 44, Batch: 300 \n",
      "Epoch Loss: 0.15908094218621652 \n",
      "Total Loss: 0.7754414258852149 \n",
      "\n",
      "Epoch: 44, Batch: 400 \n",
      "Epoch Loss: 0.15976077500730754 \n",
      "Total Loss: 0.5825003891495395 \n",
      "\n",
      "Epoch: 44, Batch: 500 \n",
      "Epoch Loss: 0.1596897692605853 \n",
      "Total Loss: 0.466724882893603 \n",
      "\n",
      "Epoch: 44, Batch: 600 \n",
      "Epoch Loss: 0.16078590614721178 \n",
      "Total Loss: 0.389567200102928 \n",
      "\n",
      "Epoch: 44, Batch: 700 \n",
      "Epoch Loss: 0.161468483752438 \n",
      "Total Loss: 0.334452288235574 \n",
      "\n",
      "Epoch: 44, Batch: 800 \n",
      "Epoch Loss: 0.16416313992347567 \n",
      "Total Loss: 0.29316571167522026 \n",
      "\n",
      "Epoch: 44, Batch: 900 \n",
      "Epoch Loss: 0.1648179743811488 \n",
      "Total Loss: 0.26102117969626276 \n",
      "\n",
      "Epoch: 45, Batch: 100 \n",
      "Epoch Loss: 0.15771399144083262 \n",
      "Total Loss: 2.3016208114085925 \n",
      "\n",
      "Epoch: 45, Batch: 200 \n",
      "Epoch Loss: 0.15948149129748346 \n",
      "Total Loss: 1.1526020611615644 \n",
      "\n",
      "Epoch: 45, Batch: 300 \n",
      "Epoch Loss: 0.16450092705587546 \n",
      "Total Loss: 0.7696942615045441 \n",
      "\n",
      "Epoch: 45, Batch: 400 \n",
      "Epoch Loss: 0.1662153485044837 \n",
      "Total Loss: 0.578222688422021 \n",
      "\n",
      "Epoch: 45, Batch: 500 \n",
      "Epoch Loss: 0.16797997865825892 \n",
      "Total Loss: 0.4633560996232761 \n",
      "\n",
      "Epoch: 45, Batch: 600 \n",
      "Epoch Loss: 0.1664303273893893 \n",
      "Total Loss: 0.3867177943936377 \n",
      "\n",
      "Epoch: 45, Batch: 700 \n",
      "Epoch Loss: 0.16728533533534834 \n",
      "Total Loss: 0.33201974561680403 \n",
      "\n",
      "Epoch: 45, Batch: 800 \n",
      "Epoch Loss: 0.16902717381715773 \n",
      "Total Loss: 0.29102066642356417 \n",
      "\n",
      "Epoch: 45, Batch: 900 \n",
      "Epoch Loss: 0.1681265078898933 \n",
      "Total Loss: 0.2590823730690244 \n",
      "\n",
      "Epoch: 46, Batch: 100 \n",
      "Epoch Loss: 0.15698161713778971 \n",
      "Total Loss: 2.2855851931716114 \n",
      "\n",
      "Epoch: 46, Batch: 200 \n",
      "Epoch Loss: 0.1619600820913911 \n",
      "Total Loss: 1.1446071460102076 \n",
      "\n",
      "Epoch: 46, Batch: 300 \n",
      "Epoch Loss: 0.1620727382103602 \n",
      "Total Loss: 0.7642475035028073 \n",
      "\n",
      "Epoch: 46, Batch: 400 \n",
      "Epoch Loss: 0.16123294712975622 \n",
      "Total Loss: 0.5740482013982356 \n",
      "\n",
      "Epoch: 46, Batch: 500 \n",
      "Epoch Loss: 0.16362967272102832 \n",
      "Total Loss: 0.4599916766624412 \n",
      "\n",
      "Epoch: 46, Batch: 600 \n",
      "Epoch Loss: 0.16350916073968014 \n",
      "Total Loss: 0.3839166385260667 \n",
      "\n",
      "Epoch: 46, Batch: 700 \n",
      "Epoch Loss: 0.16330543974148376 \n",
      "Total Loss: 0.3295747681582196 \n",
      "\n",
      "Epoch: 46, Batch: 800 \n",
      "Epoch Loss: 0.16387174162548035 \n",
      "Total Loss: 0.28883399783087005 \n",
      "\n",
      "Epoch: 46, Batch: 900 \n",
      "Epoch Loss: 0.16349892175032033 \n",
      "Total Loss: 0.2571290520881865 \n",
      "\n",
      "Epoch: 47, Batch: 100 \n",
      "Epoch Loss: 0.15480460807681085 \n",
      "Total Loss: 2.26937267639377 \n",
      "\n",
      "Epoch: 47, Batch: 200 \n",
      "Epoch Loss: 0.1573160411976278 \n",
      "Total Loss: 1.1363866304768686 \n",
      "\n",
      "Epoch: 47, Batch: 300 \n",
      "Epoch Loss: 0.15802507858723402 \n",
      "Total Loss: 0.758721889490724 \n",
      "\n",
      "Epoch: 47, Batch: 400 \n",
      "Epoch Loss: 0.15660356079228221 \n",
      "Total Loss: 0.5698517309872315 \n",
      "\n",
      "Epoch: 47, Batch: 500 \n",
      "Epoch Loss: 0.15710740166157483 \n",
      "Total Loss: 0.45655850293931177 \n",
      "\n",
      "Epoch: 47, Batch: 600 \n",
      "Epoch Loss: 0.1590935043245554 \n",
      "Total Loss: 0.3810647950651692 \n",
      "\n",
      "Epoch: 47, Batch: 700 \n",
      "Epoch Loss: 0.16185979089034455 \n",
      "Total Loss: 0.3271693912421362 \n",
      "\n",
      "Epoch: 47, Batch: 800 \n",
      "Epoch Loss: 0.1631328744115308 \n",
      "Total Loss: 0.28673078238756017 \n",
      "\n",
      "Epoch: 47, Batch: 900 \n",
      "Epoch Loss: 0.16185537884218826 \n",
      "Total Loss: 0.25523028272342807 \n",
      "\n",
      "Epoch: 48, Batch: 100 \n",
      "Epoch Loss: 0.1510440795123577 \n",
      "Total Loss: 2.2534170594516523 \n",
      "\n",
      "Epoch: 48, Batch: 200 \n",
      "Epoch Loss: 0.15737267704680563 \n",
      "Total Loss: 1.1284137513360475 \n",
      "\n",
      "Epoch: 48, Batch: 300 \n",
      "Epoch Loss: 0.16331502192964156 \n",
      "Total Loss: 0.7534924988885824 \n",
      "\n",
      "Epoch: 48, Batch: 400 \n",
      "Epoch Loss: 0.15961674448102714 \n",
      "Total Loss: 0.5658929257921409 \n",
      "\n",
      "Epoch: 48, Batch: 500 \n",
      "Epoch Loss: 0.16331128495931627 \n",
      "Total Loss: 0.45345637999568134 \n",
      "\n",
      "Epoch: 48, Batch: 600 \n",
      "Epoch Loss: 0.16412286660944422 \n",
      "Total Loss: 0.3784642776868875 \n",
      "\n",
      "Epoch: 48, Batch: 700 \n",
      "Epoch Loss: 0.16314515546496425 \n",
      "Total Loss: 0.3248660442333979 \n",
      "\n",
      "Epoch: 48, Batch: 800 \n",
      "Epoch Loss: 0.16336757871787996 \n",
      "Total Loss: 0.28468727969768226 \n",
      "\n",
      "Epoch: 48, Batch: 900 \n",
      "Epoch Loss: 0.16318223948693938 \n",
      "Total Loss: 0.2534296641887718 \n",
      "\n",
      "Epoch: 49, Batch: 100 \n",
      "Epoch Loss: 0.15327990788966417 \n",
      "Total Loss: 2.238459604255551 \n",
      "\n",
      "Epoch: 49, Batch: 200 \n",
      "Epoch Loss: 0.15519855523481965 \n",
      "Total Loss: 1.1208330388887957 \n",
      "\n",
      "Epoch: 49, Batch: 300 \n",
      "Epoch Loss: 0.16027884804954132 \n",
      "Total Loss: 0.7483814778556528 \n",
      "\n",
      "Epoch: 49, Batch: 400 \n",
      "Epoch Loss: 0.15856696971692144 \n",
      "Total Loss: 0.5620689213239797 \n",
      "\n",
      "Epoch: 49, Batch: 500 \n",
      "Epoch Loss: 0.15849964210391046 \n",
      "Total Loss: 0.45030097514755874 \n",
      "\n",
      "Epoch: 49, Batch: 600 \n",
      "Epoch Loss: 0.15860432090858617 \n",
      "Total Loss: 0.3757920633540267 \n",
      "\n",
      "Epoch: 49, Batch: 700 \n",
      "Epoch Loss: 0.1592828752632652 \n",
      "Total Loss: 0.32258373419088976 \n",
      "\n",
      "Epoch: 49, Batch: 800 \n",
      "Epoch Loss: 0.16146342161577196 \n",
      "Total Loss: 0.2827116022284656 \n",
      "\n",
      "Epoch: 49, Batch: 900 \n",
      "Epoch Loss: 0.15976020438803568 \n",
      "Total Loss: 0.2516305726533439 \n",
      "\n",
      "Epoch: 50, Batch: 100 \n",
      "Epoch Loss: 0.15116652976721526 \n",
      "Total Loss: 2.2233068762052803 \n",
      "\n",
      "Epoch: 50, Batch: 200 \n",
      "Epoch Loss: 0.15750394215807317 \n",
      "Total Loss: 1.1132918516481296 \n",
      "\n",
      "Epoch: 50, Batch: 300 \n",
      "Epoch Loss: 0.1586557004476587 \n",
      "Total Loss: 0.7432676292122652 \n",
      "\n",
      "Epoch: 50, Batch: 400 \n",
      "Epoch Loss: 0.15899409405887127 \n",
      "Total Loss: 0.5582507682836615 \n",
      "\n",
      "Epoch: 50, Batch: 500 \n",
      "Epoch Loss: 0.16122568906843662 \n",
      "Total Loss: 0.447281222903356 \n",
      "\n",
      "Epoch: 50, Batch: 600 \n",
      "Epoch Loss: 0.16301530793930094 \n",
      "Total Loss: 0.37330756376044205 \n",
      "\n",
      "Epoch: 50, Batch: 700 \n",
      "Epoch Loss: 0.1622623211890459 \n",
      "Total Loss: 0.32042861008234325 \n",
      "\n",
      "Epoch: 50, Batch: 800 \n",
      "Epoch Loss: 0.163048570654355 \n",
      "Total Loss: 0.28079641461432914 \n",
      "\n",
      "Epoch: 50, Batch: 900 \n",
      "Epoch Loss: 0.16299320256544483 \n",
      "Total Loss: 0.24995803578574624 \n",
      "\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        labels = labels.argmax(dim=1).long()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = cnn2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1} \\n', f'Epoch Loss: {epoch_loss/(i+1)} \\n',\n",
    "                  f'Total Loss: {total_loss/((i+1)*(epoch+1))} \\n', sep='')\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn.to(cpu)\n",
    "cnn2 = cnn2.to(cpu)\n",
    "X_train = X_train.to(cpu)\n",
    "X_test = X_test.to(cpu)\n",
    "y_train_onehot = y_train_onehot.to(cpu)\n",
    "y_test_onehot = y_test_onehot.to(cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate train and test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9421)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_res_mse = torch.sum(torch.argmax(cnn(X_train), dim=1) == \n",
    "                          torch.argmax(y_train_onehot, dim=1))/torch.tensor(len(X_train)).float()\n",
    "train_res_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9439)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_res_mse = torch.sum(torch.argmax(cnn(X_test), dim=1) == \n",
    "                             torch.argmax(y_test_onehot, dim=1))/torch.tensor(len(X_test)).float()\n",
    "    \n",
    "test_res_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9509)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_res_cel = torch.sum(torch.argmax(cnn2(X_train), dim=1) == \n",
    "                              torch.argmax(y_train_onehot, dim=1))/torch.tensor(len(X_train)).float()\n",
    "train_res_cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9514)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_res_cel = torch.sum(torch.argmax(cnn2(X_test), dim=1) == \n",
    "                             torch.argmax(y_test_onehot, dim=1))/torch.tensor(len(X_test)).float()\n",
    "\n",
    "test_res_cel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designing a network with deep channels actually performed substantially worse than the network above which only has 8 channels by the second layer.  Before drastically shrinking the network, I had designed it with 4 layers with pooling, with the 4th layer having 128 channels.  Additionally, the first dense layer had 256 nodes.  Perhaps a model training on a grayscale dataset with only 784 pixels per image only generates noise if too complex.  MSE and cross-entropy both yielded very similar results.  For some reason, accuracy scores were slightly better when computed on CPU vs GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
